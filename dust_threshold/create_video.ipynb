{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "\n",
    "class ModelType(Enum):\n",
    "    CLASSIFICATION = 0\n",
    "    SEGMENTATION = 1\n",
    "\n",
    "classlabels_viz_colors = ['black', 'green', 'yellow', 'blue', 'red', 'magenta', 'cyan',\n",
    "                          'lightseagreen', 'brown', 'magenta', 'olive', 'wheat', 'white', 'black']\n",
    "classlabels_viz_bounds = [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 100]\n",
    "\n",
    "classlabels_viz_cmap = mpl.colors.ListedColormap(classlabels_viz_colors)\n",
    "classlabels_viz_norm = mpl.colors.BoundaryNorm(classlabels_viz_bounds, classlabels_viz_cmap.N)\n",
    "\n",
    "confidence_heatmap_viz_colors = ['black', 'blue', 'red', 'orange', 'yellow', 'lightgreen', 'lightseagreen']\n",
    "confidence_heatmap_viz_bounds = [-1, 0,0.5,0.6,0.7,0.8,0.9,1]\n",
    "confidence_heatmap_viz_cmap = mpl.colors.ListedColormap(confidence_heatmap_viz_colors)\n",
    "confidence_heatmap_viz_norm = mpl.colors.BoundaryNorm(confidence_heatmap_viz_bounds, confidence_heatmap_viz_cmap.N)\n",
    "\n",
    "\n",
    "LabelColor = namedtuple('LabelColor', ['name', 'id', 'trainid', 'color', 'category'])\n",
    "\n",
    "LABEL_COLORS = [\n",
    "    LabelColor('class1', 1, 0, (128, 0, 128), 'driveableterrain'),\n",
    "    LabelColor('class2', 2, 1, (255, 0, 0), 'non-driveableterrain'),\n",
    "    LabelColor('class3', 3, 2, (0, 0, 255), 'sky'),\n",
    "    LabelColor('class4', 4, 3, (0, 255, 0), 'trees'),\n",
    "    LabelColor('class5', 5, 4, (255, 0, 255), 'implement'),\n",
    "    LabelColor('class6', 6, 5, (255, 255, 0), 'basket markers')\n",
    "]\n",
    "\n",
    "LABEL_COLORS_4CLASS = LABEL_COLORS[0:4]\n",
    "LABEL_COLORS_5CLASS = LABEL_COLORS[0:5]\n",
    "LABEL_COLORS_6CLASS = LABEL_COLORS[0:6]\n",
    "LABEL_COLORS_SKY_DET = [LABEL_COLORS[0], LABEL_COLORS[2]]\n",
    "\n",
    "LABEL_COLORS_IMPL = [\n",
    "    LabelColor('class1', 1, 1, (128, 0, 128), 'implement'),\n",
    "    LabelColor('class2', 2, 2, (255, 0, 0), 'sweep'),\n",
    "    LabelColor('class3', 3, 3, (0, 0, 255), 'harrow_tine'),\n",
    "    LabelColor('class4', 4, 4, (0, 255, 0), 'basket'),\n",
    "    LabelColor('class5', 5, 5, (0, 255, 0), 'basket_marker'),\n",
    "    LabelColor('class6', 0, 255, (0, 0, 0), 'ignore')\n",
    "]\n",
    "\n",
    "LABEL_COLORS_IMPL_REDUCED = [\n",
    "    LabelColor('class0', 1, 0, (0, 0, 0), 'background'),\n",
    "    LabelColor('class1', 2, 1, (0, 255, 0), 'implement'),\n",
    "    LabelColor('class2', 3, 2, (255, 0, 0), 'sweep'),\n",
    "    LabelColor('class3', 4, 3, (0, 255, 0), 'basket_marker'),\n",
    "    LabelColor('class6', 0, 255, (0, 0, 0), 'ignore')\n",
    "]\n",
    "\n",
    "PLUG_LABEL_MAP ={0: 'no-plug', 1: 'plug'}\n",
    "IMPL_SEGMENT_LABEL_MAP = {0: 'background', 1: 'implement', 2: 'sweep', 3:'basket_marker'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mpl_viz_outputs(output_path,\n",
    "                           image,\n",
    "                           prediction_labels,\n",
    "                           confidences,\n",
    "                           depth_img, \n",
    "                           image_title='Image', \n",
    "                           pred_title='Prediction', \n",
    "                           conf_title='Confidence', \n",
    "                           depth_title='Depth',\n",
    "                           bbox_coords=[]):\n",
    "    \"\"\"\n",
    "    Utility function to plot results based on order of input provided.\n",
    "\n",
    "    Axes index can have different values based on input provided.\n",
    "\n",
    "    For example: If image, prediction and groundtruth_label is provided, A three axes plot will be generated with\n",
    "    image(ax1), ground_truth(ax2), prediction(ax3).\n",
    "\n",
    "    Order of plot if all the inputs are provided will be in same order as arguments listed above.\n",
    "    \"\"\"\n",
    "    axis_index = list(range(len(list(filter(lambda x: x is not None, [image,\n",
    "                                                                      prediction_labels, confidences,\n",
    "                                                                      depth_img])))))\n",
    "    axis_curr_index = 0\n",
    "    fig, axes = mpl.pyplot.subplots(1, len(axis_index), figsize=((60, 30)))\n",
    "    if bbox_coords:\n",
    "#         xmin, ymin, xmax, ymax = bbox_coords\n",
    "        xmin, xmax, ymin, ymax = bbox_coords\n",
    "        if (xmin < 0) or (ymin < 0):\n",
    "            raise ValueError(f'Either {xmin} or {ymin} are negative')\n",
    "        else:\n",
    "            rect = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',\n",
    "                                     facecolor='none')\n",
    "    else:\n",
    "        rect = None\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        # Grab only RGB channels from image, otherwise depth with distort the image when it is displayed\n",
    "        axes[axis_curr_index].imshow(image)\n",
    "        axes[axis_curr_index].set_title(image_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        axes[axis_curr_index].imshow(depth_img, cmap='turbo')\n",
    "        axes[axis_curr_index].set_title(depth_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        axes[axis_curr_index].imshow(prediction_labels, classlabels_viz_cmap, classlabels_viz_norm, interpolation='nearest')\n",
    "        if rect is not None:\n",
    "            rect1 = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',facecolor='none')\n",
    "            axes[axis_curr_index].add_patch(rect1)\n",
    "        axes[axis_curr_index].set_title(pred_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        c = np.max(confidences, axis=2)\n",
    "        axes[axis_curr_index].imshow(c, confidence_heatmap_viz_cmap, confidence_heatmap_viz_norm, interpolation='nearest')\n",
    "#         if rect is not None:\n",
    "#             rect2 = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',\n",
    "#                                           facecolor='none')\n",
    "#             axes[axis_curr_index].add_patch(rect2)\n",
    "        axes[axis_curr_index].set_title(conf_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    mpl.pyplot.savefig(output_path, pad_inches=0, bbox_inches='tight', dpi=150)\n",
    "    mpl.pyplot.close('all')\n",
    "\n",
    "def read_image(image_path):\n",
    "    image = (np.load(image_path) * 255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "def normalize_and_clip_depth(depth, max_depth):\n",
    "    \"\"\"\n",
    "    Return an optionally normalized (and clipped) depth.\n",
    "    \"\"\"\n",
    "    depth[np.isnan(depth)] = max_depth\n",
    "    depth[depth > max_depth] = max_depth\n",
    "    depth = ((depth) / max_depth).astype(np.float32)\n",
    "    return depth\n",
    "\n",
    "DEFAULT_TONEMAP_PARAMS = {\"policy\": \"tonemap\", \"alpha\": 0.25, \"beta\": 0.9, \"gamma\": 0.9, \"eps\": 1e-6}\n",
    "def normalize_image(image, hdr_mode=True, normalization_params=DEFAULT_TONEMAP_PARAMS, return_8_bit=False):\n",
    "    \"\"\"\n",
    "    Normalize an 8 bit image according to the specified policy.\n",
    "    If return_8_bit, this returns an np.uint8 image, otherwise it returns a floating point\n",
    "    image with values in [0, 1].\n",
    "    \"\"\"\n",
    "    normalization_policy = normalization_params['policy']\n",
    "    lower_bound = 0\n",
    "    upper_bound = 1\n",
    "    if np.isnan(hdr_mode):\n",
    "        hdr_mode = False\n",
    "\n",
    "    if hdr_mode and image.dtype == np.uint8:\n",
    "        # The image was normalized during pack-perception (tonemap)\n",
    "        if return_8_bit:\n",
    "            return image\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 255.0\n",
    "    elif normalization_policy == \"percentile\" and hdr_mode:\n",
    "        lower_bound = np.array([np.percentile(image[..., i],\n",
    "                                              normalization_params['lower_bound'],\n",
    "                                              interpolation='lower')\n",
    "                                for i in range(3)])\n",
    "        upper_bound = np.array([np.percentile(image[..., i],\n",
    "                                              normalization_params['upper_bound'],\n",
    "                                              interpolation='lower')\n",
    "                                for i in range(3)])\n",
    "    elif normalization_policy == \"percentile_vpu\" and hdr_mode:\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        brightness = (3 * r + b + 4 * g) / 8\n",
    "        lower_bound = np.percentile(brightness, normalization_params['lower_bound'],\n",
    "                                    interpolation='lower')\n",
    "        upper_bound = np.percentile(brightness, normalization_params['upper_bound'],\n",
    "                                    interpolation='lower')\n",
    "    elif normalization_policy == \"3sigma\" and hdr_mode:\n",
    "        sigma_size = normalization_params['sigma_size']\n",
    "        min_variance = normalization_params['min_variance']\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        brightness = (3 * r + b + 4 * g) / 8\n",
    "        mean, sigma = np.mean(brightness), np.std(brightness)\n",
    "        brightness_min, brightness_max = np.min(brightness), np.max(brightness)\n",
    "        if (sigma * sigma_size) > mean:\n",
    "            lmin = brightness_min\n",
    "            lmax = min(brightness_max, mean * sigma_size)\n",
    "            if (lmax - lmin) < min_variance:\n",
    "                lmax = lmin + min_variance\n",
    "            lower_bound = lmin\n",
    "            upper_bound = lmax\n",
    "        else:\n",
    "            mean_var = mean - sigma_size * sigma\n",
    "            output_min = max(brightness_min, mean_var)\n",
    "            mean_var = mean + sigma_size * sigma\n",
    "            output_max = min(brightness_max, mean_var)\n",
    "            if (output_max - output_min) < min_variance:\n",
    "                output_min = mean - min_variance / 2.0\n",
    "                output_min = 0 if output_min < 0 else output_min\n",
    "                output_max = output_min + min_variance\n",
    "            lower_bound = output_min\n",
    "            upper_bound = output_max\n",
    "    elif normalization_policy == 'tonemap' and hdr_mode:\n",
    "        if image.dtype != np.float32 and image.dtype != np.uint32:\n",
    "            raise ValueError('HDR image type is {} instead of float32 or uint32'.format(image.dtype))\n",
    "        alpha = normalization_params.get('alpha', DEFAULT_TONEMAP_PARAMS['alpha'])\n",
    "        beta = normalization_params.get('beta', DEFAULT_TONEMAP_PARAMS['beta'])\n",
    "        gamma = normalization_params.get('gamma', DEFAULT_TONEMAP_PARAMS['gamma'])\n",
    "        eps = normalization_params.get('eps', DEFAULT_TONEMAP_PARAMS['eps'])\n",
    "\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        lum_in = 0.2126 * r + 0.7152 * g + 0.0722 * b\n",
    "        lum_norm = np.exp(gamma * np.mean(np.log(lum_in + eps)))\n",
    "        c = alpha * lum_in / lum_norm\n",
    "        c_max = beta * np.max(c)\n",
    "        lum_out = c / (1 + c) * (1 + c / (c_max ** 2))\n",
    "        image = image * (lum_out / (lum_in + eps))[..., None]\n",
    "    elif normalization_policy == \"none\" and hdr_mode:\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 2**20 - 1\n",
    "    elif normalization_policy == \"default\" or not hdr_mode:\n",
    "        assert np.max(image) <= 255 and np.min(image) >= 0, \"Image with default \" \\\n",
    "            \"mode should be in range [0,255]\"\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 255.0\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"--normalization-policy '{normalization_policy}' is not supported! \"\n",
    "            f\"(on image with hdr_mode={hdr_mode})\")\n",
    "\n",
    "    image = (image.astype(np.float32, copy=False) - lower_bound) / (upper_bound - lower_bound)\n",
    "\n",
    "    if return_8_bit:\n",
    "        image = np.clip(image * 255.0, 0.0, 255.0)\n",
    "        image = np.uint8(image)\n",
    "    else:\n",
    "        image = np.clip(image, 0.0, 1.0)\n",
    "\n",
    "    return image\n",
    "\n",
    "def read_saved_frame(pred_dir, image_id):\n",
    "    states_to_save = ['', 'false_positive', 'false_negative', 'large_object_false_negative', 'true_positive', 'true_negative']\n",
    "    frame = None\n",
    "    for state in states_to_save:\n",
    "        if os.path.isfile(os.path.join(pred_dir, state, image_id+'.png')):\n",
    "            frame = cv2.imread(os.path.join(pred_dir, state, image_id+'.png'))\n",
    "            break\n",
    "        if os.path.isfile(os.path.join(pred_dir, state, image_id+'.jpg')):\n",
    "            frame = cv2.imread(os.path.join(pred_dir, state, image_id+'.jpg'))\n",
    "            break\n",
    "    return frame\n",
    "\n",
    "def read_images(pred_dir, _id):\n",
    "    if not os.path.isfile(os.path.join(pred_dir, _id+'_image.npy')):\n",
    "        return None, None, None, None\n",
    "    image = np.load(os.path.join(pred_dir, _id+'_image.npy'))\n",
    "    \n",
    "    # 100m capped depth\n",
    "    depth = np.load(os.path.join(pred_dir, _id+'_depth.npy'))\n",
    "#     # raw depth\n",
    "#     raw_depth_dir = '/raum_raid/li.yu/data/Jupiter_rock_demo_2021/Jupiter_rock_demo_loamy06_Oct20_2021/model_processed_v4.1_sky_2e-3_lr_1e-3_color_aug_full_model_LR_consistency_regularization_0.2_epoch_23/images/'\n",
    "#     stereo_data = np.load(os.path.join(raw_depth_dir, _id, 'stereo_output.npz'))\n",
    "#     depth = stereo_data['point_cloud'][:,:,-1]\n",
    "#     depth = normalize_and_clip_depth(depth, 200)\n",
    "    \n",
    "    pred_label = np.load(os.path.join(pred_dir, _id+'_pred_label.npy'))\n",
    "    confidence = np.load(os.path.join(pred_dir, _id+'_confidence.npy'))\n",
    "    return image, depth, pred_label, confidence\n",
    "\n",
    "# def create_frame(pred_dir, pred_merged_dir, _id, recreate=False):\n",
    "#     canvas_path = os.path.join(pred_merged_dir, _id+'.png')\n",
    "#     if recreate:\n",
    "#         image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "#         if image is None:\n",
    "#             return None\n",
    "#         create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth)\n",
    "#     frame = cv2.imread(canvas_path)\n",
    "#     return frame\n",
    "\n",
    "def get_bbox_coords(i=-1, bbox_range_list=[], bbox_coord_list=[]):\n",
    "    for bi in range(len(bbox_range_list)):\n",
    "        bbox_range = bbox_range_list[bi]\n",
    "        if bbox_range[0] <= i <= bbox_range[1]:\n",
    "            return bbox_coord_list[bi]\n",
    "    return []\n",
    "\n",
    "def process_frame(pred_dir, pred_merged_dir, _id, recreate=False, bbox_coords=[]):\n",
    "    image, depth, pred_label, confidence = None, None, None, None\n",
    "    l = 0.0\n",
    "    avg_pixel = 0.0\n",
    "    bbox_conf = None\n",
    "    if recreate:\n",
    "        image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "        if image is None:\n",
    "            return image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "        # calculate brightness\n",
    "        hlsImg = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "        l = np.average(hlsImg[:,:,1])\n",
    "        image_title = 'Image (brightness: {:.4f})'.format(l)\n",
    "        # calculate average pixel value at bbox area\n",
    "        if bbox_coords:\n",
    "            ymin, ymax, xmin, xmax = bbox_coords\n",
    "            bbox_pred = pred_label[ymin:ymax+1, xmin:xmax+1]\n",
    "            bbox_conf = confidence[ymin:ymax+1, xmin:xmax+1]\n",
    "            avg_pixel = np.count_nonzero(bbox_pred == 1)\n",
    "    return image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "\n",
    "def create_frame(pred_dir, pred_merged_dir, _id, recreate=False, bbox_coords=[]):\n",
    "    canvas_path = os.path.join(pred_merged_dir, _id+'.png')\n",
    "    image, depth, pred_label, confidence = None, None, None, None\n",
    "    l = 0.0\n",
    "    avg_pixel = 0.0\n",
    "    bbox_conf = None\n",
    "    if recreate:\n",
    "        image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "        if image is None:\n",
    "            return None, None, None, None, None, None, None, None\n",
    "        # calculate brightness\n",
    "        hlsImg = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "        l = np.average(hlsImg[:,:,1])\n",
    "        image_title = 'Image (brightness: {:.4f})'.format(l)\n",
    "        # calculate average pixel value at bbox area\n",
    "        if bbox_coords:\n",
    "            ymin, ymax, xmin, xmax = bbox_coords\n",
    "            bbox_pred = pred_label[ymin:ymax+1, xmin:xmax+1]\n",
    "            bbox_conf = confidence[ymin:ymax+1, xmin:xmax+1]\n",
    "            avg_pixel = np.count_nonzero(bbox_pred == 1)\n",
    "        create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth, image_title=image_title, bbox_coords=bbox_coords)\n",
    "    frame = cv2.imread(canvas_path)\n",
    "    return frame, image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "\n",
    "def create_diff_frame(pred_merged_dir, _id, image1, depth1, pred_label1, confidence1, \n",
    "                      image2, depth2, pred_label2, confidence2, pred_title='prediction', conf_title='confidence'):\n",
    "    canvas_path = os.path.join(pred_merged_dir, _id+'_diff.png')\n",
    "    image = np.abs(image1 - image2)\n",
    "    depth = np.abs(depth1 - depth2)\n",
    "    pred_label = np.abs(pred_label1 - pred_label2)\n",
    "    confidence = np.abs(confidence1 - confidence2)\n",
    "    create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth, conf_title=conf_title)\n",
    "    frame = cv2.imread(canvas_path)\n",
    "    return frame\n",
    "\n",
    "def read_raw_image(data_dir, _id):\n",
    "    image_path = os.path.join(data_dir, 'images', _id, 'artifact_debayeredrgb_0_'+_id+'.png')\n",
    "    image = cv2.imread(image_path)\n",
    "    return image\n",
    "\n",
    "def create_video(ids, pred_dir, video_name, read_func=read_saved_frame, fps=2):\n",
    "    frame = read_func(pred_dir, ids[10])\n",
    "    height, width, layers = frame.shape\n",
    "    print(height, width, layers)\n",
    "\n",
    "    # .avi MJPG,  .mp4 MP4V\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width,height), isColor=True)\n",
    "    \n",
    "    good = 0\n",
    "    for _id in tqdm(ids):\n",
    "        frame = read_func(pred_dir, _id)\n",
    "        if frame is not None:\n",
    "            video.write(frame)\n",
    "            good += 1\n",
    "    print('total', len(ids), 'used', good)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_sequences(df, interval=5*60, per_camera=False):\n",
    "    df = df.sort_values('collected_on')\n",
    "    df['datetime'] = df.collected_on.apply(datetime.fromisoformat)\n",
    "    sequence_dfs = []\n",
    "    delta = timedelta(seconds=interval)\n",
    "    start = True\n",
    "    i0, i = 0, 0\n",
    "    while i < len(df):\n",
    "        if start:\n",
    "            t0 = df.iloc[i].datetime\n",
    "            start = False\n",
    "        else:\n",
    "            t1 = df.iloc[i].datetime\n",
    "            if t1 - t0 > delta or i == len(df) - 1:\n",
    "                chunk_df = df.iloc[i0 : i if i < len(df) - 1 else len(df)]\n",
    "                if per_camera:\n",
    "                    camera_locations = chunk_df.camera_location.unique()\n",
    "                    camera_locations.sort()\n",
    "                    for camera_location in camera_locations:\n",
    "                        sequence_df = chunk_df[chunk_df.camera_location == camera_location]\n",
    "                        sequence_df = sequence_df.sort_values('collected_on')\n",
    "                        sequence_dfs.append(sequence_df)\n",
    "                else:\n",
    "                    sequence_dfs.append(chunk_df)\n",
    "                start = True\n",
    "                i0 = i\n",
    "            else:\n",
    "                t0 = t1\n",
    "        i += 1\n",
    "    return sequence_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create video from PP artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_meta_with_pred(master_df, label_df, unlabeled_pred_df, labeled_pred_df):\n",
    "    print('master_df', master_df.shape)\n",
    "    df = master_df[['id', 'collected_on', 'camera_location', 'operation_time']]\n",
    "    \n",
    "    # merge with label_df\n",
    "    df = df.merge(label_df, on='id', how='left')\n",
    "    df = df.fillna(0)\n",
    "    print('merge label counts', df.shape)\n",
    "\n",
    "    # load prediction on unlabeled data to get dust ratios\n",
    "    print('unlabeled_pred_df', unlabeled_pred_df.shape)\n",
    "    if not 'total_averaged_dust_ratio' in unlabeled_pred_df:\n",
    "        unlabeled_pred_df['total_averaged_dust_ratio'] = unlabeled_pred_df['total_averaged_dust_conf']\n",
    "        unlabeled_pred_df['triangle_averaged_dust_ratio'] = unlabeled_pred_df['masked_avg_dust_conf']\n",
    "    df = df.merge(unlabeled_pred_df[['id', 'total_averaged_dust_ratio', 'triangle_averaged_dust_ratio']], on='id')\n",
    "    print('merge unlabeled dust ratios', df.shape)\n",
    "\n",
    "    # load prediction on labeled data to get the prediction \"state\"\n",
    "    print('labeled_pred_df', labeled_pred_df.shape)\n",
    "    # convert LO states to regular states and fill empty states with TNs\n",
    "    df = df.merge(labeled_pred_df[['id', 'state']], on='id', how='left').drop_duplicates(subset=['id'])\n",
    "    df = df.fillna('true_negative')\n",
    "    df = df.replace('large_object_true_positive', 'true_positive')\n",
    "    df = df.replace('large_object_false_negative', 'false_negative')\n",
    "    print('merge labeled states', df.shape)\n",
    "\n",
    "    # sort by time and add datetime column\n",
    "    df = df.sort_values('collected_on')\n",
    "    df['datetime'] = df.collected_on.apply(datetime.fromisoformat)\n",
    "    df['datehm'] = df.collected_on.apply(lambda x:str(x)[:16])\n",
    "    print('final_df', df.shape)\n",
    "    print('# TPs', len(df[df.state == 'true_positive']), '# Positives', len(df[(df.state == 'true_positive') | (df.state == 'false_negative')]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_dust_ratio_and_state(seq_df, save_path, title='', labeled_states=False, plot_entire_image=True, plot_masked_region=False):\n",
    "    seq_df['datetime'] -= seq_df.iloc[0]['datetime']\n",
    "    plt.figure(1, figsize=(20, 5))\n",
    "    if plot_entire_image and plot_masked_region:\n",
    "        plt.subplot(121)\n",
    "    states = {'true_negative': ['TN', 'blue'], 'false_positive': ['FP', 'orange']}\n",
    "    if labeled_states:\n",
    "        states['false_negative'] = ['FN', 'red']\n",
    "        states['true_positive'] = ['TP', 'green']\n",
    "    if plot_entire_image:\n",
    "        for state, [label, color] in states.items():\n",
    "            sub_df = seq_df[seq_df.state == state]\n",
    "            if len(sub_df) > 0:\n",
    "                plt.scatter(sub_df.datetime.apply(lambda x: x.total_seconds()), sub_df.total_averaged_dust_ratio, label=label, c=color)\n",
    "        plt.title(seq_df.iloc[0].collected_on if len(title) == 0 else title, fontsize=15)\n",
    "        plt.xlabel('Time in sequence (s)', fontsize=15)\n",
    "        plt.ylabel('Dust ratio in entire image', fontsize=15)\n",
    "        plt.legend()\n",
    "    if plot_entire_image and plot_masked_region:\n",
    "        plt.subplot(122)\n",
    "    if plot_masked_region:\n",
    "        for state, [label, color] in states.items():\n",
    "            sub_df = seq_df[seq_df.state == state]\n",
    "            if len(sub_df) > 0:\n",
    "                plt.scatter(sub_df.datetime.apply(lambda x: x.total_seconds()), sub_df.triangle_averaged_dust_ratio, label=label, c=color)\n",
    "        plt.title(seq_df.iloc[0].collected_on if len(title) == 0 else title, fontsize=15)\n",
    "        plt.xlabel('Time in sequence (s)', fontsize=15)\n",
    "        plt.ylabel('Dust ratio in triangles', fontsize=15)\n",
    "        plt.legend()\n",
    "    # plt.show()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def read_from_pp_artifacts(data_dir, df_row):\n",
    "    data_path = os.path.join(data_dir, 'processed/images', df_row.id, 'stereo_output.npz')\n",
    "    img = np.load(data_path)['left']\n",
    "    img_norm = normalize_image(img, df_row.hdr_mode if 'hdr_mode' in df_row else True)\n",
    "    return (img_norm * 255).astype(np.uint8)\n",
    "\n",
    "def add_text(frame, txt_row):\n",
    "    frame = cv2.putText(frame, f'Collected on: {txt_row.collected_on}', \n",
    "                        (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = '/data/jupiter/li.yu/data'\n",
    "unlabeled_datasets = [\"Jupiter_2023_03_29_10pm_30_3pm_Loamy_812_stops_stereo_2\", \n",
    "                      \"Jupiter_2023_04_05_loamy869_dust_collection_stereo\", \n",
    "                      \"Jupiter_2023_may_loamy731_vehicle_dust_human_stereo\"]\n",
    "labeled_datasets = [\"Jupiter_2023_03_02_and_2930_human_vehicle_in_dust_labeled\", \n",
    "                    \"Jupiter_2023_March_29th30th_human_vehicle_in_dust_front_pod_labeled\", \n",
    "                    \"Jupiter_2023_04_05_loamy869_dust_collection_stereo_labeled\", \n",
    "                    \"Jupiter_2023_may_loamy731_vehicle_dust_human_stereo_labeled\"]\n",
    "pred_root = '/data/jupiter/li.yu/exps/driveable_terrain_model/'\n",
    "train_id = 'v57rd_4cls_tiny0occluded5reverse5triangle5_msml_0305'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master_df (29999, 138)\n",
      "merge label counts (29999, 10)\n",
      "unlabeled_pred_df (29999, 9)\n",
      "merge unlabeled dust ratios (29999, 12)\n",
      "labeled_pred_df (6717, 10)\n",
      "merge labeled states (29999, 13)\n",
      "final_df (29999, 15)\n",
      "# TPs 3177 # Positives 3543\n"
     ]
    }
   ],
   "source": [
    "# # set 1\n",
    "# i = 0\n",
    "# master_df = pd.read_csv(os.path.join(data_root_dir, unlabeled_datasets[i], 'master_annotations.csv'), low_memory=False)\n",
    "# label_df1 = pd.read_csv(os.path.join(data_root_dir, labeled_datasets[i], 'label_count.csv'))\n",
    "# label_df2 = pd.read_csv(os.path.join(data_root_dir, labeled_datasets[i+1], 'label_count.csv'))\n",
    "# label_df = pd.concat([label_df1, label_df2], ignore_index=True)\n",
    "# unlabeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, unlabeled_datasets[i]+'_epoch43_newmask', 'dust_ratio.csv'))\n",
    "# labeled_pred_df1 = pd.read_csv(os.path.join(pred_root, train_id, labeled_datasets[i]+'_epoch43', 'output.csv'))\n",
    "# labeled_pred_df2 = pd.read_csv(os.path.join(pred_root, train_id, labeled_datasets[i+1]+'_epoch43', 'output.csv'))\n",
    "# labeled_pred_df = pd.concat([labeled_pred_df1, labeled_pred_df2], ignore_index=True)\n",
    "# df1 = merge_meta_with_pred(master_df, label_df, unlabeled_pred_df, labeled_pred_df)\n",
    "\n",
    "# # set 2\n",
    "# i = 1\n",
    "# master_df = pd.read_csv(os.path.join(data_root_dir, unlabeled_datasets[i], 'master_annotations.csv'), low_memory=False)\n",
    "# label_df = pd.read_csv(os.path.join(data_root_dir, labeled_datasets[i+1], 'label_count.csv'))\n",
    "# unlabeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, unlabeled_datasets[i]+'_epoch43', 'dust_ratio.csv'))\n",
    "# labeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, labeled_datasets[i+1]+'_epoch43', 'output.csv'))\n",
    "# df2 = merge_meta_with_pred(master_df, label_df, unlabeled_pred_df, labeled_pred_df)\n",
    "\n",
    "# set 3\n",
    "i = 2\n",
    "master_df = pd.read_csv(os.path.join(data_root_dir, unlabeled_datasets[i], 'master_annotations.csv'), low_memory=False)\n",
    "label_df = pd.read_csv(os.path.join(data_root_dir, labeled_datasets[i+1], 'label_count.csv'))\n",
    "unlabeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, unlabeled_datasets[i]+'_epoch43', 'dust_ratio.csv'))\n",
    "labeled_pred_df = pd.read_csv(os.path.join(pred_root, train_id, labeled_datasets[i+1]+'_epoch43', 'output.csv'))\n",
    "df3 = merge_meta_with_pred(master_df, label_df, unlabeled_pred_df, labeled_pred_df)\n",
    "\n",
    "df = df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>collected_on</th>\n      <th>camera_location</th>\n      <th>Background</th>\n      <th>Vehicles</th>\n      <th>Humans</th>\n      <th>Dust</th>\n      <th>Humans_location</th>\n      <th>Vehicles_location</th>\n      <th>total_averaged_dust_ratio</th>\n      <th>triangle_averaged_dust_ratio</th>\n      <th>state</th>\n      <th>datetime</th>\n      <th>datehm</th>\n    </tr>\n    <tr>\n      <th>operation_time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>dawn_dusk</th>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n      <td>2535</td>\n    </tr>\n    <tr>\n      <th>daytime</th>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n      <td>12048</td>\n    </tr>\n    <tr>\n      <th>nightime</th>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n      <td>15416</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                   id  collected_on  camera_location  Background  Vehicles  \\\noperation_time                                                               \ndawn_dusk        2535          2535             2535        2535      2535   \ndaytime         12048         12048            12048       12048     12048   \nnightime        15416         15416            15416       15416     15416   \n\n                Humans   Dust  Humans_location  Vehicles_location  \\\noperation_time                                                      \ndawn_dusk         2535   2535             2535               2535   \ndaytime          12048  12048            12048              12048   \nnightime         15416  15416            15416              15416   \n\n                total_averaged_dust_ratio  triangle_averaged_dust_ratio  \\\noperation_time                                                            \ndawn_dusk                            2535                          2535   \ndaytime                             12048                         12048   \nnightime                            15416                         15416   \n\n                state  datetime  datehm  \noperation_time                           \ndawn_dusk        2535      2535    2535  \ndaytime         12048     12048   12048  \nnightime        15416     15416   15416  "
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('operation_time').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 sequences\n"
     ]
    }
   ],
   "source": [
    "pod = 'front'\n",
    "if pod == 'front':\n",
    "    sub_df = df[df.camera_location.str.startswith('front')]\n",
    "else:\n",
    "    sub_df = df[~df.camera_location.str.startswith('front')]\n",
    "seq_dfs = get_sequences(sub_df, interval=10, per_camera=False)\n",
    "print(len(seq_dfs), 'sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dust_ratio_and_state(sub_df.head(n=len(sub_df)), save_path=f'./dust_batch{i+1}_{pod}.png', \n",
    "                        title=f'{pod} pod', labeled_states=True, plot_entire_image=True, plot_masked_region=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-05T16:56:58.078000 (145, 15) [53, 46, 46]\n",
      "2023-04-05T17:00:30.986000 (353, 15) [115, 124, 114]\n",
      "2023-04-05T17:09:43.657000 (186, 15) [61, 66, 59]\n",
      "2023-04-05T17:11:02.555000 (272, 15) [85, 98, 89]\n",
      "2023-04-05T17:13:27.596000 (559, 15) [181, 188, 190]\n",
      "2023-04-05T17:14:39.274000 (124, 15) [42, 39, 43]\n",
      "2023-04-05T17:18:11.744000 (336, 15) [111, 117, 108]\n",
      "2023-04-05T17:18:49.921000 (484, 15) [170, 152, 162]\n",
      "2023-04-05T17:20:16.846000 (946, 15) [328, 313, 305]\n",
      "2023-04-05T17:23:56.056000 (585, 15) [193, 193, 199]\n",
      "2023-04-05T17:29:33.326000 (245, 15) [79, 78, 88]\n",
      "2023-04-05T17:30:14.980000 (98, 15) [34, 32, 32]\n",
      "2023-04-05T17:30:52.100000 (178, 15) [62, 59, 57]\n",
      "2023-04-05T17:33:16.707000 (919, 15) [318, 303, 298]\n",
      "2023-04-05T17:35:04.248000 (325, 15) [114, 107, 104]\n",
      "2023-04-05T17:41:57.075000 (86, 15) [29, 29, 28]\n",
      "2023-04-05T22:04:19.466000 (363, 15) [126, 120, 117]\n",
      "2023-04-05T22:05:20.389000 (485, 15) [169, 157, 159]\n",
      "2023-04-05T22:06:58.792000 (273, 15) [88, 89, 96]\n",
      "2023-04-05T22:12:49.425000 (187, 15) [63, 59, 65]\n",
      "2023-04-05T22:15:14.208000 (280, 15) [91, 90, 99]\n",
      "2023-04-05T22:16:28.440000 (252, 15) [88, 80, 84]\n",
      "2023-04-05T22:19:57.145000 (172, 15) [56, 59, 57]\n",
      "2023-04-05T22:43:16.749000 (171, 15) [60, 56, 55]\n",
      "2023-04-05T22:49:59.650000 (350, 15) [115, 115, 120]\n",
      "2023-04-05T23:16:45.332000 (477, 15) [164, 157, 156]\n",
      "2023-04-05T23:18:52.883000 (402, 15) [141, 127, 134]\n",
      "2023-04-05T23:20:11.041000 (684, 15) [246, 220, 218]\n",
      "2023-04-05T23:31:36.789000 (128, 15) [43, 40, 45]\n",
      "2023-04-05T23:33:23.244000 (269, 15) [98, 85, 86]\n",
      "2023-04-05T23:35:36.323000 (36, 15) [12, 12, 12]\n",
      "2023-04-05T23:36:43.604000 (943, 15) [334, 302, 307]\n",
      "2023-04-05T23:39:04.635000 (889, 15) [323, 288, 278]\n",
      "2023-04-05T23:45:17.958000 (814, 15) [265, 267, 282]\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(data_root_dir, unlabeled_datasets[i])\n",
    "video_dir = os.path.join(data_root_dir, unlabeled_datasets[i], 'videos')\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "if pod == 'front':\n",
    "    cameras = ['front-left-left', 'front-center-left', 'front-right-left']\n",
    "else:\n",
    "    cameras = ['side-left-left', 'rear-left', 'side-right-left']\n",
    "for si, seq_df in enumerate(seq_dfs):\n",
    "    if len(seq_df[(seq_df.state == 'true_positive') | (seq_df.state == 'false_negative')]) <= 2:\n",
    "        continue\n",
    "    name = seq_df.iloc[0].collected_on\n",
    "    # get per camera df and truncate to same length\n",
    "    camera_dfs = [seq_df[seq_df.camera_location == camera] for camera in cameras]\n",
    "    print(name, seq_df.shape, [len(camera_df) for camera_df in camera_dfs])\n",
    "    min_len = min(len(camera_df) for camera_df in camera_dfs)\n",
    "    camera_dfs = [camera_df.head(n=min_len) for camera_df in camera_dfs]\n",
    "\n",
    "    # create video\n",
    "    video_name = os.path.join(video_dir, f'{name}_{pod}pod_{si}.mp4')\n",
    "    frame = read_from_pp_artifacts(data_dir, seq_df.iloc[5])\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), 3, (width,height*3), isColor=True)\n",
    "    for fi in range(min_len):\n",
    "        frames = []\n",
    "        for camera_df in camera_dfs:\n",
    "            frame = read_from_pp_artifacts(data_dir, camera_df.iloc[fi])\n",
    "            frame = add_text(frame, camera_df.iloc[fi])\n",
    "            frames.append(frame)\n",
    "        frame = np.concatenate(frames, axis=0)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        video.write(frame)\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('shank': conda)",
   "name": "python3810jvsc74a57bd0086b207c4b176459976fbd848c46aaef8d683e58303c49649f931f4f7fdf75bf"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "086b207c4b176459976fbd848c46aaef8d683e58303c49649f931f4f7fdf75bf"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}