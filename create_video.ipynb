{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "\n",
    "class ModelType(Enum):\n",
    "    CLASSIFICATION = 0\n",
    "    SEGMENTATION = 1\n",
    "\n",
    "classlabels_viz_colors = ['black', 'green', 'yellow', 'blue', 'red', 'magenta', 'cyan',\n",
    "                          'lightseagreen', 'brown', 'magenta', 'olive', 'wheat', 'white', 'black']\n",
    "classlabels_viz_bounds = [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 100]\n",
    "\n",
    "classlabels_viz_cmap = mpl.colors.ListedColormap(classlabels_viz_colors)\n",
    "classlabels_viz_norm = mpl.colors.BoundaryNorm(classlabels_viz_bounds, classlabels_viz_cmap.N)\n",
    "\n",
    "confidence_heatmap_viz_colors = ['black', 'blue', 'red', 'orange', 'yellow', 'lightgreen', 'lightseagreen']\n",
    "confidence_heatmap_viz_bounds = [-1, 0,0.5,0.6,0.7,0.8,0.9,1]\n",
    "confidence_heatmap_viz_cmap = mpl.colors.ListedColormap(confidence_heatmap_viz_colors)\n",
    "confidence_heatmap_viz_norm = mpl.colors.BoundaryNorm(confidence_heatmap_viz_bounds, confidence_heatmap_viz_cmap.N)\n",
    "\n",
    "\n",
    "LabelColor = namedtuple('LabelColor', ['name', 'id', 'trainid', 'color', 'category'])\n",
    "\n",
    "LABEL_COLORS = [\n",
    "    LabelColor('class1', 1, 0, (128, 0, 128), 'driveableterrain'),\n",
    "    LabelColor('class2', 2, 1, (255, 0, 0), 'non-driveableterrain'),\n",
    "    LabelColor('class3', 3, 2, (0, 0, 255), 'sky'),\n",
    "    LabelColor('class4', 4, 3, (0, 255, 0), 'trees'),\n",
    "    LabelColor('class5', 5, 4, (255, 0, 255), 'implement'),\n",
    "    LabelColor('class6', 6, 5, (255, 255, 0), 'basket markers')\n",
    "]\n",
    "\n",
    "LABEL_COLORS_4CLASS = LABEL_COLORS[0:4]\n",
    "LABEL_COLORS_5CLASS = LABEL_COLORS[0:5]\n",
    "LABEL_COLORS_6CLASS = LABEL_COLORS[0:6]\n",
    "LABEL_COLORS_SKY_DET = [LABEL_COLORS[0], LABEL_COLORS[2]]\n",
    "\n",
    "LABEL_COLORS_IMPL = [\n",
    "    LabelColor('class1', 1, 1, (128, 0, 128), 'implement'),\n",
    "    LabelColor('class2', 2, 2, (255, 0, 0), 'sweep'),\n",
    "    LabelColor('class3', 3, 3, (0, 0, 255), 'harrow_tine'),\n",
    "    LabelColor('class4', 4, 4, (0, 255, 0), 'basket'),\n",
    "    LabelColor('class5', 5, 5, (0, 255, 0), 'basket_marker'),\n",
    "    LabelColor('class6', 0, 255, (0, 0, 0), 'ignore')\n",
    "]\n",
    "\n",
    "LABEL_COLORS_IMPL_REDUCED = [\n",
    "    LabelColor('class0', 1, 0, (0, 0, 0), 'background'),\n",
    "    LabelColor('class1', 2, 1, (0, 255, 0), 'implement'),\n",
    "    LabelColor('class2', 3, 2, (255, 0, 0), 'sweep'),\n",
    "    LabelColor('class3', 4, 3, (0, 255, 0), 'basket_marker'),\n",
    "    LabelColor('class6', 0, 255, (0, 0, 0), 'ignore')\n",
    "]\n",
    "\n",
    "PLUG_LABEL_MAP ={0: 'no-plug', 1: 'plug'}\n",
    "IMPL_SEGMENT_LABEL_MAP = {0: 'background', 1: 'implement', 2: 'sweep', 3:'basket_marker'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mpl_viz_outputs(output_path,\n",
    "                           image,\n",
    "                           prediction_labels,\n",
    "                           confidences,\n",
    "                           depth_img, \n",
    "                           image_title='Image', \n",
    "                           pred_title='Prediction', \n",
    "                           conf_title='Confidence', \n",
    "                           depth_title='Depth',\n",
    "                           bbox_coords=[]):\n",
    "    \"\"\"\n",
    "    Utility function to plot results based on order of input provided.\n",
    "\n",
    "    Axes index can have different values based on input provided.\n",
    "\n",
    "    For example: If image, prediction and groundtruth_label is provided, A three axes plot will be generated with\n",
    "    image(ax1), ground_truth(ax2), prediction(ax3).\n",
    "\n",
    "    Order of plot if all the inputs are provided will be in same order as arguments listed above.\n",
    "    \"\"\"\n",
    "    axis_index = list(range(len(list(filter(lambda x: x is not None, [image,\n",
    "                                                                      prediction_labels, confidences,\n",
    "                                                                      depth_img])))))\n",
    "    axis_curr_index = 0\n",
    "    fig, axes = mpl.pyplot.subplots(1, len(axis_index), figsize=((60, 30)))\n",
    "    if bbox_coords:\n",
    "#         xmin, ymin, xmax, ymax = bbox_coords\n",
    "        xmin, xmax, ymin, ymax = bbox_coords\n",
    "        if (xmin < 0) or (ymin < 0):\n",
    "            raise ValueError(f'Either {xmin} or {ymin} are negative')\n",
    "        else:\n",
    "            rect = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',\n",
    "                                     facecolor='none')\n",
    "    else:\n",
    "        rect = None\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        # Grab only RGB channels from image, otherwise depth with distort the image when it is displayed\n",
    "        axes[axis_curr_index].imshow(image)\n",
    "        axes[axis_curr_index].set_title(image_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        axes[axis_curr_index].imshow(depth_img, cmap='turbo')\n",
    "        axes[axis_curr_index].set_title(depth_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        axes[axis_curr_index].imshow(prediction_labels, classlabels_viz_cmap, classlabels_viz_norm, interpolation='nearest')\n",
    "        if rect is not None:\n",
    "            rect1 = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',facecolor='none')\n",
    "            axes[axis_curr_index].add_patch(rect1)\n",
    "        axes[axis_curr_index].set_title(pred_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        c = np.max(confidences, axis=2)\n",
    "        axes[axis_curr_index].imshow(c, confidence_heatmap_viz_cmap, confidence_heatmap_viz_norm, interpolation='nearest')\n",
    "#         if rect is not None:\n",
    "#             rect2 = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',\n",
    "#                                           facecolor='none')\n",
    "#             axes[axis_curr_index].add_patch(rect2)\n",
    "        axes[axis_curr_index].set_title(conf_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    mpl.pyplot.savefig(output_path, pad_inches=0, bbox_inches='tight', dpi=150)\n",
    "    mpl.pyplot.close('all')\n",
    "\n",
    "def read_image(image_path):\n",
    "    image = (np.load(image_path) * 255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "def normalize_and_clip_depth(depth, max_depth):\n",
    "    \"\"\"\n",
    "    Return an optionally normalized (and clipped) depth.\n",
    "    \"\"\"\n",
    "    depth[np.isnan(depth)] = max_depth\n",
    "    depth[depth > max_depth] = max_depth\n",
    "    depth = ((depth) / max_depth).astype(np.float32)\n",
    "    return depth\n",
    "\n",
    "DEFAULT_TONEMAP_PARAMS = {\"policy\": \"tonemap\", \"alpha\": 0.25, \"beta\": 0.9, \"gamma\": 0.9, \"eps\": 1e-6}\n",
    "def normalize_image(image, hdr_mode=True, normalization_params=DEFAULT_TONEMAP_PARAMS, return_8_bit=False):\n",
    "    \"\"\"\n",
    "    Normalize an 8 bit image according to the specified policy.\n",
    "    If return_8_bit, this returns an np.uint8 image, otherwise it returns a floating point\n",
    "    image with values in [0, 1].\n",
    "    \"\"\"\n",
    "    normalization_policy = normalization_params['policy']\n",
    "    lower_bound = 0\n",
    "    upper_bound = 1\n",
    "    if np.isnan(hdr_mode):\n",
    "        hdr_mode = False\n",
    "\n",
    "    if hdr_mode and image.dtype == np.uint8:\n",
    "        # The image was normalized during pack-perception (tonemap)\n",
    "        if return_8_bit:\n",
    "            return image\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 255.0\n",
    "    elif normalization_policy == \"percentile\" and hdr_mode:\n",
    "        lower_bound = np.array([np.percentile(image[..., i],\n",
    "                                              normalization_params['lower_bound'],\n",
    "                                              interpolation='lower')\n",
    "                                for i in range(3)])\n",
    "        upper_bound = np.array([np.percentile(image[..., i],\n",
    "                                              normalization_params['upper_bound'],\n",
    "                                              interpolation='lower')\n",
    "                                for i in range(3)])\n",
    "    elif normalization_policy == \"percentile_vpu\" and hdr_mode:\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        brightness = (3 * r + b + 4 * g) / 8\n",
    "        lower_bound = np.percentile(brightness, normalization_params['lower_bound'],\n",
    "                                    interpolation='lower')\n",
    "        upper_bound = np.percentile(brightness, normalization_params['upper_bound'],\n",
    "                                    interpolation='lower')\n",
    "    elif normalization_policy == \"3sigma\" and hdr_mode:\n",
    "        sigma_size = normalization_params['sigma_size']\n",
    "        min_variance = normalization_params['min_variance']\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        brightness = (3 * r + b + 4 * g) / 8\n",
    "        mean, sigma = np.mean(brightness), np.std(brightness)\n",
    "        brightness_min, brightness_max = np.min(brightness), np.max(brightness)\n",
    "        if (sigma * sigma_size) > mean:\n",
    "            lmin = brightness_min\n",
    "            lmax = min(brightness_max, mean * sigma_size)\n",
    "            if (lmax - lmin) < min_variance:\n",
    "                lmax = lmin + min_variance\n",
    "            lower_bound = lmin\n",
    "            upper_bound = lmax\n",
    "        else:\n",
    "            mean_var = mean - sigma_size * sigma\n",
    "            output_min = max(brightness_min, mean_var)\n",
    "            mean_var = mean + sigma_size * sigma\n",
    "            output_max = min(brightness_max, mean_var)\n",
    "            if (output_max - output_min) < min_variance:\n",
    "                output_min = mean - min_variance / 2.0\n",
    "                output_min = 0 if output_min < 0 else output_min\n",
    "                output_max = output_min + min_variance\n",
    "            lower_bound = output_min\n",
    "            upper_bound = output_max\n",
    "    elif normalization_policy == 'tonemap' and hdr_mode:\n",
    "        if image.dtype != np.float32 and image.dtype != np.uint32:\n",
    "            raise ValueError('HDR image type is {} instead of float32 or uint32'.format(image.dtype))\n",
    "        alpha = normalization_params.get('alpha', DEFAULT_TONEMAP_PARAMS['alpha'])\n",
    "        beta = normalization_params.get('beta', DEFAULT_TONEMAP_PARAMS['beta'])\n",
    "        gamma = normalization_params.get('gamma', DEFAULT_TONEMAP_PARAMS['gamma'])\n",
    "        eps = normalization_params.get('eps', DEFAULT_TONEMAP_PARAMS['eps'])\n",
    "\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        lum_in = 0.2126 * r + 0.7152 * g + 0.0722 * b\n",
    "        lum_norm = np.exp(gamma * np.mean(np.log(lum_in + eps)))\n",
    "        c = alpha * lum_in / lum_norm\n",
    "        c_max = beta * np.max(c)\n",
    "        lum_out = c / (1 + c) * (1 + c / (c_max ** 2))\n",
    "        image = image * (lum_out / (lum_in + eps))[..., None]\n",
    "    elif normalization_policy == \"none\" and hdr_mode:\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 2**20 - 1\n",
    "    elif normalization_policy == \"default\" or not hdr_mode:\n",
    "        assert np.max(image) <= 255 and np.min(image) >= 0, \"Image with default \" \\\n",
    "            \"mode should be in range [0,255]\"\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 255.0\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"--normalization-policy '{normalization_policy}' is not supported! \"\n",
    "            f\"(on image with hdr_mode={hdr_mode})\")\n",
    "\n",
    "    image = (image.astype(np.float32, copy=False) - lower_bound) / (upper_bound - lower_bound)\n",
    "\n",
    "    if return_8_bit:\n",
    "        image = np.clip(image * 255.0, 0.0, 255.0)\n",
    "        image = np.uint8(image)\n",
    "    else:\n",
    "        image = np.clip(image, 0.0, 1.0)\n",
    "\n",
    "    return image\n",
    "\n",
    "def read_saved_frame(pred_dir, image_id):\n",
    "    states_to_save = ['', 'false_positive', 'false_negative', 'large_object_false_negative', 'true_positive', 'true_negative']\n",
    "    frame = None\n",
    "    for state in states_to_save:\n",
    "        if os.path.isfile(os.path.join(pred_dir, state, image_id+'.png')):\n",
    "            frame = cv2.imread(os.path.join(pred_dir, state, image_id+'.png'))\n",
    "            break\n",
    "        if os.path.isfile(os.path.join(pred_dir, state, image_id+'.jpg')):\n",
    "            frame = cv2.imread(os.path.join(pred_dir, state, image_id+'.jpg'))\n",
    "            break\n",
    "    return frame\n",
    "\n",
    "def read_images(pred_dir, _id):\n",
    "    if not os.path.isfile(os.path.join(pred_dir, _id+'_image.npy')):\n",
    "        return None, None, None, None\n",
    "    image = np.load(os.path.join(pred_dir, _id+'_image.npy'))\n",
    "    \n",
    "    # 100m capped depth\n",
    "    depth = np.load(os.path.join(pred_dir, _id+'_depth.npy'))\n",
    "#     # raw depth\n",
    "#     raw_depth_dir = '/raum_raid/li.yu/data/Jupiter_rock_demo_2021/Jupiter_rock_demo_loamy06_Oct20_2021/model_processed_v4.1_sky_2e-3_lr_1e-3_color_aug_full_model_LR_consistency_regularization_0.2_epoch_23/images/'\n",
    "#     stereo_data = np.load(os.path.join(raw_depth_dir, _id, 'stereo_output.npz'))\n",
    "#     depth = stereo_data['point_cloud'][:,:,-1]\n",
    "#     depth = normalize_and_clip_depth(depth, 200)\n",
    "    \n",
    "    pred_label = np.load(os.path.join(pred_dir, _id+'_pred_label.npy'))\n",
    "    confidence = np.load(os.path.join(pred_dir, _id+'_confidence.npy'))\n",
    "    return image, depth, pred_label, confidence\n",
    "\n",
    "# def create_frame(pred_dir, pred_merged_dir, _id, recreate=False):\n",
    "#     canvas_path = os.path.join(pred_merged_dir, _id+'.png')\n",
    "#     if recreate:\n",
    "#         image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "#         if image is None:\n",
    "#             return None\n",
    "#         create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth)\n",
    "#     frame = cv2.imread(canvas_path)\n",
    "#     return frame\n",
    "\n",
    "def get_bbox_coords(i=-1, bbox_range_list=[], bbox_coord_list=[]):\n",
    "    for bi in range(len(bbox_range_list)):\n",
    "        bbox_range = bbox_range_list[bi]\n",
    "        if bbox_range[0] <= i <= bbox_range[1]:\n",
    "            return bbox_coord_list[bi]\n",
    "    return []\n",
    "\n",
    "def process_frame(pred_dir, pred_merged_dir, _id, recreate=False, bbox_coords=[]):\n",
    "    image, depth, pred_label, confidence = None, None, None, None\n",
    "    l = 0.0\n",
    "    avg_pixel = 0.0\n",
    "    bbox_conf = None\n",
    "    if recreate:\n",
    "        image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "        if image is None:\n",
    "            return image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "        # calculate brightness\n",
    "        hlsImg = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "        l = np.average(hlsImg[:,:,1])\n",
    "        image_title = 'Image (brightness: {:.4f})'.format(l)\n",
    "        # calculate average pixel value at bbox area\n",
    "        if bbox_coords:\n",
    "            ymin, ymax, xmin, xmax = bbox_coords\n",
    "            bbox_pred = pred_label[ymin:ymax+1, xmin:xmax+1]\n",
    "            bbox_conf = confidence[ymin:ymax+1, xmin:xmax+1]\n",
    "            avg_pixel = np.count_nonzero(bbox_pred == 1)\n",
    "    return image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "\n",
    "def create_frame(pred_dir, pred_merged_dir, _id, recreate=False, bbox_coords=[]):\n",
    "    canvas_path = os.path.join(pred_merged_dir, _id+'.png')\n",
    "    image, depth, pred_label, confidence = None, None, None, None\n",
    "    l = 0.0\n",
    "    avg_pixel = 0.0\n",
    "    bbox_conf = None\n",
    "    if recreate:\n",
    "        image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "        if image is None:\n",
    "            return None, None, None, None, None, None, None, None\n",
    "        # calculate brightness\n",
    "        hlsImg = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "        l = np.average(hlsImg[:,:,1])\n",
    "        image_title = 'Image (brightness: {:.4f})'.format(l)\n",
    "        # calculate average pixel value at bbox area\n",
    "        if bbox_coords:\n",
    "            ymin, ymax, xmin, xmax = bbox_coords\n",
    "            bbox_pred = pred_label[ymin:ymax+1, xmin:xmax+1]\n",
    "            bbox_conf = confidence[ymin:ymax+1, xmin:xmax+1]\n",
    "            avg_pixel = np.count_nonzero(bbox_pred == 1)\n",
    "        create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth, image_title=image_title, bbox_coords=bbox_coords)\n",
    "    frame = cv2.imread(canvas_path)\n",
    "    return frame, image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "\n",
    "def create_diff_frame(pred_merged_dir, _id, image1, depth1, pred_label1, confidence1, \n",
    "                      image2, depth2, pred_label2, confidence2, pred_title='prediction', conf_title='confidence'):\n",
    "    canvas_path = os.path.join(pred_merged_dir, _id+'_diff.png')\n",
    "    image = np.abs(image1 - image2)\n",
    "    depth = np.abs(depth1 - depth2)\n",
    "    pred_label = np.abs(pred_label1 - pred_label2)\n",
    "    confidence = np.abs(confidence1 - confidence2)\n",
    "    create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth, conf_title=conf_title)\n",
    "    frame = cv2.imread(canvas_path)\n",
    "    return frame\n",
    "\n",
    "def read_raw_image_by_id(data_dir, _id):\n",
    "    image_path = os.path.join(data_dir, 'images', _id, 'artifact_debayeredrgb_0_'+_id+'.png')\n",
    "    image = cv2.imread(image_path)\n",
    "    return image\n",
    "\n",
    "def read_raw_image_by_row(data_dir, row):\n",
    "    image = cv2.imread(os.path.join(data_dir, row.artifact_debayeredrgb_0_save_path))\n",
    "    return image\n",
    "\n",
    "def create_video(ids, pred_dir, video_name, read_func=read_saved_frame, fps=2):\n",
    "    frame = read_func(pred_dir, ids[10])\n",
    "    height, width, layers = frame.shape\n",
    "    print(height, width, layers)\n",
    "\n",
    "    # .avi MJPG,  .mp4 MP4V\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width,height), isColor=True)\n",
    "    \n",
    "    good = 0\n",
    "    for _id in tqdm(ids):\n",
    "        frame = read_func(pred_dir, _id)\n",
    "        if frame is not None:\n",
    "            video.write(frame)\n",
    "            good += 1\n",
    "    print('total', len(ids), 'used', good)\n",
    "\n",
    "    # cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_sequences(df, interval=5*60, per_camera=False):\n",
    "    df = df.sort_values('collected_on')\n",
    "    df['datetime'] = df.collected_on.apply(datetime.fromisoformat)\n",
    "    sequence_dfs = []\n",
    "    delta = timedelta(seconds=interval)\n",
    "    start = True\n",
    "    i0, i = 0, 0\n",
    "    while i < len(df):\n",
    "        if start:\n",
    "            t0 = df.iloc[i].datetime\n",
    "            start = False\n",
    "        else:\n",
    "            t1 = df.iloc[i].datetime\n",
    "            if t1 - t0 > delta or i == len(df) - 1:\n",
    "                chunk_df = df.iloc[i0 : i if i < len(df) - 1 else len(df)]\n",
    "                if per_camera:\n",
    "                    camera_locations = chunk_df.camera_location.unique()\n",
    "                    camera_locations.sort()\n",
    "                    for camera_location in camera_locations:\n",
    "                        sequence_df = chunk_df[chunk_df.camera_location == camera_location]\n",
    "                        sequence_df = sequence_df.sort_values('collected_on')\n",
    "                        sequence_dfs.append(sequence_df)\n",
    "                else:\n",
    "                    sequence_dfs.append(chunk_df)\n",
    "                start = True\n",
    "                i0 = i\n",
    "            else:\n",
    "                t0 = t1\n",
    "        i += 1\n",
    "    return sequence_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create video from raw left images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48102, 108) 4\n"
     ]
    }
   ],
   "source": [
    "# create video from raw left images\n",
    "# data_dir = '/data/jupiter/datasets/halo_failure_case_of_box_in_dust'\n",
    "# data_dir = '/data/jupiter/datasets/halo_missed_lo_rock_0509_stereo'\n",
    "# data_dir = '/data/jupiter/li.yu/data/halo_sample_terrains_15images'\n",
    "# data_dir = '/data/jupiter/li.yu/data/halo_sample_terrains_15images_all_camera_sequence'\n",
    "# data_dir = '/data2/jupiter/datasets/halo_vehicles_in_dust_collection_march2024'\n",
    "data_dir = '/data/jupiter/datasets/halo_human_in_dust_day_collection_may29'\n",
    "df = pd.read_csv(os.path.join(data_dir, 'annotations.csv'))\n",
    "df = df.sort_values('collected_on')\n",
    "\n",
    "# ids = df.id.to_list()\n",
    "# video_name = os.path.join(data_dir, f'video.mp4')\n",
    "# create_video(ids, data_dir, video_name, read_raw_image_by_id, fps=1)\n",
    "\n",
    "seq_dfs = get_sequences(df)\n",
    "print(df.shape, len(seq_dfs))\n",
    "\n",
    "# for i,sub_df in enumerate(seq_dfs):\n",
    "#     ids = sub_df.id.to_list()\n",
    "#     video_name = os.path.join(data_dir, f'dust_seq{i}.mp4')\n",
    "#     create_video(ids, data_dir, video_name, read_raw_image_by_id, fps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for selection of images for binary labeling - halo_failure_case_of_box_in_dust\n",
    "# selected = [3, 5, [9, 1/5], [10, 1/5], [11, 1/6], \n",
    "#     [12, [['2024-04-23T18:11:28', '2024-04-23T18:18:12'], ['2024-04-23T18:27:40', '2024-04-23T18:28:38'], ['2024-04-23T18:29:05', '2024-04-23T18:29:57'], \n",
    "#         ['2024-04-23T18:48:44', '2024-04-23T18:50:59'], ['2024-04-23T19:01:50', '2024-04-23T19:03:37']]], \n",
    "#     [13, [['2024-04-23T19:47:48', '2024-04-23T19:47:59']]], \n",
    "#     [14, [['2024-04-23T20:12:40', '2024-04-23T20:13:02'], ['2024-04-23T20:15:53', '2024-04-23T20:17:49'], ['2024-04-23T20:22:19', '2024-04-23T20:22:45'], \n",
    "#         ['2024-04-23T20:32:47', '2024-04-23T20:33:18'], ['2024-04-23T20:34:48', '2024-04-23T20:35:56'], ['2024-04-23T20:38:00', '2024-04-23T20:38:17'], \n",
    "#         ['2024-04-23T20:39:55', '2024-04-23T20:41:31'], ['2024-04-23T20:42:57', '2024-04-23T20:48:27']]], \n",
    "#     [16, 1/2]]\n",
    "# selected_ids = []\n",
    "# for s in selected:\n",
    "#     if isinstance(s, int):\n",
    "#         selected_ids += seq_dfs[s].id.to_list()\n",
    "#     else:\n",
    "#         si, sj = s\n",
    "#         if isinstance(sj, float):\n",
    "#             selected_ids += seq_dfs[si].iloc[:int(len(seq_dfs[si])*sj)].id.to_list()\n",
    "#         else:\n",
    "#             for t1, t2 in sj:\n",
    "#                 selected_ids += seq_dfs[si][(seq_dfs[si].collected_on >= t1) & (seq_dfs[si].collected_on <= t2)].id.to_list()\n",
    "#     # print(s, len(selected_ids))\n",
    "# selected_ids = list(set(selected_ids))\n",
    "# selected_df = pd.DataFrame(data={'id': selected_ids})\n",
    "# print(selected_df.shape)\n",
    "# selected_df.to_csv('/data/jupiter/datasets/halo_failure_case_of_box_in_dust/selected_for_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "front 0 2024-05-29T19:33:00.080000 2024-05-29T19:34:02.351000 301\n",
      "front 1 2024-05-29T19:40:15.644000 2024-05-29T19:48:13 2676\n",
      "front 2 2024-05-29T20:28:50 2024-05-29T20:30:49.940000 1690\n",
      "right 2 2024-05-29T20:19:06.311000 2024-05-29T20:28:53 1687\n",
      "back 1 2024-05-29T20:02:08 2024-05-29T20:13:58.460000 4231\n",
      "left 1 2024-05-29T19:48:10 2024-05-29T20:02:08 4237\n",
      "(5827, 1)\n"
     ]
    }
   ],
   "source": [
    "# for selection of images for partial human labeling - halo_human_in_dust_day_collection_may29\n",
    "selected = {'front': {0: 'all', 1: ['begin', '2024-05-29T19:48:13'], 2: ['2024-05-29T20:28:50', 'end']},\n",
    "            'right': {2: ['begin', '2024-05-29T20:28:53']},\n",
    "            'back': {1: ['2024-05-29T20:02:08', 'end']},\n",
    "            'left': {1: ['2024-05-29T19:48:10', '2024-05-29T20:02:08']}}\n",
    "all_cameras = {'front': ['T01', 'T02', 'T03', 'T04'], 'right': ['T05', 'T06', 'T07', 'T08'], 'back': ['T09', 'T10', 'T11', 'T12'], 'left': ['T13', 'T14', 'T15', 'T16']}\n",
    "left_cameras = ['front-center-left', 'front-left-left', 'front-right-left', 'side-left-left', 'side-right-left', 'rear-left', 'T01', 'T02', 'T05', 'T06', 'T09', 'T10', 'T13', 'T14', 'I01', 'I02']\n",
    "\n",
    "selected_ids = []\n",
    "for pod, cameras in all_cameras.items():\n",
    "    cameras = set(cameras).intersection(left_cameras)\n",
    "    selected_times = selected[pod]\n",
    "    for seq_i, t1t2 in selected_times.items():\n",
    "        seq_df = seq_dfs[seq_i]\n",
    "        seq_df = seq_df[seq_df.camera_location.isin(cameras)]\n",
    "        if t1t2 == 'all':\n",
    "            t1, t2 = seq_df.iloc[0].collected_on, seq_df.iloc[-1].collected_on\n",
    "        else:\n",
    "            t1, t2 = t1t2\n",
    "            if t1 == 'begin':\n",
    "                t1 = seq_df.iloc[0].collected_on\n",
    "            if t2 == 'end':\n",
    "                t2 = seq_df.iloc[-1].collected_on\n",
    "        print(pod, seq_i, t1, t2, len(seq_df))\n",
    "        selected_ids += seq_df[(seq_df.collected_on >= t1) & (seq_df.collected_on <= t2)].id.to_list()\n",
    "selected_ids = list(set(selected_ids))\n",
    "selected_df = pd.DataFrame(data={'id': selected_ids})\n",
    "print(selected_df.shape)\n",
    "selected_df.to_csv('/data/jupiter/datasets/halo_human_in_dust_day_collection_may29/selected_for_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/149 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T05', 'T06', 'T07', 'T08']\n",
      "0 [150, 149, 150, 150]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:41<00:00,  3.62it/s]\n",
      "  0%|          | 0/1332 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1336, 1332, 1336, 1336]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1332/1332 [06:09<00:00,  3.60it/s]\n",
      "  0%|          | 0/843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [843, 844, 843, 845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 32/843 [00:09<03:52,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32th image read failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 843/843 [03:56<00:00,  3.56it/s]\n",
      "  0%|          | 0/372 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [372, 372, 372, 372]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 372/372 [01:41<00:00,  3.66it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T09', 'T10', 'T11', 'T12']\n",
      "0 [17, 17, 16, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
      "  0%|          | 0/2113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [2113, 2118, 2119, 2119]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1327/2113 [06:16<03:21,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1326th image read failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 1787/2113 [08:29<01:19,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1786th image read failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1814/2113 [08:36<01:15,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813th image read failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1815/2113 [08:37<01:18,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1815th image read failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2113/2113 [10:01<00:00,  3.51it/s]\n",
      "  0%|          | 0/1040 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [1040, 1044, 1044, 1046]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1040/1040 [04:42<00:00,  3.69it/s]\n",
      "  0%|          | 0/131 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [131, 131, 131, 131]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:35<00:00,  3.73it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T13', 'T14', 'T15', 'T16']\n",
      "0 [16, 16, 17, 17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:04<00:00,  3.58it/s]\n",
      "  0%|          | 0/2116 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [2121, 2116, 2119, 2121]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 929/2116 [04:20<05:31,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929th image read failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1026/2116 [04:47<04:07,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025th image read failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1784/2116 [08:20<01:21,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1783th image read failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 1790/2116 [08:21<01:18,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789th image read failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2116/2116 [09:52<00:00,  3.57it/s]\n",
      "  0%|          | 0/1041 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [1048, 1043, 1043, 1041]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1041/1041 [04:49<00:00,  3.60it/s]\n",
      "  0%|          | 0/131 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [131, 132, 131, 131]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:34<00:00,  3.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# read from multiple cameras and put in once frame\n",
    "# cameras = ['T01', 'T02', 'T03', 'T04']\n",
    "# cameras = ['T02', 'T06', 'T14', 'T10']\n",
    "# left_pass_pairs = ['T09_T11', 'T14_T16', 'T14_T15', 'T13_T15']\n",
    "# right_pass_pairs = ['T05_T07', 'T10_T12', 'T06_T08', 'T06_T07']\n",
    "# cameras = [f'T{str(i+1).zfill(2)}' for i in range(16)][12:]\n",
    "# cameras = ['T16', 'T15', 'T14', 'T13', 'T11', 'T09']\n",
    "all_cameras = {'front': ['T01', 'T02', 'T03', 'T04'], 'right': ['T05', 'T06', 'T07', 'T08'], 'back': ['T09', 'T10', 'T11', 'T12'], 'left': ['T13', 'T14', 'T15', 'T16']}\n",
    "for pod, cameras in all_cameras.items():\n",
    "    if pod == 'front':\n",
    "        continue\n",
    "    print(cameras)\n",
    "    H = 2  # number of camera rows\n",
    "    W = 2  # number of camera cols\n",
    "    for i,seq_df in enumerate(seq_dfs):\n",
    "        cam_dfs = [seq_df[seq_df.camera_location == c] for c in cameras]\n",
    "        print(i, [len(cdf) for cdf in cam_dfs])\n",
    "        min_len = min(len(cdf) for cdf in cam_dfs)\n",
    "        cam_dfs = [cdf.sort_values('collected_on').iloc[:min_len] for cdf in cam_dfs]\n",
    "        if min_len < 2:\n",
    "            continue\n",
    "        # print(i, [cdf.iloc[0].collected_on for cdf in cam_dfs])\n",
    "\n",
    "        frame = read_raw_image_by_row(data_dir, seq_df.iloc[0])\n",
    "        height, width, layers = frame.shape\n",
    "        # print(height, width, layers)\n",
    "\n",
    "        # .avi MJPG,  .mp4 MP4V\n",
    "        video_dir = os.path.join(data_dir, f'videos_{len(cameras)}cams')\n",
    "        os.makedirs(video_dir, exist_ok=True)\n",
    "        video_name = os.path.join(video_dir, f'{pod}_seq{i}.mp4')\n",
    "        video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), 3, (width*W,height*H), isColor=True)\n",
    "\n",
    "        for ii in tqdm(range(min_len)):\n",
    "            try:\n",
    "                canvas = np.zeros((height*H, width*W, 3), dtype=np.uint8)\n",
    "                for ci in range(len(cam_dfs)):\n",
    "                    frame = read_raw_image_by_row(data_dir, cam_dfs[ci].iloc[ii])\n",
    "                    frame = cv2.putText(frame, f'{cam_dfs[ci].iloc[ii].collected_on}', \n",
    "                            (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "                    fi, fj = ci // W, ci % W\n",
    "                    canvas[height*fi:height*(fi+1), width*fj:width*(fj+1)] = frame\n",
    "                video.write(canvas)\n",
    "            except:\n",
    "                print(f'{ii}th image read failed')\n",
    "\n",
    "        # cv2.destroyAllWindows()\n",
    "        video.release()\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create video from saved pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(ids, pred_dir, video_name, read_func=read_saved_frame, fps=2):\n",
    "    frame = read_func(pred_dir, ids[10])\n",
    "    height, width, layers = frame.shape\n",
    "    print(height, width, layers)\n",
    "\n",
    "    # .avi MJPG,  .mp4 MP4V\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width,height), isColor=True)\n",
    "    \n",
    "    good = 0\n",
    "    for _id in tqdm(ids):\n",
    "        frame = read_func(pred_dir, _id)\n",
    "        if frame is not None:\n",
    "            video.write(frame)\n",
    "            good += 1\n",
    "    print('total', len(ids), 'used', good)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 3\n",
      "0 2021-11-10T00:33:05.931000 49\n",
      "311 3141 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:01<00:00, 37.81it/s]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 49 used 49\n",
      "1 2021-12-16T22:14:00.050000 71\n",
      "311 3141 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:01<00:00, 38.55it/s]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 71 used 71\n",
      "2 2022-11-10T23:19:10.901000 88\n",
      "311 3141 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:02<00:00, 40.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88 used 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/data/jupiter/li.yu/data/Jupiter_human_on_path_3_fn_sequence'\n",
    "# csv_file = '/data/jupiter/datasets/Jupiter_gilroy_reverse_manny_v2/master_annotations_1fps.csv'\n",
    "# csv_file = '/data/jupiter/li.yu/data/mannequin_in_dust_v1/master_annotations.csv'\n",
    "csv_file = f'{data_dir}/master_annotations.csv'\n",
    "master_df = pd.read_csv(csv_file)\n",
    "master_df = master_df.sort_values('collected_on')\n",
    "\n",
    "# pred_dir = '/mnt/sandbox1/rakhil.immidisetti/output/driveable_terrain_model/471_cloud_v45_cutnpaste_s35/Jupiter_gilroy_reverse_manny_v2reverse_val_bestmodel/'\n",
    "# pred_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model/v471_nobadiqnohard_6cls_cnp_alpha2_0805/Jupiter_gilroy_reverse_manny_v2/'\n",
    "# pred_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/mannequin_in_dust_v1'\n",
    "pred_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/Jupiter_human_on_path_3_fn_sequence'\n",
    "save_dir = pred_dir\n",
    "\n",
    "# # save as a single video\n",
    "# print(master_df.shape)\n",
    "# video_name = os.path.join(save_dir, 'pred.mp4')\n",
    "# ids = master_df.image_id.to_list()\n",
    "# create_video(ids, pred_dir, video_name, fps=3)\n",
    "\n",
    "# break into sequences\n",
    "seq_dfs = get_sequences(master_df, interval=5, per_camera=True)\n",
    "print(len(master_df), len(seq_dfs))\n",
    "video_dir = os.path.join(pred_dir, 'videos')\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "for si, seq_df in enumerate(seq_dfs):\n",
    "    if len(seq_df) < 10:\n",
    "        continue\n",
    "    name = seq_df.iloc[0].collected_on\n",
    "    camera = seq_df.iloc[0].camera_location[:-5]\n",
    "    print(si, name, len(seq_df))\n",
    "    \n",
    "    # create video\n",
    "    # video_name = os.path.join(video_dir, str(si).zfill(3)+'.mp4')\n",
    "    video_name = os.path.join(video_dir, f'{name}_{camera}_{si}.mp4')\n",
    "    ids = seq_df.image_id.to_list()\n",
    "    create_video(ids, pred_dir, video_name, fps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create video from PP artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_pp_artifacts(data_dir, df_row):\n",
    "    data_path = os.path.join(data_dir, df_row.stereo_pipeline_npz_save_path)\n",
    "    img = np.load(data_path)['left']\n",
    "    img_norm = normalize_image(img, df_row.hdr_mode if 'hdr_mode' in df_row else True)\n",
    "    return cv2.cvtColor((img_norm * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# def add_text(frame, txt_row):\n",
    "#     frame = cv2.putText(frame, f'Collected on: {txt_row.collected_on}', \n",
    "#                         (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "#     return frame\n",
    "\n",
    "def add_text(frame, txt):\n",
    "    frame = cv2.putText(frame, txt, \n",
    "                        (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "    return frame\n",
    "\n",
    "def add_texts(frame, txts: list):\n",
    "    txt_pw, txt_ph = 10, 25\n",
    "    for i, txt in enumerate(txts):\n",
    "        if 'false_positive' in txt:\n",
    "            frame = cv2.putText(frame, txt, \n",
    "                                (txt_pw, txt_ph+i*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            frame = cv2.putText(frame, txt, \n",
    "                                (txt_pw, txt_ph+i*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/li.yu/anaconda3/envs/pytorchlightning/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3437: DtypeWarning: Columns (37,46,58,60,74,86,101,103,106,116,118) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23023, 150) 432\n"
     ]
    }
   ],
   "source": [
    "# data_root_dir = '/data/jupiter/li.yu/data'\n",
    "data_root_dir = '/data/jupiter/datasets/'\n",
    "# dataset = 'mannequin_in_dust_v1'\n",
    "# dataset = 'Jupiter_human_on_path_3_fn_sequence'\n",
    "dataset = 'halo_missed_lo_rock_0509_stereo'\n",
    "data_dir = os.path.join(data_root_dir, dataset)\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, 'master_annotations.csv'))\n",
    "seq_dfs = get_sequences(df, interval=5, per_camera=True)\n",
    "print(df.shape, len(seq_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['18:16:02', '18:17:02'] 606\n",
      "['18:17:01', '18:18:01'] 750\n",
      "['18:17:48', '18:18:48'] 498\n",
      "['18:18:45', '18:19:45'] 354\n",
      "['18:21:31', '18:22:31'] 732\n",
      "['18:22:26', '18:23:26'] 900\n",
      "['18:23:12', '18:24:12'] 426\n",
      "['18:24:15', '18:25:15'] 432\n",
      "['20:16:20', '20:17:20'] 930\n",
      "['20:18:38', '20:19:38'] 850\n",
      "['20:20:28', '20:21:28'] 1200\n",
      "(6568, 151)\n",
      "(6568, 151) 2\n"
     ]
    }
   ],
   "source": [
    "bags = [\n",
    "[\"05_09_2024_18_16_02\", \"05_09_2024_18_17_02\"],\n",
    "[\"05_09_2024_18_17_01\", \"05_09_2024_18_18_01\"],\n",
    "[\"05_09_2024_18_17_48\", \"05_09_2024_18_18_48\"],\n",
    "[\"05_09_2024_18_18_45\", \"05_09_2024_18_19_45\"],\n",
    "[\"05_09_2024_18_21_31\", \"05_09_2024_18_22_31\"],\n",
    "[\"05_09_2024_18_22_26\", \"05_09_2024_18_23_26\"],\n",
    "[\"05_09_2024_18_23_12\", \"05_09_2024_18_24_12\"],\n",
    "[\"05_09_2024_18_24_15\", \"05_09_2024_18_25_15\"],\n",
    "[\"05_09_2024_20_16_20\", \"05_09_2024_20_17_20\"],\n",
    "[\"05_09_2024_20_18_38\", \"05_09_2024_20_19_38\"],\n",
    "[\"05_09_2024_20_20_28\", \"05_09_2024_20_21_28\"],\n",
    "]\n",
    "bags = [[bag[0][11:].replace('_', ':'), bag[1][11:].replace('_', ':')] for bag in bags]\n",
    "df['collected_hms'] = df['collected_on'].apply(lambda t: t[11:])\n",
    "df_oi = []\n",
    "for bag in bags:\n",
    "    sub_df = df[(df.collected_hms >= bag[0]) & (df.collected_hms < bag[1])]\n",
    "    print(bag, len(sub_df))\n",
    "    df_oi.append(sub_df)\n",
    "df_oi = pd.concat(df_oi, ignore_index=True).drop_duplicates()\n",
    "print(df_oi.shape)\n",
    "\n",
    "seq_dfs = get_sequences(df_oi)\n",
    "print(df_oi.shape, len(seq_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_id</th>\n      <th>id</th>\n      <th>camera_location</th>\n      <th>operation_time</th>\n      <th>special_notes</th>\n      <th>jdb_s3_path</th>\n      <th>result_state</th>\n      <th>result_human_state</th>\n      <th>result_vehicle_state</th>\n      <th>min_pixels_threshold</th>\n      <th>features</th>\n      <th>n_gt_human_pixels</th>\n      <th>gt_human_depth</th>\n      <th>n_pred_human_pixels</th>\n      <th>pred_human_depth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>663ec898321f043ed7ad8a62_T02_T03</td>\n      <td>663ec898321f043ed7ad8a62</td>\n      <td>T02</td>\n      <td>daytime</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>true_negative</td>\n      <td>true_negative</td>\n      <td>true_negative</td>\n      <td>108</td>\n      <td>{\"large_object_pixels\": 688, \"large_object_min...</td>\n      <td>0</td>\n      <td>1000</td>\n      <td>0</td>\n      <td>1000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                          unique_id                        id camera_location  \\\n0  663ec898321f043ed7ad8a62_T02_T03  663ec898321f043ed7ad8a62             T02   \n\n  operation_time special_notes  jdb_s3_path   result_state result_human_state  \\\n0        daytime           NaN          NaN  true_negative      true_negative   \n\n  result_vehicle_state  min_pixels_threshold  \\\n0        true_negative                   108   \n\n                                            features  n_gt_human_pixels  \\\n0  {\"large_object_pixels\": 688, \"large_object_min...                  0   \n\n   gt_human_depth  n_pred_human_pixels  pred_human_depth  \n0            1000                    0              1000  "
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model'\n",
    "models = [\n",
    "    'ds_v8_1_nextvit_small_openimages_with_rev1_train_human_test_using_random_val_mhc_20_epochs_finetune_rev1_lr',\n",
    "    'v81_80k_maxfov_wn_ft_kore_0430'\n",
    "]\n",
    "suffix_list = ['_mhc_depth0125', '']\n",
    "pred_dfs = [pd.read_csv(os.path.join(pred_dir, model, dataset+suffix, 'output.csv')) for model,suffix in zip(models, suffix_list)]\n",
    "pred_dfs[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/248 [00:00<00:29,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[248, 248, 248]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:30<00:00,  8.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_pred_texts(pred_dfs, unique_id, text_prefix):\n",
    "    texts = []\n",
    "    for pred_df, prefix in zip(pred_dfs, text_prefix):\n",
    "        pred_row = pred_df[pred_df.unique_id == unique_id].iloc[0]\n",
    "        texts.append(f'{prefix}: {pred_row.result_state}')\n",
    "    return texts\n",
    "\n",
    "def create_video_from_pp_add_text(seq_df, camera_pairs, pred_dfs, H, W, data_dir, video_dir_name):\n",
    "    cam_dfs = [seq_df[seq_df.unique_id.str.endswith(c)] for c in camera_pairs]\n",
    "    cam_dfs = [cdf.sort_values('collected_on', ignore_index=True) for cdf in cam_dfs]\n",
    "    min_len = min(len(cdf) for cdf in cam_dfs)\n",
    "    cam_dfs = [cdf.iloc[:min_len] for cdf in cam_dfs]\n",
    "    print([len(cdf) for cdf in cam_dfs])\n",
    "\n",
    "    # .avi MJPG,  .mp4 MP4V\n",
    "    os.makedirs(os.path.join(data_dir, video_dir_name), exist_ok=True)\n",
    "    video_name = os.path.join(data_dir, f'{video_dir_name}/{seq_df.iloc[0].collected_on}.mp4')\n",
    "\n",
    "    # print(f'{H} rows, {W} cols out of {len(camera_pairs)} camera_pairs')\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), 3, (768*W,512*H), isColor=True)\n",
    "\n",
    "    for ii in tqdm(range(min_len)):\n",
    "        # try:\n",
    "        canvas = np.zeros((512*H, 768*W, 3), dtype=np.uint8)\n",
    "        for ci in range(len(cam_dfs)):\n",
    "            row = cam_dfs[ci].iloc[ii]\n",
    "            frame = read_from_pp_artifacts(data_dir, row)\n",
    "            texts = [f'{row.collected_on}'] + get_pred_texts(pred_dfs, row.unique_id, ['MHC pred', 'MAXFOV pred'])\n",
    "            # print(texts)\n",
    "            frame = add_texts(frame, texts)\n",
    "            fi, fj = ci // W, ci % W\n",
    "            canvas[512*fi:512*(fi+1), 768*fj:768*fj+frame.shape[1]] = frame\n",
    "        video.write(canvas)\n",
    "        # except:\n",
    "        #     print(f'{ii}th image read failed')\n",
    "\n",
    "    # cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "front_pairs = ['T02_T04', 'T02_T03', 'T01_T03']\n",
    "left_pass_pairs = ['T02_T04', 'T02_T03', 'T01_T03', 'T13_T15', 'T14_T15', 'T14_T16']\n",
    "right_pass_pairs = ['T02_T04', 'T02_T03', 'T01_T03', 'T06_T08', 'T05_T07', 'T06_T07']\n",
    "# left_pass_pairs = ['T09_T11', 'T14_T16', 'T14_T15', 'T13_T15']\n",
    "# right_pass_pairs = ['T05_T07', 'T10_T12', 'T06_T08', 'T06_T07']\n",
    "H = 1  # number of image rows\n",
    "W = 3  # number of image cols\n",
    "create_video_from_pp_add_text(seq_dfs[1], front_pairs, pred_dfs, H, W, data_dir, 'videos_front_pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array(['T01_T03', 'T02_T03', 'T02_T04', 'T05_T07', 'T06_T07', 'T06_T08',\n       'T09_T11', 'T10_T12', 'T10_T11', 'T13_T15', 'T14_T16', 'T14_T15'],\n      dtype=object)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"camera_pair\"] = df[\"unique_id\"].apply(lambda t: t[-7:])\n",
    "df[\"camera_pair\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read frame and add prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 11)\n"
     ]
    }
   ],
   "source": [
    "# compare BRT model pred and CenterTrack pred\n",
    "# pred_csv = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/mannequin_in_dust_v1/output.csv'\n",
    "pred_csv = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/Jupiter_human_on_path_3_fn_sequence/output.csv'\n",
    "pred_df = pd.read_csv(pred_csv)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 2023-07-08T01:37:09.798000 153\n"
     ]
    }
   ],
   "source": [
    "video_with_pred_dir = os.path.join(data_root_dir, dataset, 'videos_with_pred')\n",
    "# pred_sequence_dir = '/home/li.yu/code/CenterTrack/results/2023-07-08T01:37:09.798000_front-center_15'\n",
    "pred_sequence_dir = '/home/li.yu/code/CenterTrack/results/brt50000/nopreimg_noprehm/2023-07-08T01:37:09.798000_front-center_15'\n",
    "os.makedirs(video_with_pred_dir, exist_ok=True)\n",
    "height, width = 512, 1024\n",
    "\n",
    "for si, seq_df in enumerate(seq_dfs):\n",
    "    if len(seq_df) < 10 or si != 15:\n",
    "        continue\n",
    "    name = seq_df.iloc[0].collected_on\n",
    "    camera = seq_df.iloc[0].camera_location[:-5]\n",
    "    print(si, name, len(seq_df))\n",
    "    \n",
    "    # merge pred from BRT model\n",
    "    seq_df = seq_df.drop(columns=['state']).merge(pred_df[['id', 'state', 'human_state']], on='id')\n",
    "\n",
    "    # create video\n",
    "    video_name = os.path.join(video_with_pred_dir, f'{name}_{camera}_{si}_nopreimg_noprehm.mp4')\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 3, (width,height), isColor=True)\n",
    "    fi = 0\n",
    "    for _, df_row in seq_df.iterrows():\n",
    "        frame = cv2.imread(os.path.join(pred_sequence_dir, str(fi).zfill(3)+'_'+df_row.id+'.png'))\n",
    "        frame = cv2.putText(frame, f'BRT Seg Pred: {df_row.state}, Strict: {df_row.human_state}', \n",
    "                            (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "        video.write(frame)\n",
    "        fi += 1\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read from video and prediction results to each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2022-11-10T23:19:10.901000 88\n"
     ]
    }
   ],
   "source": [
    "video_dir = '/home/li.yu/code/CenterTrack/results/'\n",
    "old_video_name = 'brt50000_2022-11-10T23:19:10.901000_side-right_2_preimg.mp4'\n",
    "new_video_name = 'brt50000_2022-11-10T23:19:10.901000_side-right_2_preimg_withbrtpred.mp4'\n",
    "height, width = 512, 1024\n",
    "\n",
    "for si, seq_df in enumerate(seq_dfs):\n",
    "    if len(seq_df) < 10 or si != 2:\n",
    "        continue\n",
    "    name = seq_df.iloc[0].collected_on\n",
    "    camera = seq_df.iloc[0].camera_location[:-5]\n",
    "    print(si, name, len(seq_df))\n",
    "    \n",
    "    # merge pred from BRT model\n",
    "    seq_df = seq_df.drop(columns=['state']).merge(pred_df[['id', 'state', 'human_state']], on='id')\n",
    "\n",
    "    # read video\n",
    "    cam = cv2.VideoCapture(os.path.join(video_dir, old_video_name))\n",
    "\n",
    "    # create video\n",
    "    video_name = os.path.join(video_dir, new_video_name)\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 3, (width,height), isColor=True)\n",
    "    for _, df_row in seq_df.iterrows():\n",
    "        _, frame = cam.read()\n",
    "        frame = cv2.putText(frame, f'BRT Seg Pred: {df_row.state}, Strict: {df_row.human_state}', \n",
    "                            (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "        video.write(frame)\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "    # break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorchlightning': conda)",
   "name": "python388jvsc74a57bd01eceddbeeb55f686303d64ef8e05e300429be7c506c9f9cad24a6dfe5f27b555"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "1eceddbeeb55f686303d64ef8e05e300429be7c506c9f9cad24a6dfe5f27b555"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}