{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "\n",
    "class ModelType(Enum):\n",
    "    CLASSIFICATION = 0\n",
    "    SEGMENTATION = 1\n",
    "\n",
    "classlabels_viz_colors = ['black', 'green', 'yellow', 'blue', 'red', 'magenta', 'cyan',\n",
    "                          'lightseagreen', 'brown', 'magenta', 'olive', 'wheat', 'white', 'black']\n",
    "classlabels_viz_bounds = [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 100]\n",
    "\n",
    "classlabels_viz_cmap = mpl.colors.ListedColormap(classlabels_viz_colors)\n",
    "classlabels_viz_norm = mpl.colors.BoundaryNorm(classlabels_viz_bounds, classlabels_viz_cmap.N)\n",
    "\n",
    "confidence_heatmap_viz_colors = ['black', 'blue', 'red', 'orange', 'yellow', 'lightgreen', 'lightseagreen']\n",
    "confidence_heatmap_viz_bounds = [-1, 0,0.5,0.6,0.7,0.8,0.9,1]\n",
    "confidence_heatmap_viz_cmap = mpl.colors.ListedColormap(confidence_heatmap_viz_colors)\n",
    "confidence_heatmap_viz_norm = mpl.colors.BoundaryNorm(confidence_heatmap_viz_bounds, confidence_heatmap_viz_cmap.N)\n",
    "\n",
    "\n",
    "LabelColor = namedtuple('LabelColor', ['name', 'id', 'trainid', 'color', 'category'])\n",
    "\n",
    "LABEL_COLORS = [\n",
    "    LabelColor('class1', 1, 0, (128, 0, 128), 'driveableterrain'),\n",
    "    LabelColor('class2', 2, 1, (255, 0, 0), 'non-driveableterrain'),\n",
    "    LabelColor('class3', 3, 2, (0, 0, 255), 'sky'),\n",
    "    LabelColor('class4', 4, 3, (0, 255, 0), 'trees'),\n",
    "    LabelColor('class5', 5, 4, (255, 0, 255), 'implement'),\n",
    "    LabelColor('class6', 6, 5, (255, 255, 0), 'basket markers')\n",
    "]\n",
    "\n",
    "LABEL_COLORS_4CLASS = LABEL_COLORS[0:4]\n",
    "LABEL_COLORS_5CLASS = LABEL_COLORS[0:5]\n",
    "LABEL_COLORS_6CLASS = LABEL_COLORS[0:6]\n",
    "LABEL_COLORS_SKY_DET = [LABEL_COLORS[0], LABEL_COLORS[2]]\n",
    "\n",
    "LABEL_COLORS_IMPL = [\n",
    "    LabelColor('class1', 1, 1, (128, 0, 128), 'implement'),\n",
    "    LabelColor('class2', 2, 2, (255, 0, 0), 'sweep'),\n",
    "    LabelColor('class3', 3, 3, (0, 0, 255), 'harrow_tine'),\n",
    "    LabelColor('class4', 4, 4, (0, 255, 0), 'basket'),\n",
    "    LabelColor('class5', 5, 5, (0, 255, 0), 'basket_marker'),\n",
    "    LabelColor('class6', 0, 255, (0, 0, 0), 'ignore')\n",
    "]\n",
    "\n",
    "LABEL_COLORS_IMPL_REDUCED = [\n",
    "    LabelColor('class0', 1, 0, (0, 0, 0), 'background'),\n",
    "    LabelColor('class1', 2, 1, (0, 255, 0), 'implement'),\n",
    "    LabelColor('class2', 3, 2, (255, 0, 0), 'sweep'),\n",
    "    LabelColor('class3', 4, 3, (0, 255, 0), 'basket_marker'),\n",
    "    LabelColor('class6', 0, 255, (0, 0, 0), 'ignore')\n",
    "]\n",
    "\n",
    "PLUG_LABEL_MAP ={0: 'no-plug', 1: 'plug'}\n",
    "IMPL_SEGMENT_LABEL_MAP = {0: 'background', 1: 'implement', 2: 'sweep', 3:'basket_marker'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mpl_viz_outputs(output_path,\n",
    "                           image,\n",
    "                           prediction_labels,\n",
    "                           confidences,\n",
    "                           depth_img, \n",
    "                           image_title='Image', \n",
    "                           pred_title='Prediction', \n",
    "                           conf_title='Confidence', \n",
    "                           depth_title='Depth',\n",
    "                           bbox_coords=[]):\n",
    "    \"\"\"\n",
    "    Utility function to plot results based on order of input provided.\n",
    "\n",
    "    Axes index can have different values based on input provided.\n",
    "\n",
    "    For example: If image, prediction and groundtruth_label is provided, A three axes plot will be generated with\n",
    "    image(ax1), ground_truth(ax2), prediction(ax3).\n",
    "\n",
    "    Order of plot if all the inputs are provided will be in same order as arguments listed above.\n",
    "    \"\"\"\n",
    "    axis_index = list(range(len(list(filter(lambda x: x is not None, [image,\n",
    "                                                                      prediction_labels, confidences,\n",
    "                                                                      depth_img])))))\n",
    "    axis_curr_index = 0\n",
    "    fig, axes = mpl.pyplot.subplots(1, len(axis_index), figsize=((60, 30)))\n",
    "    if bbox_coords:\n",
    "#         xmin, ymin, xmax, ymax = bbox_coords\n",
    "        xmin, xmax, ymin, ymax = bbox_coords\n",
    "        if (xmin < 0) or (ymin < 0):\n",
    "            raise ValueError(f'Either {xmin} or {ymin} are negative')\n",
    "        else:\n",
    "            rect = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',\n",
    "                                     facecolor='none')\n",
    "    else:\n",
    "        rect = None\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        # Grab only RGB channels from image, otherwise depth with distort the image when it is displayed\n",
    "        axes[axis_curr_index].imshow(image)\n",
    "        axes[axis_curr_index].set_title(image_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        axes[axis_curr_index].imshow(depth_img, cmap='turbo')\n",
    "        axes[axis_curr_index].set_title(depth_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        axes[axis_curr_index].imshow(prediction_labels, classlabels_viz_cmap, classlabels_viz_norm, interpolation='nearest')\n",
    "        if rect is not None:\n",
    "            rect1 = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',facecolor='none')\n",
    "            axes[axis_curr_index].add_patch(rect1)\n",
    "        axes[axis_curr_index].set_title(pred_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        c = np.max(confidences, axis=2)\n",
    "        axes[axis_curr_index].imshow(c, confidence_heatmap_viz_cmap, confidence_heatmap_viz_norm, interpolation='nearest')\n",
    "#         if rect is not None:\n",
    "#             rect2 = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',\n",
    "#                                           facecolor='none')\n",
    "#             axes[axis_curr_index].add_patch(rect2)\n",
    "        axes[axis_curr_index].set_title(conf_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    mpl.pyplot.savefig(output_path, pad_inches=0, bbox_inches='tight', dpi=150)\n",
    "    mpl.pyplot.close('all')\n",
    "\n",
    "def read_image(image_path):\n",
    "    image = (np.load(image_path) * 255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "def normalize_and_clip_depth(depth, max_depth):\n",
    "    \"\"\"\n",
    "    Return an optionally normalized (and clipped) depth.\n",
    "    \"\"\"\n",
    "    depth[np.isnan(depth)] = max_depth\n",
    "    depth[depth > max_depth] = max_depth\n",
    "    depth = ((depth) / max_depth).astype(np.float32)\n",
    "    return depth\n",
    "\n",
    "DEFAULT_TONEMAP_PARAMS = {\"policy\": \"tonemap\", \"alpha\": 0.25, \"beta\": 0.9, \"gamma\": 0.9, \"eps\": 1e-6}\n",
    "def normalize_image(image, hdr_mode=True, normalization_params=DEFAULT_TONEMAP_PARAMS, return_8_bit=False):\n",
    "    \"\"\"\n",
    "    Normalize an 8 bit image according to the specified policy.\n",
    "    If return_8_bit, this returns an np.uint8 image, otherwise it returns a floating point\n",
    "    image with values in [0, 1].\n",
    "    \"\"\"\n",
    "    normalization_policy = normalization_params['policy']\n",
    "    lower_bound = 0\n",
    "    upper_bound = 1\n",
    "    if np.isnan(hdr_mode):\n",
    "        hdr_mode = False\n",
    "\n",
    "    if hdr_mode and image.dtype == np.uint8:\n",
    "        # The image was normalized during pack-perception (tonemap)\n",
    "        if return_8_bit:\n",
    "            return image\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 255.0\n",
    "    elif normalization_policy == \"percentile\" and hdr_mode:\n",
    "        lower_bound = np.array([np.percentile(image[..., i],\n",
    "                                              normalization_params['lower_bound'],\n",
    "                                              interpolation='lower')\n",
    "                                for i in range(3)])\n",
    "        upper_bound = np.array([np.percentile(image[..., i],\n",
    "                                              normalization_params['upper_bound'],\n",
    "                                              interpolation='lower')\n",
    "                                for i in range(3)])\n",
    "    elif normalization_policy == \"percentile_vpu\" and hdr_mode:\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        brightness = (3 * r + b + 4 * g) / 8\n",
    "        lower_bound = np.percentile(brightness, normalization_params['lower_bound'],\n",
    "                                    interpolation='lower')\n",
    "        upper_bound = np.percentile(brightness, normalization_params['upper_bound'],\n",
    "                                    interpolation='lower')\n",
    "    elif normalization_policy == \"3sigma\" and hdr_mode:\n",
    "        sigma_size = normalization_params['sigma_size']\n",
    "        min_variance = normalization_params['min_variance']\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        brightness = (3 * r + b + 4 * g) / 8\n",
    "        mean, sigma = np.mean(brightness), np.std(brightness)\n",
    "        brightness_min, brightness_max = np.min(brightness), np.max(brightness)\n",
    "        if (sigma * sigma_size) > mean:\n",
    "            lmin = brightness_min\n",
    "            lmax = min(brightness_max, mean * sigma_size)\n",
    "            if (lmax - lmin) < min_variance:\n",
    "                lmax = lmin + min_variance\n",
    "            lower_bound = lmin\n",
    "            upper_bound = lmax\n",
    "        else:\n",
    "            mean_var = mean - sigma_size * sigma\n",
    "            output_min = max(brightness_min, mean_var)\n",
    "            mean_var = mean + sigma_size * sigma\n",
    "            output_max = min(brightness_max, mean_var)\n",
    "            if (output_max - output_min) < min_variance:\n",
    "                output_min = mean - min_variance / 2.0\n",
    "                output_min = 0 if output_min < 0 else output_min\n",
    "                output_max = output_min + min_variance\n",
    "            lower_bound = output_min\n",
    "            upper_bound = output_max\n",
    "    elif normalization_policy == 'tonemap' and hdr_mode:\n",
    "        if image.dtype != np.float32 and image.dtype != np.uint32:\n",
    "            raise ValueError('HDR image type is {} instead of float32 or uint32'.format(image.dtype))\n",
    "        alpha = normalization_params.get('alpha', DEFAULT_TONEMAP_PARAMS['alpha'])\n",
    "        beta = normalization_params.get('beta', DEFAULT_TONEMAP_PARAMS['beta'])\n",
    "        gamma = normalization_params.get('gamma', DEFAULT_TONEMAP_PARAMS['gamma'])\n",
    "        eps = normalization_params.get('eps', DEFAULT_TONEMAP_PARAMS['eps'])\n",
    "\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        lum_in = 0.2126 * r + 0.7152 * g + 0.0722 * b\n",
    "        lum_norm = np.exp(gamma * np.mean(np.log(lum_in + eps)))\n",
    "        c = alpha * lum_in / lum_norm\n",
    "        c_max = beta * np.max(c)\n",
    "        lum_out = c / (1 + c) * (1 + c / (c_max ** 2))\n",
    "        image = image * (lum_out / (lum_in + eps))[..., None]\n",
    "    elif normalization_policy == \"none\" and hdr_mode:\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 2**20 - 1\n",
    "    elif normalization_policy == \"default\" or not hdr_mode:\n",
    "        assert np.max(image) <= 255 and np.min(image) >= 0, \"Image with default \" \\\n",
    "            \"mode should be in range [0,255]\"\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 255.0\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"--normalization-policy '{normalization_policy}' is not supported! \"\n",
    "            f\"(on image with hdr_mode={hdr_mode})\")\n",
    "\n",
    "    image = (image.astype(np.float32, copy=False) - lower_bound) / (upper_bound - lower_bound)\n",
    "\n",
    "    if return_8_bit:\n",
    "        image = np.clip(image * 255.0, 0.0, 255.0)\n",
    "        image = np.uint8(image)\n",
    "    else:\n",
    "        image = np.clip(image, 0.0, 1.0)\n",
    "\n",
    "    return image\n",
    "\n",
    "def read_saved_frame(pred_dir, image_id):\n",
    "    states_to_save = ['', 'false_positive', 'false_negative', 'large_object_false_negative', 'true_positive', 'true_negative']\n",
    "    frame = None\n",
    "    for state in states_to_save:\n",
    "        if os.path.isfile(os.path.join(pred_dir, state, image_id+'.png')):\n",
    "            frame = cv2.imread(os.path.join(pred_dir, state, image_id+'.png'))\n",
    "            break\n",
    "        if os.path.isfile(os.path.join(pred_dir, state, image_id+'.jpg')):\n",
    "            frame = cv2.imread(os.path.join(pred_dir, state, image_id+'.jpg'))\n",
    "            break\n",
    "    return frame\n",
    "\n",
    "def read_images(pred_dir, _id):\n",
    "    if not os.path.isfile(os.path.join(pred_dir, _id+'_image.npy')):\n",
    "        return None, None, None, None\n",
    "    image = np.load(os.path.join(pred_dir, _id+'_image.npy'))\n",
    "    \n",
    "    # 100m capped depth\n",
    "    depth = np.load(os.path.join(pred_dir, _id+'_depth.npy'))\n",
    "#     # raw depth\n",
    "#     raw_depth_dir = '/raum_raid/li.yu/data/Jupiter_rock_demo_2021/Jupiter_rock_demo_loamy06_Oct20_2021/model_processed_v4.1_sky_2e-3_lr_1e-3_color_aug_full_model_LR_consistency_regularization_0.2_epoch_23/images/'\n",
    "#     stereo_data = np.load(os.path.join(raw_depth_dir, _id, 'stereo_output.npz'))\n",
    "#     depth = stereo_data['point_cloud'][:,:,-1]\n",
    "#     depth = normalize_and_clip_depth(depth, 200)\n",
    "    \n",
    "    pred_label = np.load(os.path.join(pred_dir, _id+'_pred_label.npy'))\n",
    "    confidence = np.load(os.path.join(pred_dir, _id+'_confidence.npy'))\n",
    "    return image, depth, pred_label, confidence\n",
    "\n",
    "# def create_frame(pred_dir, pred_merged_dir, _id, recreate=False):\n",
    "#     canvas_path = os.path.join(pred_merged_dir, _id+'.png')\n",
    "#     if recreate:\n",
    "#         image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "#         if image is None:\n",
    "#             return None\n",
    "#         create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth)\n",
    "#     frame = cv2.imread(canvas_path)\n",
    "#     return frame\n",
    "\n",
    "def get_bbox_coords(i=-1, bbox_range_list=[], bbox_coord_list=[]):\n",
    "    for bi in range(len(bbox_range_list)):\n",
    "        bbox_range = bbox_range_list[bi]\n",
    "        if bbox_range[0] <= i <= bbox_range[1]:\n",
    "            return bbox_coord_list[bi]\n",
    "    return []\n",
    "\n",
    "def process_frame(pred_dir, pred_merged_dir, _id, recreate=False, bbox_coords=[]):\n",
    "    image, depth, pred_label, confidence = None, None, None, None\n",
    "    l = 0.0\n",
    "    avg_pixel = 0.0\n",
    "    bbox_conf = None\n",
    "    if recreate:\n",
    "        image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "        if image is None:\n",
    "            return image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "        # calculate brightness\n",
    "        hlsImg = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "        l = np.average(hlsImg[:,:,1])\n",
    "        image_title = 'Image (brightness: {:.4f})'.format(l)\n",
    "        # calculate average pixel value at bbox area\n",
    "        if bbox_coords:\n",
    "            ymin, ymax, xmin, xmax = bbox_coords\n",
    "            bbox_pred = pred_label[ymin:ymax+1, xmin:xmax+1]\n",
    "            bbox_conf = confidence[ymin:ymax+1, xmin:xmax+1]\n",
    "            avg_pixel = np.count_nonzero(bbox_pred == 1)\n",
    "    return image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "\n",
    "def create_frame(pred_dir, pred_merged_dir, _id, recreate=False, bbox_coords=[]):\n",
    "    canvas_path = os.path.join(pred_merged_dir, _id+'.png')\n",
    "    image, depth, pred_label, confidence = None, None, None, None\n",
    "    l = 0.0\n",
    "    avg_pixel = 0.0\n",
    "    bbox_conf = None\n",
    "    if recreate:\n",
    "        image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "        if image is None:\n",
    "            return None, None, None, None, None, None, None, None\n",
    "        # calculate brightness\n",
    "        hlsImg = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "        l = np.average(hlsImg[:,:,1])\n",
    "        image_title = 'Image (brightness: {:.4f})'.format(l)\n",
    "        # calculate average pixel value at bbox area\n",
    "        if bbox_coords:\n",
    "            ymin, ymax, xmin, xmax = bbox_coords\n",
    "            bbox_pred = pred_label[ymin:ymax+1, xmin:xmax+1]\n",
    "            bbox_conf = confidence[ymin:ymax+1, xmin:xmax+1]\n",
    "            avg_pixel = np.count_nonzero(bbox_pred == 1)\n",
    "        create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth, image_title=image_title, bbox_coords=bbox_coords)\n",
    "    frame = cv2.imread(canvas_path)\n",
    "    return frame, image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "\n",
    "def create_diff_frame(pred_merged_dir, _id, image1, depth1, pred_label1, confidence1, \n",
    "                      image2, depth2, pred_label2, confidence2, pred_title='prediction', conf_title='confidence'):\n",
    "    canvas_path = os.path.join(pred_merged_dir, _id+'_diff.png')\n",
    "    image = np.abs(image1 - image2)\n",
    "    depth = np.abs(depth1 - depth2)\n",
    "    pred_label = np.abs(pred_label1 - pred_label2)\n",
    "    confidence = np.abs(confidence1 - confidence2)\n",
    "    create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth, conf_title=conf_title)\n",
    "    frame = cv2.imread(canvas_path)\n",
    "    return frame\n",
    "\n",
    "def read_raw_image_by_id(data_dir, _id):\n",
    "    image_path = os.path.join(data_dir, 'images', _id, 'artifact_debayeredrgb_0_'+_id+'.png')\n",
    "    image = cv2.imread(image_path)\n",
    "    return image\n",
    "\n",
    "def read_raw_image_by_row(data_dir, row):\n",
    "    image = cv2.imread(os.path.join(data_dir, row.artifact_debayeredrgb_0_save_path))\n",
    "    return image\n",
    "\n",
    "def create_video(ids, pred_dir, video_name, read_func=read_saved_frame, fps=2):\n",
    "    frame = read_func(pred_dir, ids[10])\n",
    "    height, width, layers = frame.shape\n",
    "    print(height, width, layers)\n",
    "\n",
    "    # .avi MJPG,  .mp4 MP4V\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width,height), isColor=True)\n",
    "    \n",
    "    good = 0\n",
    "    for _id in tqdm(ids):\n",
    "        frame = read_func(pred_dir, _id)\n",
    "        if frame is not None:\n",
    "            video.write(frame)\n",
    "            good += 1\n",
    "    print('total', len(ids), 'used', good)\n",
    "\n",
    "    # cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_sequences(df, interval=5*60, per_camera=False):\n",
    "    df = df.sort_values('collected_on')\n",
    "    df['datetime'] = df.collected_on.apply(datetime.fromisoformat)\n",
    "    sequence_dfs = []\n",
    "    delta = timedelta(seconds=interval)\n",
    "    start = True\n",
    "    i0, i = 0, 0\n",
    "    while i < len(df):\n",
    "        if start:\n",
    "            t0 = df.iloc[i].datetime\n",
    "            start = False\n",
    "        else:\n",
    "            t1 = df.iloc[i].datetime\n",
    "            if t1 - t0 > delta or i == len(df) - 1:\n",
    "                chunk_df = df.iloc[i0 : i if i < len(df) - 1 else len(df)]\n",
    "                if per_camera:\n",
    "                    camera_locations = chunk_df.camera_location.unique()\n",
    "                    camera_locations.sort()\n",
    "                    for camera_location in camera_locations:\n",
    "                        sequence_df = chunk_df[chunk_df.camera_location == camera_location]\n",
    "                        sequence_df = sequence_df.sort_values('collected_on')\n",
    "                        sequence_dfs.append(sequence_df)\n",
    "                else:\n",
    "                    sequence_dfs.append(chunk_df)\n",
    "                start = True\n",
    "                i0 = i\n",
    "            else:\n",
    "                t0 = t1\n",
    "        i += 1\n",
    "    return sequence_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create video from raw left images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39034, 106) 13\n"
     ]
    }
   ],
   "source": [
    "# create video from raw left images\n",
    "# data_dir = '/data/jupiter/datasets/halo_failure_case_of_box_in_dust'\n",
    "# data_dir = '/data/jupiter/datasets/halo_missed_lo_rock_0509_stereo'\n",
    "# data_dir = '/data/jupiter/li.yu/data/halo_sample_terrains_15images'\n",
    "# data_dir = '/data/jupiter/li.yu/data/halo_sample_terrains_15images_all_camera_sequence'\n",
    "# data_dir = '/data/jupiter/datasets/halo_human_in_dust_day_collection_may29'\n",
    "# data_dir = '/data/jupiter/datasets/halo_human_in_dust_night_collection_june03'\n",
    "# data_dir = '/data/jupiter/datasets/halo_human_in_dust_night_collection_june03_2'\n",
    "# data_dir = '/data/jupiter/datasets/halo_human_in_dust_day_collection_back_june05'\n",
    "# data_dir = '/data/jupiter/datasets/halo_human_in_dust_dusk_collection_front_june07'\n",
    "# data_dir = '/data/jupiter/datasets/halo_vehicles_in_dust_collection_march2024'\n",
    "# data_dir = '/data/jupiter/datasets/halo_vehicles_in_dust_collection_june06'\n",
    "data_dir = '/data/jupiter/datasets/halo_vehicles_in_dust_collection_june07'\n",
    "df = pd.read_csv(os.path.join(data_dir, 'annotations.csv'))\n",
    "df = df.sort_values('collected_on')\n",
    "\n",
    "# ids = df.id.to_list()\n",
    "# video_name = os.path.join(data_dir, f'video.mp4')\n",
    "# create_video(ids, data_dir, video_name, read_raw_image_by_id, fps=1)\n",
    "\n",
    "seq_dfs = get_sequences(df, interval=60)  # break the data by intervals between sequences\n",
    "print(df.shape, len(seq_dfs))\n",
    "\n",
    "# for i,sub_df in enumerate(seq_dfs):\n",
    "#     ids = sub_df.id.to_list()\n",
    "#     video_name = os.path.join(data_dir, f'dust_seq{i}.mp4')\n",
    "#     create_video(ids, data_dir, video_name, read_raw_image_by_id, fps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select images for labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halo_human_in_dust_day_collection_may29 (48102, 108) day\n",
      "halo_human_in_dust_night_collection_june03_2 (30284, 109) 19 {'dawn_dusk': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'night': [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]}\n",
      "halo_human_in_dust_day_collection_back_june05 (11675, 108) day\n",
      "halo_human_in_dust_dusk_collection_front_june07 (36338, 104) dawn_dusk\n",
      "(126399, 2)\n"
     ]
    }
   ],
   "source": [
    "# combine and create datasets for human in dust\n",
    "data_root_dir = '/data/jupiter/datasets'\n",
    "unlabeled_datasets = [\n",
    "    \"halo_human_in_dust_day_collection_may29\",\n",
    "    \"halo_human_in_dust_night_collection_june03\",  # for lying down humans\n",
    "    \"halo_human_in_dust_night_collection_june03_2\",\n",
    "    \"halo_human_in_dust_day_collection_back_june05\",\n",
    "    \"halo_human_in_dust_dusk_collection_front_june07\",\n",
    "]\n",
    "day_night_splits = ['day', 'dawn_dusk', {'dawn_dusk': list(range(0, 9)), 'night': list(range(9, 19))}, 'day', 'dawn_dusk']\n",
    "use_ds = [0, 2, 3, 4]\n",
    "# split data by day time\n",
    "image_ids_by_time = {'id':[], 'day_night_split': []}\n",
    "for di in use_ds:\n",
    "    dataset = unlabeled_datasets[di]\n",
    "    df = pd.read_csv(os.path.join(data_root_dir, dataset, 'annotations.csv'))\n",
    "    df = df.sort_values('collected_on')\n",
    "    day_night_split = day_night_splits[di]\n",
    "    if isinstance(day_night_split, str):\n",
    "        print(dataset, df.shape, day_night_split)\n",
    "        image_ids_by_time['id'] += df.id.to_list()\n",
    "        image_ids_by_time['day_night_split'] += [day_night_split] * len(df)\n",
    "    else:\n",
    "        seq_dfs = get_sequences(df, interval=60)  # break the data by intervals between sequences\n",
    "        print(dataset, df.shape, len(seq_dfs), day_night_split)\n",
    "        for dn_split, seq_ids in day_night_split.items():\n",
    "            for seq_id in seq_ids:\n",
    "                seq_df = seq_dfs[seq_id]\n",
    "                image_ids_by_time['id'] += seq_df.id.to_list()\n",
    "                image_ids_by_time['day_night_split'] += [dn_split] * len(seq_df)\n",
    "# save to csv\n",
    "day_night_split_df = pd.DataFrame(data=image_ids_by_time)\n",
    "print(day_night_split_df.shape)\n",
    "day_night_split_df.to_csv('/data/jupiter/li.yu/data/halo_hard_cases/halo_human_in_dust_daynightsplit_for_july01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176217, 2)\n"
     ]
    }
   ],
   "source": [
    "# image_ids_by_time = {'id':[], 'day_night_split': []}\n",
    "# split data in day/dusk/night\n",
    "day_night_splits = [{'day': list(range(0,25)), 'dawn_dusk': list(range(25, 31)), 'night': list(range(31, 38))}, \n",
    "                   {'dawn_dusk': list(range(0, 14)), 'night': list(range(14, 29))},\n",
    "                   {'dawn_dusk': [0, 1, 2, 7, 8, 9], 'night': [3, 4, 5, 6, 10, 11, 12]}]\n",
    "day_night_split = day_night_splits[2]\n",
    "for dn_split, seq_ids in day_night_split.items():\n",
    "    for seq_id in seq_ids:\n",
    "        seq_df = seq_dfs[seq_id]\n",
    "        image_ids_by_time['id'] += seq_df.id.to_list()\n",
    "        image_ids_by_time['day_night_split'] += [dn_split] * len(seq_df)\n",
    "# save to csv\n",
    "day_night_split_df = pd.DataFrame(data=image_ids_by_time)\n",
    "print(day_night_split_df.shape)\n",
    "day_night_split_df.to_csv('/data/jupiter/li.yu/data/halo_hard_cases/halo_vehicle_in_dust_daynightsplit_excludebadnight_for_july01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>day_night_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65e9e939e879f487a282d576</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65e9e9371d130730de6a5943</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id day_night_split\n",
       "0  65e9e939e879f487a282d576             day\n",
       "1  65e9e9371d130730de6a5943             day"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_night_split_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for selection of images for binary labeling - halo_failure_case_of_box_in_dust\n",
    "# selected = [3, 5, [9, 1/5], [10, 1/5], [11, 1/6], \n",
    "#     [12, [['2024-04-23T18:11:28', '2024-04-23T18:18:12'], ['2024-04-23T18:27:40', '2024-04-23T18:28:38'], ['2024-04-23T18:29:05', '2024-04-23T18:29:57'], \n",
    "#         ['2024-04-23T18:48:44', '2024-04-23T18:50:59'], ['2024-04-23T19:01:50', '2024-04-23T19:03:37']]], \n",
    "#     [13, [['2024-04-23T19:47:48', '2024-04-23T19:47:59']]], \n",
    "#     [14, [['2024-04-23T20:12:40', '2024-04-23T20:13:02'], ['2024-04-23T20:15:53', '2024-04-23T20:17:49'], ['2024-04-23T20:22:19', '2024-04-23T20:22:45'], \n",
    "#         ['2024-04-23T20:32:47', '2024-04-23T20:33:18'], ['2024-04-23T20:34:48', '2024-04-23T20:35:56'], ['2024-04-23T20:38:00', '2024-04-23T20:38:17'], \n",
    "#         ['2024-04-23T20:39:55', '2024-04-23T20:41:31'], ['2024-04-23T20:42:57', '2024-04-23T20:48:27']]], \n",
    "#     [16, 1/2]]\n",
    "# selected_ids = []\n",
    "# for s in selected:\n",
    "#     if isinstance(s, int):\n",
    "#         selected_ids += seq_dfs[s].id.to_list()\n",
    "#     else:\n",
    "#         si, sj = s\n",
    "#         if isinstance(sj, float):\n",
    "#             selected_ids += seq_dfs[si].iloc[:int(len(seq_dfs[si])*sj)].id.to_list()\n",
    "#         else:\n",
    "#             for t1, t2 in sj:\n",
    "#                 selected_ids += seq_dfs[si][(seq_dfs[si].collected_on >= t1) & (seq_dfs[si].collected_on <= t2)].id.to_list()\n",
    "#     # print(s, len(selected_ids))\n",
    "# selected_ids = list(set(selected_ids))\n",
    "# selected_df = pd.DataFrame(data={'id': selected_ids})\n",
    "# print(selected_df.shape)\n",
    "# selected_df.to_csv('/data/jupiter/datasets/halo_failure_case_of_box_in_dust/selected_for_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_skipped_human_label(data_dir, seq_dfs, suffix, same_human_sequence):\n",
    "    all_cameras = {'front': ['T01', 'T02', 'T03', 'T04'], 'right': ['T05', 'T06', 'T07', 'T08'], 'back': ['T09', 'T10', 'T11', 'T12'], 'left': ['T13', 'T14', 'T15', 'T16']}\n",
    "\n",
    "    raw_ms_df = pd.read_csv(os.path.join(data_dir, 'master_annotations.csv'))\n",
    "    raw_ms_df['camera_pair'] = raw_ms_df['unique_id'].apply(lambda s: s[-7:])\n",
    "    raw_ms_df['rectified_label_save_path'] = ''\n",
    "    raw_ms_df['use_recovered_label_in_sequence'] = True\n",
    "    labeled_ms_df = pd.read_csv(os.path.join(data_dir+suffix, 'master_annotations.csv'))\n",
    "    labeled_ms_df.drop(columns=[\"label_counts\"], inplace=True)\n",
    "    labeled_ms_df['camera_pair'] = labeled_ms_df['unique_id'].apply(lambda s: s[-7:])\n",
    "    print(raw_ms_df.shape, labeled_ms_df.shape)\n",
    "\n",
    "    recovered_dfs = []\n",
    "    for pod, seq_ids in same_human_sequence.items():\n",
    "        for seq_id in seq_ids:\n",
    "            # get seq df in pod\n",
    "            seq_df = seq_dfs[seq_id]\n",
    "            seq_df = seq_df[seq_df.camera_location.isin(all_cameras[pod])]\n",
    "            # get corresponding raw seq df and labeled seq df\n",
    "            raw_seq_df = raw_ms_df[raw_ms_df.id.isin(seq_df.id)]\n",
    "            labeled_seq_df = labeled_ms_df[labeled_ms_df.id.isin(seq_df.id)]\n",
    "            labeled_seq_df = labeled_seq_df.sort_values('collected_on')\n",
    "            # get camera locations where there are human labels\n",
    "            labeled_camera_pairs = labeled_seq_df.camera_pair.unique()\n",
    "            for camera_pair in labeled_camera_pairs:\n",
    "                raw_seq_cp_df = raw_seq_df[raw_seq_df.camera_pair == camera_pair]\n",
    "                labeled_seq_cp_df = labeled_seq_df[labeled_seq_df.camera_pair == camera_pair]\n",
    "                # assign label path to raw df\n",
    "                for i, row in raw_seq_cp_df.iterrows():\n",
    "                    labeled_rows = labeled_seq_cp_df[labeled_seq_cp_df.unique_id == row.unique_id]\n",
    "                    if len(labeled_rows) > 0:\n",
    "                        raw_seq_cp_df.loc[i, 'rectified_label_save_path'] = labeled_rows.iloc[0].rectified_label_save_path\n",
    "                        raw_seq_cp_df.loc[i, 'use_recovered_label_in_sequence'] = False\n",
    "                    else:\n",
    "                        raw_seq_cp_df.loc[i, 'rectified_label_save_path'] = labeled_seq_cp_df.iloc[0].rectified_label_save_path\n",
    "                recovered_dfs.append(raw_seq_cp_df)\n",
    "            print(pod, seq_id, len(seq_df), len(raw_seq_df), len(labeled_seq_df), raw_seq_df.camera_pair.unique(), labeled_seq_df.camera_pair.unique())\n",
    "    recovered_df = pd.concat(recovered_dfs, ignore_index=True)\n",
    "    print(recovered_df.shape, labeled_ms_df.shape)\n",
    "\n",
    "    # remove duplicates and add in extra sequence images from labeled_ms_df\n",
    "    recovered_df = pd.concat([recovered_df.drop_duplicates(subset='unique_id'), labeled_ms_df[~labeled_ms_df.unique_id.isin(recovered_df.unique_id)]], ignore_index=True)\n",
    "    # change relative path to label path\n",
    "    recovered_df['rectified_label_save_path'] = recovered_df['rectified_label_save_path'].apply(lambda p: f'../{os.path.basename(data_dir)+suffix}/{p}')\n",
    "    recovered_df['label_map'] = labeled_ms_df.iloc[0].label_map\n",
    "    print(recovered_df.shape, labeled_ms_df.shape)\n",
    "    recovered_df.to_csv(os.path.join(data_dir, 'label_recovered_master_annotations.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-56d6a59a9e9d>:34: DtypeWarning: Columns (25) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  recover_skipped_human_label(data_dir, seq_dfs, suffix, same_human_sequence)\n",
      "/home/li.yu/anaconda3/envs/pytorchlightning/lib/python3.8/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35854, 153) (5967, 275)\n",
      "front 0 599 446 246 ['T01_T03' 'T02_T03' 'T02_T04'] ['T02_T04' 'T02_T03']\n",
      "front 1 219 164 74 ['T01_T03' 'T02_T03' 'T02_T04'] ['T02_T04' 'T02_T03']\n",
      "front 3 145 108 42 ['T01_T03' 'T02_T03' 'T02_T04'] ['T02_T04' 'T02_T03']\n",
      "front 14 1156 867 284 ['T01_T03' 'T02_T04' 'T02_T03'] ['T01_T03' 'T02_T03' 'T02_T04']\n",
      "right 11 173 120 117 ['T05_T07' 'T06_T07' 'T06_T08'] ['T06_T08' 'T06_T07' 'T05_T07']\n",
      "right 12 951 709 705 ['T05_T07' 'T06_T07' 'T06_T08'] ['T06_T08' 'T05_T07' 'T06_T07']\n",
      "right 13 1095 820 535 ['T05_T07' 'T06_T07' 'T06_T08'] ['T06_T07' 'T06_T08']\n",
      "back 7 1274 948 943 ['T09_T11' 'T10_T11' 'T10_T12'] ['T10_T11' 'T09_T11' 'T10_T12']\n",
      "back 8 1146 854 558 ['T09_T11' 'T10_T11' 'T10_T12'] ['T10_T12' 'T10_T11']\n",
      "back 9 454 332 222 ['T09_T11' 'T10_T11' 'T10_T12'] ['T10_T11' 'T10_T12']\n",
      "back 10 897 670 447 ['T09_T11' 'T10_T12' 'T10_T11'] ['T10_T12' 'T10_T11']\n",
      "left 4 1177 875 805 ['T13_T15' 'T14_T16' 'T14_T15'] ['T14_T16' 'T14_T15' 'T13_T15']\n",
      "left 5 1223 912 502 ['T13_T15' 'T14_T16' 'T14_T15'] ['T14_T16' 'T14_T15' 'T13_T15']\n",
      "left 6 1135 849 487 ['T13_T15' 'T14_T16' 'T14_T15'] ['T14_T16' 'T14_T15']\n",
      "(7262, 153) (5967, 275)\n",
      "(7260, 276) (5967, 275)\n"
     ]
    }
   ],
   "source": [
    "# # for selection of images for partial human labeling - halo_human_in_dust_day_collection_may29\n",
    "# selected = {'front': {0: 'all', 1: ['begin', '2024-05-29T19:48:13'], 2: ['2024-05-29T20:28:50', 'end']},\n",
    "#             'right': {2: ['begin', '2024-05-29T20:28:53']},\n",
    "#             'back': {1: ['2024-05-29T20:02:08', 'end']},\n",
    "#             'left': {1: ['2024-05-29T19:48:10', '2024-05-29T20:02:08']}}\n",
    "# all_cameras = {'front': ['T01', 'T02', 'T03', 'T04'], 'right': ['T05', 'T06', 'T07', 'T08'], 'back': ['T09', 'T10', 'T11', 'T12'], 'left': ['T13', 'T14', 'T15', 'T16']}\n",
    "# left_cameras = ['front-center-left', 'front-left-left', 'front-right-left', 'side-left-left', 'side-right-left', 'rear-left', 'T01', 'T02', 'T05', 'T06', 'T09', 'T10', 'T13', 'T14', 'I01', 'I02']\n",
    "\n",
    "# selected_ids = []\n",
    "# for pod, cameras in all_cameras.items():\n",
    "#     cameras = set(cameras).intersection(left_cameras)\n",
    "#     selected_times = selected[pod]\n",
    "#     for seq_i, t1t2 in selected_times.items():\n",
    "#         seq_df = seq_dfs[seq_i]\n",
    "#         seq_df = seq_df[seq_df.camera_location.isin(cameras)]\n",
    "#         if t1t2 == 'all':\n",
    "#             t1, t2 = seq_df.iloc[0].collected_on, seq_df.iloc[-1].collected_on\n",
    "#         else:\n",
    "#             t1, t2 = t1t2\n",
    "#             if t1 == 'begin':\n",
    "#                 t1 = seq_df.iloc[0].collected_on\n",
    "#             if t2 == 'end':\n",
    "#                 t2 = seq_df.iloc[-1].collected_on\n",
    "#         print(pod, seq_i, t1, t2, len(seq_df))\n",
    "#         selected_ids += seq_df[(seq_df.collected_on >= t1) & (seq_df.collected_on <= t2)].id.to_list()\n",
    "# selected_ids = list(set(selected_ids))\n",
    "# selected_df = pd.DataFrame(data={'id': selected_ids})\n",
    "# print(selected_df.shape)\n",
    "# selected_df.to_csv('/data/jupiter/datasets/halo_human_in_dust_day_collection_may29/selected_for_label.csv', index=False)\n",
    "\n",
    "# recover skipped human labels in heavy dust, by images in the same sequence\n",
    "same_human_sequence = {'front': [0, 1, 3, 14], 'right': [11, 12, 13], 'back': [7, 8, 9, 10], 'left': [4, 5, 6]}\n",
    "suffix = '_human_labeled_stereo'\n",
    "recover_skipped_human_label(data_dir, seq_dfs, suffix, same_human_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_pass ['T09', 'T14', 'T13', 'T02']\n",
      "right_pass ['T10', 'T05', 'T06', 'T01']\n",
      "(13141, 1)\n",
      "(9893, 1)\n"
     ]
    }
   ],
   "source": [
    "# # load in labeled dataset\n",
    "# labeled_csv = '/data2/jupiter/datasets/halo_vehicles_driving_through_dust_images_nodust_reserved_labeled_maxfov_alleysson_depth0423/annotations.csv'\n",
    "# labeled_df = pd.read_csv(labeled_csv)\n",
    "# print(labeled_df.shape)\n",
    "# labeled_ids = set(labeled_df.id.to_list())\n",
    "# print(len(labeled_ids))\n",
    "\n",
    "# # for selection of images for missing partial vehicle labeling - halo_vehicles_in_dust_collection_march2024\n",
    "# left_cameras = {'left_pass': ['T09', 'T14', 'T13', 'T02'], 'right_pass': ['T10', 'T05', 'T06', 'T01']}\n",
    "# selected = {'left_pass': [2, 3, 4, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 28, 30, 31, 32, 33, 34, 35, 36], \n",
    "#             'right_pass': [3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 20, 21, 26, 27, 37]}\n",
    "# selected_ids = []\n",
    "# for pod, cameras in left_cameras.items():\n",
    "#     print(pod, cameras)\n",
    "#     for i,seq_df in enumerate(seq_dfs):\n",
    "#         if i in selected[pod]:\n",
    "#             sub_df = seq_df[seq_df.camera_location.isin(cameras)]\n",
    "#             selected_ids += sub_df.id.to_list()\n",
    "# selected_ids = list(set(selected_ids))\n",
    "# selected_df = pd.DataFrame(data={'id': selected_ids})\n",
    "# print(selected_df.shape)\n",
    "# selected_df = selected_df[~selected_df.id.isin(labeled_ids)]\n",
    "# print(selected_df.shape)\n",
    "# selected_df.to_csv('/data/jupiter/datasets/halo_vehicles_in_dust_collection_march2024/selected_for_missing_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22675, 153) (11839, 278)\n",
      "front 15 1471 1101 691 ['T01_T03' 'T02_T03' 'T02_T04'] ['T02_T04' 'T02_T03']\n",
      "front 17 809 606 281 ['T01_T03' 'T02_T03' 'T02_T04'] ['T02_T04' 'T01_T03' 'T02_T03']\n",
      "right 18 748 559 361 ['T05_T07' 'T06_T07' 'T06_T08'] ['T06_T08' 'T06_T07']\n",
      "back 7 932 697 161 ['T09_T11' 'T10_T11' 'T10_T12'] ['T09_T11']\n",
      "back 8 564 423 216 ['T09_T11' 'T10_T11' 'T10_T12'] ['T10_T12' 'T10_T11']\n",
      "back 9 1368 1025 583 ['T09_T11' 'T10_T11' 'T10_T12'] ['T10_T12' 'T10_T11']\n",
      "back 10 1552 1161 809 ['T09_T11' 'T10_T11' 'T10_T12'] ['T10_T11' 'T09_T11' 'T10_T12']\n",
      "left 5 1104 826 488 ['T13_T15' 'T14_T15' 'T14_T16'] ['T14_T15' 'T14_T16']\n",
      "left 6 1460 1090 676 ['T13_T15' 'T14_T15' 'T14_T16'] ['T14_T16' 'T14_T15']\n",
      "(5346, 153) (11839, 278)\n",
      "(12919, 279) (11839, 278)\n"
     ]
    }
   ],
   "source": [
    "# # for selection of images for missing partial human labeling - halo_human_in_dust_night_collection_june03\n",
    "# left_cameras = {'front': ['T01', 'T02'], 'right': ['T05', 'T06'], 'back': ['T09', 'T10'], 'left': ['T13', 'T14']}\n",
    "# all_cameras = {'front': ['T01', 'T02', 'T03', 'T04'], 'right': ['T05', 'T06', 'T07', 'T08'], 'back': ['T09', 'T10', 'T11', 'T12'], 'left': ['T13', 'T14', 'T15', 'T16']}\n",
    "# selected = {'front': [25, 26, 27, 30]}\n",
    "# selected_ids = []\n",
    "# for pod, cameras in all_cameras.items():\n",
    "#     print(pod, cameras)\n",
    "#     for i,seq_df in enumerate(seq_dfs):\n",
    "#         if i in selected[pod]:\n",
    "#             sub_df = seq_df[seq_df.camera_location.isin(cameras)]\n",
    "#             selected_ids += sub_df.id.to_list()\n",
    "\n",
    "# # for selection of images for missing partial human labeling - halo_human_in_dust_night_collection_june03_2\n",
    "# left_cameras = {'front': ['T01', 'T02'], 'right': ['T05', 'T06'], 'back': ['T09', 'T10'], 'left': ['T13', 'T14']}\n",
    "# selected = {'front': [0, 1, 2, 3, 15, 16, 17], 'right': [18,], 'back': [7, 8, 9, 10, 11], 'left': [4, 5, 6, 12, 13, 14]}\n",
    "# selected_ids = []\n",
    "# for pod, cameras in all_cameras.items():\n",
    "#     print(pod, cameras)\n",
    "#     for i,seq_df in enumerate(seq_dfs):\n",
    "#         if i in selected[pod]:\n",
    "#             sub_df = seq_df[seq_df.camera_location.isin(cameras)]\n",
    "#             selected_ids += sub_df.id.to_list()\n",
    "# selected_ids = list(set(selected_ids))\n",
    "# selected_df = pd.DataFrame(data={'id': selected_ids})\n",
    "# print(selected_df.shape)\n",
    "# selected_df.to_csv('/data/jupiter/datasets/halo_human_in_dust_night_collection_june03_2/selected_for_label.csv', index=False)\n",
    "\n",
    "# recover skipped human labels in heavy dust, by images in the same sequence\n",
    "same_human_sequence = {'front': [15, 17], 'right': [18], 'back': [7, 8, 9, 10], 'left': [5, 6]}\n",
    "suffix = '_human_labeled_stereo'\n",
    "recover_skipped_human_label(data_dir, seq_dfs, suffix, same_human_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-9e5b735fb503>:5: DtypeWarning: Columns (25) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  recovered_df, labeled_ms_df = recover_skipped_human_label(data_dir, seq_dfs, suffix, same_human_sequence)\n",
      "/home/li.yu/anaconda3/envs/pytorchlightning/lib/python3.8/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8740, 153) (3511, 277)\n",
      "back 0 752 562 184 ['T09_T11' 'T10_T12' 'T10_T11'] ['T09_T11']\n",
      "back 1 1216 911 114 ['T09_T11' 'T10_T12' 'T10_T11'] ['T09_T11']\n",
      "back 2 248 186 11 ['T09_T11' 'T10_T12' 'T10_T11'] ['T09_T11']\n",
      "back 3 1476 1102 196 ['T09_T11' 'T10_T12' 'T10_T11'] ['T09_T11']\n",
      "back 5 1688 1262 576 ['T09_T11' 'T10_T12' 'T10_T11'] ['T10_T12' 'T10_T11']\n",
      "back 6 2176 1631 967 ['T09_T11' 'T10_T12' 'T10_T11'] ['T10_T11' 'T10_T12']\n",
      "back 7 1380 1034 687 ['T09_T11' 'T10_T12' 'T10_T11'] ['T10_T12' 'T10_T11']\n",
      "(3538, 153) (3511, 277)\n",
      "(4314, 278) (3511, 277)\n"
     ]
    }
   ],
   "source": [
    "# for selection of images for missing partial human labeling - halo_human_in_dust_day_collection_back_june05\n",
    "# recover skipped human labels in heavy dust, by images in the same sequence\n",
    "same_human_sequence = {'back': [0, 1, 2, 3, 5, 6, 7]}\n",
    "suffix = '_human_labeled_stereo'\n",
    "recover_skipped_human_label(data_dir, seq_dfs, suffix, same_human_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "front ['T01', 'T02']\n",
      "right ['T05', 'T06']\n",
      "back ['T09', 'T10']\n",
      "left ['T13', 'T14']\n",
      "(3354, 1)\n"
     ]
    }
   ],
   "source": [
    "# # for selection of images for missing partial human labeling - halo_human_in_dust_dusk_collection_front_june07\n",
    "# left_cameras = {'front': ['T01', 'T02'], 'right': ['T05', 'T06'], 'back': ['T09', 'T10'], 'left': ['T13', 'T14']}\n",
    "# selected = {'front': [5], 'right': [], 'back': [], 'left': []}\n",
    "# selected_ids = []\n",
    "# for pod, cameras in left_cameras.items():\n",
    "#     print(pod, cameras)\n",
    "#     for i,seq_df in enumerate(seq_dfs):\n",
    "#         if i in selected[pod]:\n",
    "#             sub_df = seq_df[seq_df.camera_location.isin(cameras)]\n",
    "#             selected_ids += sub_df.id.to_list()\n",
    "# selected_ids = list(set(selected_ids))\n",
    "# selected_df = pd.DataFrame(data={'id': selected_ids})\n",
    "# print(selected_df.shape)\n",
    "# selected_df.to_csv('/data/jupiter/datasets/halo_human_in_dust_dusk_collection_front_june07/selected_for_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_pass ['T09', 'T14', 'T13', 'T02']\n",
      "right_pass ['T10', 'T05', 'T06', 'T01']\n",
      "(4513, 1)\n"
     ]
    }
   ],
   "source": [
    "# # for selection of images for missing partial vehicle labeling - halo_vehicles_in_dust_collection_june06\n",
    "# left_cameras = {'left_pass': ['T09', 'T14', 'T13', 'T02'], 'right_pass': ['T10', 'T05', 'T06', 'T01']}\n",
    "# selected = {'left_pass': [11, 12, 15], \n",
    "#             'right_pass': [10, 13, 14]}\n",
    "# selected_ids = []\n",
    "# for pod, cameras in left_cameras.items():\n",
    "#     print(pod, cameras)\n",
    "#     for i,seq_df in enumerate(seq_dfs):\n",
    "#         if i in selected[pod]:\n",
    "#             sub_df = seq_df[seq_df.camera_location.isin(cameras)]\n",
    "#             selected_ids += sub_df.id.to_list()\n",
    "# selected_ids = list(set(selected_ids))\n",
    "# selected_df = pd.DataFrame(data={'id': selected_ids})\n",
    "# print(selected_df.shape)\n",
    "# selected_df.to_csv('/data/jupiter/datasets/halo_vehicles_in_dust_collection_june06/selected_for_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_pass ['T09', 'T14', 'T13', 'T02']\n",
      "right_pass ['T10', 'T05', 'T06', 'T01']\n",
      "(14354, 1)\n"
     ]
    }
   ],
   "source": [
    "# # for selection of images for missing partial vehicle labeling - halo_vehicles_in_dust_collection_june07\n",
    "# left_cameras = {'left_pass': ['T09', 'T14', 'T13', 'T02'], 'right_pass': ['T10', 'T05', 'T06', 'T01']}\n",
    "# selected = {'left_pass': [2, 6, 7, 10], \n",
    "#             'right_pass': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}\n",
    "# selected_ids = []\n",
    "# for pod, cameras in left_cameras.items():\n",
    "#     print(pod, cameras)\n",
    "#     for i,seq_df in enumerate(seq_dfs):\n",
    "#         if i in selected[pod]:\n",
    "#             sub_df = seq_df[seq_df.camera_location.isin(cameras)]\n",
    "#             selected_ids += sub_df.id.to_list()\n",
    "# selected_ids = list(set(selected_ids))\n",
    "# selected_df = pd.DataFrame(data={'id': selected_ids})\n",
    "# print(selected_df.shape)\n",
    "# selected_df.to_csv('/data/jupiter/datasets/halo_vehicles_in_dust_collection_june07/selected_for_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csvs(pred_root_dir, model, labeled_dataset):\n",
    "    pred_df = pd.read_csv(os.path.join(pred_root_dir, model, labeled_dataset, 'output.csv'))\n",
    "    if not 'state' in pred_df:\n",
    "        pred_df['state'] = pred_df['result_state']\n",
    "    dust_df = pd.read_csv(os.path.join(pred_root_dir, model, labeled_dataset, 'dust_ratio.csv'))\n",
    "    print(labeled_dataset, pred_df.shape, dust_df.shape)\n",
    "    df = pred_df[['unique_id', 'state', 'camera_location', 'operation_time']].merge(dust_df, on='unique_id')\n",
    "    all_cameras = {'Front Pod': ['T01', 'T02', 'T03', 'T04'], 'Right Pod': ['T05', 'T06', 'T07', 'T08'], 'Rear Pod': ['T09', 'T10', 'T11', 'T12'], 'Left Pod': ['T13', 'T14', 'T15', 'T16']}\n",
    "    all_cameras = {c: pod for pod, cameras in all_cameras.items() for c in cameras}\n",
    "    df['pod'] = df.camera_location.apply(lambda c: all_cameras[c])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halo_human_in_dust_day_collection_may29_human_labeled_stereo (5968, 16) (5968, 7)\n",
      "(5968, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>state</th>\n",
       "      <th>camera_location</th>\n",
       "      <th>operation_time</th>\n",
       "      <th>id</th>\n",
       "      <th>gt_dust_ratio</th>\n",
       "      <th>total_averaged_dust_conf</th>\n",
       "      <th>total_thresholded_dust_ratio</th>\n",
       "      <th>masked_avg_dust_conf</th>\n",
       "      <th>masked_dust_ratio</th>\n",
       "      <th>pod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66595e558b545d525752cfb2_T02_T03</td>\n",
       "      <td>true_positive</td>\n",
       "      <td>T02</td>\n",
       "      <td>daytime</td>\n",
       "      <td>66595e558b545d525752cfb2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124939</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.152748</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>Front Pod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66595eea568146714ce37514_T02_T03</td>\n",
       "      <td>true_positive</td>\n",
       "      <td>T02</td>\n",
       "      <td>daytime</td>\n",
       "      <td>66595eea568146714ce37514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125615</td>\n",
       "      <td>0.028809</td>\n",
       "      <td>0.125125</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>Front Pod</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          unique_id          state camera_location  \\\n",
       "0  66595e558b545d525752cfb2_T02_T03  true_positive             T02   \n",
       "1  66595eea568146714ce37514_T02_T03  true_positive             T02   \n",
       "\n",
       "  operation_time                        id  gt_dust_ratio  \\\n",
       "0        daytime  66595e558b545d525752cfb2            0.0   \n",
       "1        daytime  66595eea568146714ce37514            0.0   \n",
       "\n",
       "   total_averaged_dust_conf  total_thresholded_dust_ratio  \\\n",
       "0                  0.124939                      0.036865   \n",
       "1                  0.125615                      0.028809   \n",
       "\n",
       "   masked_avg_dust_conf  masked_dust_ratio        pod  \n",
       "0              0.152748           0.047904  Front Pod  \n",
       "1              0.125125           0.029940  Front Pod  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in model predictions\n",
    "pred_root_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model/'\n",
    "model = 'all_rev2_rev1_lying_down_sitting_headlights_round_2_25_ep_prod_weights_10_lo_10_tr_dust_0601'\n",
    "suffix = '_human_labeled_stereo'\n",
    "pred_df = read_csvs(pred_root_dir, model, os.path.basename(data_dir)+suffix)\n",
    "print(pred_df.shape)\n",
    "pred_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1330, 109) (246, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "state\n",
       "false_negative      1\n",
       "false_positive      5\n",
       "true_negative     126\n",
       "true_positive     114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_df = seq_dfs[0]\n",
    "pred_seq_df = pred_df[pred_df.id.isin(seq_df.id)]\n",
    "print(seq_df.shape, pred_seq_df.shape)\n",
    "pred_seq_df.groupby('state').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "front ['T01', 'T02', 'T03', 'T04']\n",
      "0 [436, 436, 436, 437]\n",
      "1 [588, 589, 589, 589]\n",
      "2 [1130, 1131, 1130, 1131]\n",
      "3 [692, 692, 692, 692]\n",
      "4 [0, 0, 0, 0]\n",
      "5 [0, 0, 0, 0]\n",
      "6 [0, 0, 0, 0]\n",
      "7 [0, 0, 0, 0]\n",
      "8 [0, 0, 0, 0]\n",
      "9 [0, 0, 0, 0]\n",
      "10 [0, 0, 0, 0]\n",
      "11 [0, 0, 0, 0]\n",
      "12 [0, 0, 0, 0]\n",
      "13 [0, 0, 0, 0]\n",
      "14 [0, 0, 0, 0]\n",
      "15 [368, 367, 368, 368]\n",
      "16 [337, 337, 337, 337]\n",
      "17 [202, 203, 203, 201]\n",
      "18 [0, 0, 0, 0]\n",
      "right ['T05', 'T06', 'T07', 'T08']\n",
      "0 [0, 0, 0, 0]\n",
      "1 [0, 0, 0, 0]\n",
      "2 [0, 0, 0, 0]\n",
      "3 [0, 0, 0, 0]\n",
      "4 [0, 0, 0, 0]\n",
      "5 [0, 0, 0, 0]\n",
      "6 [0, 0, 0, 0]\n",
      "7 [0, 0, 0, 0]\n",
      "8 [0, 0, 0, 0]\n",
      "9 [0, 0, 0, 0]\n",
      "10 [0, 0, 0, 0]\n",
      "11 [0, 0, 0, 0]\n",
      "12 [0, 0, 0, 0]\n",
      "13 [0, 0, 0, 0]\n",
      "14 [0, 0, 0, 0]\n",
      "15 [0, 0, 0, 0]\n",
      "16 [0, 0, 0, 0]\n",
      "17 [0, 0, 0, 0]\n",
      "18 [187, 187, 187, 187]\n",
      "back ['T09', 'T10', 'T11', 'T12']\n",
      "0 [0, 0, 0, 0]\n",
      "1 [0, 0, 0, 0]\n",
      "2 [0, 0, 0, 0]\n",
      "3 [0, 0, 0, 0]\n",
      "4 [0, 0, 0, 0]\n",
      "5 [0, 0, 0, 0]\n",
      "6 [0, 0, 0, 0]\n",
      "7 [233, 233, 233, 233]\n",
      "8 [141, 141, 141, 141]\n",
      "9 [342, 342, 342, 342]\n",
      "10 [388, 388, 388, 388]\n",
      "11 [384, 384, 384, 384]\n",
      "12 [0, 0, 0, 0]\n",
      "13 [0, 0, 0, 0]\n",
      "14 [0, 0, 0, 0]\n",
      "15 [0, 0, 0, 0]\n",
      "16 [0, 0, 0, 0]\n",
      "17 [0, 0, 0, 0]\n",
      "18 [0, 0, 0, 0]\n",
      "left ['T13', 'T14', 'T15', 'T16']\n",
      "0 [0, 0, 0, 0]\n",
      "1 [0, 0, 0, 0]\n",
      "2 [0, 0, 0, 0]\n",
      "3 [0, 0, 0, 0]\n",
      "4 [404, 404, 405, 404]\n",
      "5 [276, 276, 276, 276]\n",
      "6 [365, 365, 365, 365]\n",
      "7 [0, 0, 0, 0]\n",
      "8 [0, 0, 0, 0]\n",
      "9 [0, 0, 0, 0]\n",
      "10 [0, 0, 0, 0]\n",
      "11 [0, 0, 0, 0]\n",
      "12 [337, 337, 338, 336]\n",
      "13 [394, 394, 394, 395]\n",
      "14 [365, 365, 365, 365]\n",
      "15 [0, 0, 0, 0]\n",
      "16 [0, 0, 0, 0]\n",
      "17 [0, 0, 0, 0]\n",
      "18 [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# read from multiple cameras and put in once frame\n",
    "# left_pass_pairs = ['T09_T11', 'T14_T16', 'T14_T15', 'T13_T15']\n",
    "# right_pass_pairs = ['T05_T07', 'T10_T12', 'T06_T08', 'T06_T07']\n",
    "# cameras = [f'T{str(i+1).zfill(2)}' for i in range(16)][12:]\n",
    "# cameras = ['T16', 'T15', 'T14', 'T13', 'T11', 'T09']\n",
    "# all_cameras = {'front': ['T01', 'T02', 'T03', 'T04'], 'right': ['T05', 'T06', 'T07', 'T08'], 'back': ['T09', 'T10', 'T11', 'T12'], 'left': ['T13', 'T14', 'T15', 'T16']}\n",
    "# all_left_cameras = {'front': ['T01', 'T02'], 'right': ['T05', 'T06'], 'back': ['T09', 'T10'], 'left': ['T13', 'T14']}\n",
    "# left_cameras = ['T01', 'T02', 'T05', 'T06', 'T09', 'T10', 'T13', 'T14']\n",
    "# back_left_cameras = ['T09', 'T10']\n",
    "left_cameras = {'left_pass': ['T09', 'T14', 'T13', 'T02'], 'right_pass': ['T10', 'T05', 'T06', 'T01']}\n",
    "for pod, cameras in left_cameras.items():\n",
    "    # pod = 'all_left'\n",
    "    # cameras = left_cameras\n",
    "    print(pod, cameras)\n",
    "    H = 2  # number of camera rows\n",
    "    W = 2  # number of camera cols\n",
    "    for i,seq_df in enumerate(seq_dfs):\n",
    "        # # check model prediction and filter by dust/seg outputs\n",
    "        # pred_seq_df = pred_df[pred_df.id.isin(seq_df.id)]\n",
    "        # labeled_ids = pred_seq_df.id.to_list()\n",
    "\n",
    "        cam_dfs = [seq_df[seq_df.camera_location == c] for c in cameras]\n",
    "        print(i, [len(cdf) for cdf in cam_dfs])\n",
    "        min_len = min(len(cdf) for cdf in cam_dfs)\n",
    "        cam_dfs = [cdf.sort_values('collected_on').iloc[:min_len] for cdf in cam_dfs]\n",
    "        if min_len < 2:\n",
    "            continue\n",
    "        # print(i, [cdf.iloc[0].collected_on for cdf in cam_dfs])\n",
    "\n",
    "        frame = read_raw_image_by_row(data_dir, seq_df.iloc[0])\n",
    "        height, width, layers = frame.shape\n",
    "        # print(height, width, layers)\n",
    "\n",
    "        # .avi MJPG,  .mp4 MP4V\n",
    "        video_dir = os.path.join(data_dir, f'videos_{len(cameras)}cams')\n",
    "        os.makedirs(video_dir, exist_ok=True)\n",
    "        video_name = os.path.join(video_dir, f'{pod}_seq{str(i).zfill(2)}.mp4')\n",
    "        video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), 3, (width*W,height*H), isColor=True)\n",
    "\n",
    "        for ii in tqdm(range(min_len)):\n",
    "            try:\n",
    "                canvas = np.zeros((height*H, width*W, 3), dtype=np.uint8)\n",
    "                for ci in range(len(cam_dfs)):\n",
    "                    cam_df_row = cam_dfs[ci].iloc[ii]\n",
    "                    frame = read_raw_image_by_row(data_dir, cam_df_row)\n",
    "                    s = f'{cam_df_row.camera_location} {cam_df_row.collected_on}'\n",
    "                    # if cam_df_row.id in labeled_ids:\n",
    "                    #     pred_row = pred_seq_df[pred_seq_df.id == cam_df_row.id].iloc[0]\n",
    "                    #     s += f' pred: {pred_row.state}, dust: {pred_row.total_averaged_dust_conf:.4f}'\n",
    "                    frame = cv2.putText(frame, s, \n",
    "                            (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "                    fi, fj = ci // W, ci % W\n",
    "                    canvas[height*fi:height*(fi+1), width*fj:width*(fj+1)] = frame\n",
    "                video.write(canvas)\n",
    "            except:\n",
    "                print(f'{ii}th image read failed')\n",
    "\n",
    "        # cv2.destroyAllWindows()\n",
    "        video.release()\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create video from saved pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(ids, pred_dir, video_name, read_func=read_saved_frame, fps=2):\n",
    "    frame = read_func(pred_dir, ids[10])\n",
    "    height, width, layers = frame.shape\n",
    "    print(height, width, layers)\n",
    "\n",
    "    # .avi MJPG,  .mp4 MP4V\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width,height), isColor=True)\n",
    "    \n",
    "    good = 0\n",
    "    for _id in tqdm(ids):\n",
    "        frame = read_func(pred_dir, _id)\n",
    "        if frame is not None:\n",
    "            video.write(frame)\n",
    "            good += 1\n",
    "    print('total', len(ids), 'used', good)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 3\n",
      "0 2021-11-10T00:33:05.931000 49\n",
      "311 3141 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 49/49 [00:01<00:00, 37.81it/s]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 49 used 49\n",
      "1 2021-12-16T22:14:00.050000 71\n",
      "311 3141 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 71/71 [00:01<00:00, 38.55it/s]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 71 used 71\n",
      "2 2022-11-10T23:19:10.901000 88\n",
      "311 3141 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 88/88 [00:02<00:00, 40.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88 used 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/data/jupiter/li.yu/data/Jupiter_human_on_path_3_fn_sequence'\n",
    "# csv_file = '/data/jupiter/datasets/Jupiter_gilroy_reverse_manny_v2/master_annotations_1fps.csv'\n",
    "# csv_file = '/data/jupiter/li.yu/data/mannequin_in_dust_v1/master_annotations.csv'\n",
    "csv_file = f'{data_dir}/master_annotations.csv'\n",
    "master_df = pd.read_csv(csv_file)\n",
    "master_df = master_df.sort_values('collected_on')\n",
    "\n",
    "# pred_dir = '/mnt/sandbox1/rakhil.immidisetti/output/driveable_terrain_model/471_cloud_v45_cutnpaste_s35/Jupiter_gilroy_reverse_manny_v2reverse_val_bestmodel/'\n",
    "# pred_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model/v471_nobadiqnohard_6cls_cnp_alpha2_0805/Jupiter_gilroy_reverse_manny_v2/'\n",
    "# pred_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/mannequin_in_dust_v1'\n",
    "pred_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/Jupiter_human_on_path_3_fn_sequence'\n",
    "save_dir = pred_dir\n",
    "\n",
    "# # save as a single video\n",
    "# print(master_df.shape)\n",
    "# video_name = os.path.join(save_dir, 'pred.mp4')\n",
    "# ids = master_df.image_id.to_list()\n",
    "# create_video(ids, pred_dir, video_name, fps=3)\n",
    "\n",
    "# break into sequences\n",
    "seq_dfs = get_sequences(master_df, interval=5, per_camera=True)\n",
    "print(len(master_df), len(seq_dfs))\n",
    "video_dir = os.path.join(pred_dir, 'videos')\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "for si, seq_df in enumerate(seq_dfs):\n",
    "    if len(seq_df) < 10:\n",
    "        continue\n",
    "    name = seq_df.iloc[0].collected_on\n",
    "    camera = seq_df.iloc[0].camera_location[:-5]\n",
    "    print(si, name, len(seq_df))\n",
    "    \n",
    "    # create video\n",
    "    # video_name = os.path.join(video_dir, str(si).zfill(3)+'.mp4')\n",
    "    video_name = os.path.join(video_dir, f'{name}_{camera}_{si}.mp4')\n",
    "    ids = seq_df.image_id.to_list()\n",
    "    create_video(ids, pred_dir, video_name, fps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create video from PP artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_pp_artifacts(data_dir, df_row):\n",
    "    data_path = os.path.join(data_dir, df_row.stereo_pipeline_npz_save_path)\n",
    "    img = np.load(data_path)['left']\n",
    "    img_norm = normalize_image(img, df_row.hdr_mode if 'hdr_mode' in df_row else True)\n",
    "    return cv2.cvtColor((img_norm * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# def add_text(frame, txt_row):\n",
    "#     frame = cv2.putText(frame, f'Collected on: {txt_row.collected_on}', \n",
    "#                         (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "#     return frame\n",
    "\n",
    "def add_text(frame, txt):\n",
    "    frame = cv2.putText(frame, txt, \n",
    "                        (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "    return frame\n",
    "\n",
    "def add_texts(frame, txts: list):\n",
    "    txt_pw, txt_ph = 10, 25\n",
    "    for i, txt in enumerate(txts):\n",
    "        if 'false_positive' in txt:\n",
    "            frame = cv2.putText(frame, txt, \n",
    "                                (txt_pw, txt_ph+i*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            frame = cv2.putText(frame, txt, \n",
    "                                (txt_pw, txt_ph+i*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/li.yu/anaconda3/envs/pytorchlightning/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3437: DtypeWarning: Columns (37,46,58,60,74,86,101,103,106,116,118) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23023, 150) 432\n"
     ]
    }
   ],
   "source": [
    "# data_root_dir = '/data/jupiter/li.yu/data'\n",
    "data_root_dir = '/data/jupiter/datasets/'\n",
    "# dataset = 'mannequin_in_dust_v1'\n",
    "# dataset = 'Jupiter_human_on_path_3_fn_sequence'\n",
    "dataset = 'halo_missed_lo_rock_0509_stereo'\n",
    "data_dir = os.path.join(data_root_dir, dataset)\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, 'master_annotations.csv'))\n",
    "seq_dfs = get_sequences(df, interval=5, per_camera=True)\n",
    "print(df.shape, len(seq_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['18:16:02', '18:17:02'] 606\n",
      "['18:17:01', '18:18:01'] 750\n",
      "['18:17:48', '18:18:48'] 498\n",
      "['18:18:45', '18:19:45'] 354\n",
      "['18:21:31', '18:22:31'] 732\n",
      "['18:22:26', '18:23:26'] 900\n",
      "['18:23:12', '18:24:12'] 426\n",
      "['18:24:15', '18:25:15'] 432\n",
      "['20:16:20', '20:17:20'] 930\n",
      "['20:18:38', '20:19:38'] 850\n",
      "['20:20:28', '20:21:28'] 1200\n",
      "(6568, 151)\n",
      "(6568, 151) 2\n"
     ]
    }
   ],
   "source": [
    "bags = [\n",
    "[\"05_09_2024_18_16_02\", \"05_09_2024_18_17_02\"],\n",
    "[\"05_09_2024_18_17_01\", \"05_09_2024_18_18_01\"],\n",
    "[\"05_09_2024_18_17_48\", \"05_09_2024_18_18_48\"],\n",
    "[\"05_09_2024_18_18_45\", \"05_09_2024_18_19_45\"],\n",
    "[\"05_09_2024_18_21_31\", \"05_09_2024_18_22_31\"],\n",
    "[\"05_09_2024_18_22_26\", \"05_09_2024_18_23_26\"],\n",
    "[\"05_09_2024_18_23_12\", \"05_09_2024_18_24_12\"],\n",
    "[\"05_09_2024_18_24_15\", \"05_09_2024_18_25_15\"],\n",
    "[\"05_09_2024_20_16_20\", \"05_09_2024_20_17_20\"],\n",
    "[\"05_09_2024_20_18_38\", \"05_09_2024_20_19_38\"],\n",
    "[\"05_09_2024_20_20_28\", \"05_09_2024_20_21_28\"],\n",
    "]\n",
    "bags = [[bag[0][11:].replace('_', ':'), bag[1][11:].replace('_', ':')] for bag in bags]\n",
    "df['collected_hms'] = df['collected_on'].apply(lambda t: t[11:])\n",
    "df_oi = []\n",
    "for bag in bags:\n",
    "    sub_df = df[(df.collected_hms >= bag[0]) & (df.collected_hms < bag[1])]\n",
    "    print(bag, len(sub_df))\n",
    "    df_oi.append(sub_df)\n",
    "df_oi = pd.concat(df_oi, ignore_index=True).drop_duplicates()\n",
    "print(df_oi.shape)\n",
    "\n",
    "seq_dfs = get_sequences(df_oi)\n",
    "print(df_oi.shape, len(seq_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>id</th>\n",
       "      <th>camera_location</th>\n",
       "      <th>operation_time</th>\n",
       "      <th>special_notes</th>\n",
       "      <th>jdb_s3_path</th>\n",
       "      <th>result_state</th>\n",
       "      <th>result_human_state</th>\n",
       "      <th>result_vehicle_state</th>\n",
       "      <th>min_pixels_threshold</th>\n",
       "      <th>features</th>\n",
       "      <th>n_gt_human_pixels</th>\n",
       "      <th>gt_human_depth</th>\n",
       "      <th>n_pred_human_pixels</th>\n",
       "      <th>pred_human_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>663ec898321f043ed7ad8a62_T02_T03</td>\n",
       "      <td>663ec898321f043ed7ad8a62</td>\n",
       "      <td>T02</td>\n",
       "      <td>daytime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>true_negative</td>\n",
       "      <td>true_negative</td>\n",
       "      <td>true_negative</td>\n",
       "      <td>108</td>\n",
       "      <td>{\"large_object_pixels\": 688, \"large_object_min...</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          unique_id                        id camera_location  \\\n",
       "0  663ec898321f043ed7ad8a62_T02_T03  663ec898321f043ed7ad8a62             T02   \n",
       "\n",
       "  operation_time special_notes  jdb_s3_path   result_state result_human_state  \\\n",
       "0        daytime           NaN          NaN  true_negative      true_negative   \n",
       "\n",
       "  result_vehicle_state  min_pixels_threshold  \\\n",
       "0        true_negative                   108   \n",
       "\n",
       "                                            features  n_gt_human_pixels  \\\n",
       "0  {\"large_object_pixels\": 688, \"large_object_min...                  0   \n",
       "\n",
       "   gt_human_depth  n_pred_human_pixels  pred_human_depth  \n",
       "0            1000                    0              1000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model'\n",
    "models = [\n",
    "    'ds_v8_1_nextvit_small_openimages_with_rev1_train_human_test_using_random_val_mhc_20_epochs_finetune_rev1_lr',\n",
    "    'v81_80k_maxfov_wn_ft_kore_0430'\n",
    "]\n",
    "suffix_list = ['_mhc_depth0125', '']\n",
    "pred_dfs = [pd.read_csv(os.path.join(pred_dir, model, dataset+suffix, 'output.csv')) for model,suffix in zip(models, suffix_list)]\n",
    "pred_dfs[0].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/248 [00:00<00:29,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[248, 248, 248]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 248/248 [00:30<00:00,  8.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_pred_texts(pred_dfs, unique_id, text_prefix):\n",
    "    texts = []\n",
    "    for pred_df, prefix in zip(pred_dfs, text_prefix):\n",
    "        pred_row = pred_df[pred_df.unique_id == unique_id].iloc[0]\n",
    "        texts.append(f'{prefix}: {pred_row.result_state}')\n",
    "    return texts\n",
    "\n",
    "def create_video_from_pp_add_text(seq_df, camera_pairs, pred_dfs, H, W, data_dir, video_dir_name):\n",
    "    cam_dfs = [seq_df[seq_df.unique_id.str.endswith(c)] for c in camera_pairs]\n",
    "    cam_dfs = [cdf.sort_values('collected_on', ignore_index=True) for cdf in cam_dfs]\n",
    "    min_len = min(len(cdf) for cdf in cam_dfs)\n",
    "    cam_dfs = [cdf.iloc[:min_len] for cdf in cam_dfs]\n",
    "    print([len(cdf) for cdf in cam_dfs])\n",
    "\n",
    "    # .avi MJPG,  .mp4 MP4V\n",
    "    os.makedirs(os.path.join(data_dir, video_dir_name), exist_ok=True)\n",
    "    video_name = os.path.join(data_dir, f'{video_dir_name}/{seq_df.iloc[0].collected_on}.mp4')\n",
    "\n",
    "    # print(f'{H} rows, {W} cols out of {len(camera_pairs)} camera_pairs')\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), 3, (768*W,512*H), isColor=True)\n",
    "\n",
    "    for ii in tqdm(range(min_len)):\n",
    "        # try:\n",
    "        canvas = np.zeros((512*H, 768*W, 3), dtype=np.uint8)\n",
    "        for ci in range(len(cam_dfs)):\n",
    "            row = cam_dfs[ci].iloc[ii]\n",
    "            frame = read_from_pp_artifacts(data_dir, row)\n",
    "            texts = [f'{row.collected_on}'] + get_pred_texts(pred_dfs, row.unique_id, ['MHC pred', 'MAXFOV pred'])\n",
    "            # print(texts)\n",
    "            frame = add_texts(frame, texts)\n",
    "            fi, fj = ci // W, ci % W\n",
    "            canvas[512*fi:512*(fi+1), 768*fj:768*fj+frame.shape[1]] = frame\n",
    "        video.write(canvas)\n",
    "        # except:\n",
    "        #     print(f'{ii}th image read failed')\n",
    "\n",
    "    # cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "front_pairs = ['T02_T04', 'T02_T03', 'T01_T03']\n",
    "left_pass_pairs = ['T02_T04', 'T02_T03', 'T01_T03', 'T13_T15', 'T14_T15', 'T14_T16']\n",
    "right_pass_pairs = ['T02_T04', 'T02_T03', 'T01_T03', 'T06_T08', 'T05_T07', 'T06_T07']\n",
    "# left_pass_pairs = ['T09_T11', 'T14_T16', 'T14_T15', 'T13_T15']\n",
    "# right_pass_pairs = ['T05_T07', 'T10_T12', 'T06_T08', 'T06_T07']\n",
    "H = 1  # number of image rows\n",
    "W = 3  # number of image cols\n",
    "create_video_from_pp_add_text(seq_dfs[1], front_pairs, pred_dfs, H, W, data_dir, 'videos_front_pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['T01_T03', 'T02_T03', 'T02_T04', 'T05_T07', 'T06_T07', 'T06_T08',\n",
       "       'T09_T11', 'T10_T12', 'T10_T11', 'T13_T15', 'T14_T16', 'T14_T15'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"camera_pair\"] = df[\"unique_id\"].apply(lambda t: t[-7:])\n",
    "df[\"camera_pair\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read frame and add prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 11)\n"
     ]
    }
   ],
   "source": [
    "# compare BRT model pred and CenterTrack pred\n",
    "# pred_csv = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/mannequin_in_dust_v1/output.csv'\n",
    "pred_csv = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/Jupiter_human_on_path_3_fn_sequence/output.csv'\n",
    "pred_df = pd.read_csv(pred_csv)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 2023-07-08T01:37:09.798000 153\n"
     ]
    }
   ],
   "source": [
    "video_with_pred_dir = os.path.join(data_root_dir, dataset, 'videos_with_pred')\n",
    "# pred_sequence_dir = '/home/li.yu/code/CenterTrack/results/2023-07-08T01:37:09.798000_front-center_15'\n",
    "pred_sequence_dir = '/home/li.yu/code/CenterTrack/results/brt50000/nopreimg_noprehm/2023-07-08T01:37:09.798000_front-center_15'\n",
    "os.makedirs(video_with_pred_dir, exist_ok=True)\n",
    "height, width = 512, 1024\n",
    "\n",
    "for si, seq_df in enumerate(seq_dfs):\n",
    "    if len(seq_df) < 10 or si != 15:\n",
    "        continue\n",
    "    name = seq_df.iloc[0].collected_on\n",
    "    camera = seq_df.iloc[0].camera_location[:-5]\n",
    "    print(si, name, len(seq_df))\n",
    "    \n",
    "    # merge pred from BRT model\n",
    "    seq_df = seq_df.drop(columns=['state']).merge(pred_df[['id', 'state', 'human_state']], on='id')\n",
    "\n",
    "    # create video\n",
    "    video_name = os.path.join(video_with_pred_dir, f'{name}_{camera}_{si}_nopreimg_noprehm.mp4')\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 3, (width,height), isColor=True)\n",
    "    fi = 0\n",
    "    for _, df_row in seq_df.iterrows():\n",
    "        frame = cv2.imread(os.path.join(pred_sequence_dir, str(fi).zfill(3)+'_'+df_row.id+'.png'))\n",
    "        frame = cv2.putText(frame, f'BRT Seg Pred: {df_row.state}, Strict: {df_row.human_state}', \n",
    "                            (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "        video.write(frame)\n",
    "        fi += 1\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read from video and prediction results to each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2022-11-10T23:19:10.901000 88\n"
     ]
    }
   ],
   "source": [
    "video_dir = '/home/li.yu/code/CenterTrack/results/'\n",
    "old_video_name = 'brt50000_2022-11-10T23:19:10.901000_side-right_2_preimg.mp4'\n",
    "new_video_name = 'brt50000_2022-11-10T23:19:10.901000_side-right_2_preimg_withbrtpred.mp4'\n",
    "height, width = 512, 1024\n",
    "\n",
    "for si, seq_df in enumerate(seq_dfs):\n",
    "    if len(seq_df) < 10 or si != 2:\n",
    "        continue\n",
    "    name = seq_df.iloc[0].collected_on\n",
    "    camera = seq_df.iloc[0].camera_location[:-5]\n",
    "    print(si, name, len(seq_df))\n",
    "    \n",
    "    # merge pred from BRT model\n",
    "    seq_df = seq_df.drop(columns=['state']).merge(pred_df[['id', 'state', 'human_state']], on='id')\n",
    "\n",
    "    # read video\n",
    "    cam = cv2.VideoCapture(os.path.join(video_dir, old_video_name))\n",
    "\n",
    "    # create video\n",
    "    video_name = os.path.join(video_dir, new_video_name)\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 3, (width,height), isColor=True)\n",
    "    for _, df_row in seq_df.iterrows():\n",
    "        _, frame = cam.read()\n",
    "        frame = cv2.putText(frame, f'BRT Seg Pred: {df_row.state}, Strict: {df_row.human_state}', \n",
    "                            (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "        video.write(frame)\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "    # break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorchlightning': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd01eceddbeeb55f686303d64ef8e05e300429be7c506c9f9cad24a6dfe5f27b555"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "1eceddbeeb55f686303d64ef8e05e300429be7c506c9f9cad24a6dfe5f27b555"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
