{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "\n",
    "class ModelType(Enum):\n",
    "    CLASSIFICATION = 0\n",
    "    SEGMENTATION = 1\n",
    "\n",
    "classlabels_viz_colors = ['black', 'green', 'yellow', 'blue', 'red', 'magenta', 'cyan',\n",
    "                          'lightseagreen', 'brown', 'magenta', 'olive', 'wheat', 'white', 'black']\n",
    "classlabels_viz_bounds = [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 100]\n",
    "\n",
    "classlabels_viz_cmap = mpl.colors.ListedColormap(classlabels_viz_colors)\n",
    "classlabels_viz_norm = mpl.colors.BoundaryNorm(classlabels_viz_bounds, classlabels_viz_cmap.N)\n",
    "\n",
    "confidence_heatmap_viz_colors = ['black', 'blue', 'red', 'orange', 'yellow', 'lightgreen', 'lightseagreen']\n",
    "confidence_heatmap_viz_bounds = [-1, 0,0.5,0.6,0.7,0.8,0.9,1]\n",
    "confidence_heatmap_viz_cmap = mpl.colors.ListedColormap(confidence_heatmap_viz_colors)\n",
    "confidence_heatmap_viz_norm = mpl.colors.BoundaryNorm(confidence_heatmap_viz_bounds, confidence_heatmap_viz_cmap.N)\n",
    "\n",
    "\n",
    "LabelColor = namedtuple('LabelColor', ['name', 'id', 'trainid', 'color', 'category'])\n",
    "\n",
    "LABEL_COLORS = [\n",
    "    LabelColor('class1', 1, 0, (128, 0, 128), 'driveableterrain'),\n",
    "    LabelColor('class2', 2, 1, (255, 0, 0), 'non-driveableterrain'),\n",
    "    LabelColor('class3', 3, 2, (0, 0, 255), 'sky'),\n",
    "    LabelColor('class4', 4, 3, (0, 255, 0), 'trees'),\n",
    "    LabelColor('class5', 5, 4, (255, 0, 255), 'implement'),\n",
    "    LabelColor('class6', 6, 5, (255, 255, 0), 'basket markers')\n",
    "]\n",
    "\n",
    "LABEL_COLORS_4CLASS = LABEL_COLORS[0:4]\n",
    "LABEL_COLORS_5CLASS = LABEL_COLORS[0:5]\n",
    "LABEL_COLORS_6CLASS = LABEL_COLORS[0:6]\n",
    "LABEL_COLORS_SKY_DET = [LABEL_COLORS[0], LABEL_COLORS[2]]\n",
    "\n",
    "LABEL_COLORS_IMPL = [\n",
    "    LabelColor('class1', 1, 1, (128, 0, 128), 'implement'),\n",
    "    LabelColor('class2', 2, 2, (255, 0, 0), 'sweep'),\n",
    "    LabelColor('class3', 3, 3, (0, 0, 255), 'harrow_tine'),\n",
    "    LabelColor('class4', 4, 4, (0, 255, 0), 'basket'),\n",
    "    LabelColor('class5', 5, 5, (0, 255, 0), 'basket_marker'),\n",
    "    LabelColor('class6', 0, 255, (0, 0, 0), 'ignore')\n",
    "]\n",
    "\n",
    "LABEL_COLORS_IMPL_REDUCED = [\n",
    "    LabelColor('class0', 1, 0, (0, 0, 0), 'background'),\n",
    "    LabelColor('class1', 2, 1, (0, 255, 0), 'implement'),\n",
    "    LabelColor('class2', 3, 2, (255, 0, 0), 'sweep'),\n",
    "    LabelColor('class3', 4, 3, (0, 255, 0), 'basket_marker'),\n",
    "    LabelColor('class6', 0, 255, (0, 0, 0), 'ignore')\n",
    "]\n",
    "\n",
    "PLUG_LABEL_MAP ={0: 'no-plug', 1: 'plug'}\n",
    "IMPL_SEGMENT_LABEL_MAP = {0: 'background', 1: 'implement', 2: 'sweep', 3:'basket_marker'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mpl_viz_outputs(output_path,\n",
    "                           image,\n",
    "                           prediction_labels,\n",
    "                           confidences,\n",
    "                           depth_img, \n",
    "                           image_title='Image', \n",
    "                           pred_title='Prediction', \n",
    "                           conf_title='Confidence', \n",
    "                           depth_title='Depth',\n",
    "                           bbox_coords=[]):\n",
    "    \"\"\"\n",
    "    Utility function to plot results based on order of input provided.\n",
    "\n",
    "    Axes index can have different values based on input provided.\n",
    "\n",
    "    For example: If image, prediction and groundtruth_label is provided, A three axes plot will be generated with\n",
    "    image(ax1), ground_truth(ax2), prediction(ax3).\n",
    "\n",
    "    Order of plot if all the inputs are provided will be in same order as arguments listed above.\n",
    "    \"\"\"\n",
    "    axis_index = list(range(len(list(filter(lambda x: x is not None, [image,\n",
    "                                                                      prediction_labels, confidences,\n",
    "                                                                      depth_img])))))\n",
    "    axis_curr_index = 0\n",
    "    fig, axes = mpl.pyplot.subplots(1, len(axis_index), figsize=((60, 30)))\n",
    "    if bbox_coords:\n",
    "#         xmin, ymin, xmax, ymax = bbox_coords\n",
    "        xmin, xmax, ymin, ymax = bbox_coords\n",
    "        if (xmin < 0) or (ymin < 0):\n",
    "            raise ValueError(f'Either {xmin} or {ymin} are negative')\n",
    "        else:\n",
    "            rect = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',\n",
    "                                     facecolor='none')\n",
    "    else:\n",
    "        rect = None\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        # Grab only RGB channels from image, otherwise depth with distort the image when it is displayed\n",
    "        axes[axis_curr_index].imshow(image)\n",
    "        axes[axis_curr_index].set_title(image_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        axes[axis_curr_index].imshow(depth_img, cmap='turbo')\n",
    "        axes[axis_curr_index].set_title(depth_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        axes[axis_curr_index].imshow(prediction_labels, classlabels_viz_cmap, classlabels_viz_norm, interpolation='nearest')\n",
    "        if rect is not None:\n",
    "            rect1 = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',facecolor='none')\n",
    "            axes[axis_curr_index].add_patch(rect1)\n",
    "        axes[axis_curr_index].set_title(pred_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    if axis_curr_index < len(axes):\n",
    "        c = np.max(confidences, axis=2)\n",
    "        axes[axis_curr_index].imshow(c, confidence_heatmap_viz_cmap, confidence_heatmap_viz_norm, interpolation='nearest')\n",
    "#         if rect is not None:\n",
    "#             rect2 = mpl.patches.Rectangle((ymin, xmin), (ymax - ymin), (xmax - xmin), linewidth=3, edgecolor='k',\n",
    "#                                           facecolor='none')\n",
    "#             axes[axis_curr_index].add_patch(rect2)\n",
    "        axes[axis_curr_index].set_title(conf_title, fontsize=30)\n",
    "        axes[axis_curr_index].axis('off')\n",
    "        axis_curr_index += 1\n",
    "\n",
    "    mpl.pyplot.savefig(output_path, pad_inches=0, bbox_inches='tight', dpi=150)\n",
    "    mpl.pyplot.close('all')\n",
    "\n",
    "def read_image(image_path):\n",
    "    image = (np.load(image_path) * 255).astype(np.uint8)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "def normalize_and_clip_depth(depth, max_depth):\n",
    "    \"\"\"\n",
    "    Return an optionally normalized (and clipped) depth.\n",
    "    \"\"\"\n",
    "    depth[np.isnan(depth)] = max_depth\n",
    "    depth[depth > max_depth] = max_depth\n",
    "    depth = ((depth) / max_depth).astype(np.float32)\n",
    "    return depth\n",
    "\n",
    "DEFAULT_TONEMAP_PARAMS = {\"policy\": \"tonemap\", \"alpha\": 0.25, \"beta\": 0.9, \"gamma\": 0.9, \"eps\": 1e-6}\n",
    "def normalize_image(image, hdr_mode=True, normalization_params=DEFAULT_TONEMAP_PARAMS, return_8_bit=False):\n",
    "    \"\"\"\n",
    "    Normalize an 8 bit image according to the specified policy.\n",
    "    If return_8_bit, this returns an np.uint8 image, otherwise it returns a floating point\n",
    "    image with values in [0, 1].\n",
    "    \"\"\"\n",
    "    normalization_policy = normalization_params['policy']\n",
    "    lower_bound = 0\n",
    "    upper_bound = 1\n",
    "    if np.isnan(hdr_mode):\n",
    "        hdr_mode = False\n",
    "\n",
    "    if hdr_mode and image.dtype == np.uint8:\n",
    "        # The image was normalized during pack-perception (tonemap)\n",
    "        if return_8_bit:\n",
    "            return image\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 255.0\n",
    "    elif normalization_policy == \"percentile\" and hdr_mode:\n",
    "        lower_bound = np.array([np.percentile(image[..., i],\n",
    "                                              normalization_params['lower_bound'],\n",
    "                                              interpolation='lower')\n",
    "                                for i in range(3)])\n",
    "        upper_bound = np.array([np.percentile(image[..., i],\n",
    "                                              normalization_params['upper_bound'],\n",
    "                                              interpolation='lower')\n",
    "                                for i in range(3)])\n",
    "    elif normalization_policy == \"percentile_vpu\" and hdr_mode:\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        brightness = (3 * r + b + 4 * g) / 8\n",
    "        lower_bound = np.percentile(brightness, normalization_params['lower_bound'],\n",
    "                                    interpolation='lower')\n",
    "        upper_bound = np.percentile(brightness, normalization_params['upper_bound'],\n",
    "                                    interpolation='lower')\n",
    "    elif normalization_policy == \"3sigma\" and hdr_mode:\n",
    "        sigma_size = normalization_params['sigma_size']\n",
    "        min_variance = normalization_params['min_variance']\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        brightness = (3 * r + b + 4 * g) / 8\n",
    "        mean, sigma = np.mean(brightness), np.std(brightness)\n",
    "        brightness_min, brightness_max = np.min(brightness), np.max(brightness)\n",
    "        if (sigma * sigma_size) > mean:\n",
    "            lmin = brightness_min\n",
    "            lmax = min(brightness_max, mean * sigma_size)\n",
    "            if (lmax - lmin) < min_variance:\n",
    "                lmax = lmin + min_variance\n",
    "            lower_bound = lmin\n",
    "            upper_bound = lmax\n",
    "        else:\n",
    "            mean_var = mean - sigma_size * sigma\n",
    "            output_min = max(brightness_min, mean_var)\n",
    "            mean_var = mean + sigma_size * sigma\n",
    "            output_max = min(brightness_max, mean_var)\n",
    "            if (output_max - output_min) < min_variance:\n",
    "                output_min = mean - min_variance / 2.0\n",
    "                output_min = 0 if output_min < 0 else output_min\n",
    "                output_max = output_min + min_variance\n",
    "            lower_bound = output_min\n",
    "            upper_bound = output_max\n",
    "    elif normalization_policy == 'tonemap' and hdr_mode:\n",
    "        if image.dtype != np.float32 and image.dtype != np.uint32:\n",
    "            raise ValueError('HDR image type is {} instead of float32 or uint32'.format(image.dtype))\n",
    "        alpha = normalization_params.get('alpha', DEFAULT_TONEMAP_PARAMS['alpha'])\n",
    "        beta = normalization_params.get('beta', DEFAULT_TONEMAP_PARAMS['beta'])\n",
    "        gamma = normalization_params.get('gamma', DEFAULT_TONEMAP_PARAMS['gamma'])\n",
    "        eps = normalization_params.get('eps', DEFAULT_TONEMAP_PARAMS['eps'])\n",
    "\n",
    "        r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
    "        lum_in = 0.2126 * r + 0.7152 * g + 0.0722 * b\n",
    "        lum_norm = np.exp(gamma * np.mean(np.log(lum_in + eps)))\n",
    "        c = alpha * lum_in / lum_norm\n",
    "        c_max = beta * np.max(c)\n",
    "        lum_out = c / (1 + c) * (1 + c / (c_max ** 2))\n",
    "        image = image * (lum_out / (lum_in + eps))[..., None]\n",
    "    elif normalization_policy == \"none\" and hdr_mode:\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 2**20 - 1\n",
    "    elif normalization_policy == \"default\" or not hdr_mode:\n",
    "        assert np.max(image) <= 255 and np.min(image) >= 0, \"Image with default \" \\\n",
    "            \"mode should be in range [0,255]\"\n",
    "        lower_bound = 0.0\n",
    "        upper_bound = 255.0\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"--normalization-policy '{normalization_policy}' is not supported! \"\n",
    "            f\"(on image with hdr_mode={hdr_mode})\")\n",
    "\n",
    "    image = (image.astype(np.float32, copy=False) - lower_bound) / (upper_bound - lower_bound)\n",
    "\n",
    "    if return_8_bit:\n",
    "        image = np.clip(image * 255.0, 0.0, 255.0)\n",
    "        image = np.uint8(image)\n",
    "    else:\n",
    "        image = np.clip(image, 0.0, 1.0)\n",
    "\n",
    "    return image\n",
    "\n",
    "def read_saved_frame(pred_dir, image_id):\n",
    "    states_to_save = ['', 'false_positive', 'false_negative', 'large_object_false_negative', 'true_positive', 'true_negative']\n",
    "    frame = None\n",
    "    for state in states_to_save:\n",
    "        if os.path.isfile(os.path.join(pred_dir, state, image_id+'.png')):\n",
    "            frame = cv2.imread(os.path.join(pred_dir, state, image_id+'.png'))\n",
    "            break\n",
    "        if os.path.isfile(os.path.join(pred_dir, state, image_id+'.jpg')):\n",
    "            frame = cv2.imread(os.path.join(pred_dir, state, image_id+'.jpg'))\n",
    "            break\n",
    "    return frame\n",
    "\n",
    "def read_images(pred_dir, _id):\n",
    "    if not os.path.isfile(os.path.join(pred_dir, _id+'_image.npy')):\n",
    "        return None, None, None, None\n",
    "    image = np.load(os.path.join(pred_dir, _id+'_image.npy'))\n",
    "    \n",
    "    # 100m capped depth\n",
    "    depth = np.load(os.path.join(pred_dir, _id+'_depth.npy'))\n",
    "#     # raw depth\n",
    "#     raw_depth_dir = '/raum_raid/li.yu/data/Jupiter_rock_demo_2021/Jupiter_rock_demo_loamy06_Oct20_2021/model_processed_v4.1_sky_2e-3_lr_1e-3_color_aug_full_model_LR_consistency_regularization_0.2_epoch_23/images/'\n",
    "#     stereo_data = np.load(os.path.join(raw_depth_dir, _id, 'stereo_output.npz'))\n",
    "#     depth = stereo_data['point_cloud'][:,:,-1]\n",
    "#     depth = normalize_and_clip_depth(depth, 200)\n",
    "    \n",
    "    pred_label = np.load(os.path.join(pred_dir, _id+'_pred_label.npy'))\n",
    "    confidence = np.load(os.path.join(pred_dir, _id+'_confidence.npy'))\n",
    "    return image, depth, pred_label, confidence\n",
    "\n",
    "# def create_frame(pred_dir, pred_merged_dir, _id, recreate=False):\n",
    "#     canvas_path = os.path.join(pred_merged_dir, _id+'.png')\n",
    "#     if recreate:\n",
    "#         image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "#         if image is None:\n",
    "#             return None\n",
    "#         create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth)\n",
    "#     frame = cv2.imread(canvas_path)\n",
    "#     return frame\n",
    "\n",
    "def get_bbox_coords(i=-1, bbox_range_list=[], bbox_coord_list=[]):\n",
    "    for bi in range(len(bbox_range_list)):\n",
    "        bbox_range = bbox_range_list[bi]\n",
    "        if bbox_range[0] <= i <= bbox_range[1]:\n",
    "            return bbox_coord_list[bi]\n",
    "    return []\n",
    "\n",
    "def process_frame(pred_dir, pred_merged_dir, _id, recreate=False, bbox_coords=[]):\n",
    "    image, depth, pred_label, confidence = None, None, None, None\n",
    "    l = 0.0\n",
    "    avg_pixel = 0.0\n",
    "    bbox_conf = None\n",
    "    if recreate:\n",
    "        image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "        if image is None:\n",
    "            return image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "        # calculate brightness\n",
    "        hlsImg = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "        l = np.average(hlsImg[:,:,1])\n",
    "        image_title = 'Image (brightness: {:.4f})'.format(l)\n",
    "        # calculate average pixel value at bbox area\n",
    "        if bbox_coords:\n",
    "            ymin, ymax, xmin, xmax = bbox_coords\n",
    "            bbox_pred = pred_label[ymin:ymax+1, xmin:xmax+1]\n",
    "            bbox_conf = confidence[ymin:ymax+1, xmin:xmax+1]\n",
    "            avg_pixel = np.count_nonzero(bbox_pred == 1)\n",
    "    return image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "\n",
    "def create_frame(pred_dir, pred_merged_dir, _id, recreate=False, bbox_coords=[]):\n",
    "    canvas_path = os.path.join(pred_merged_dir, _id+'.png')\n",
    "    image, depth, pred_label, confidence = None, None, None, None\n",
    "    l = 0.0\n",
    "    avg_pixel = 0.0\n",
    "    bbox_conf = None\n",
    "    if recreate:\n",
    "        image, depth, pred_label, confidence = read_images(pred_dir, _id)\n",
    "        if image is None:\n",
    "            return None, None, None, None, None, None, None, None\n",
    "        # calculate brightness\n",
    "        hlsImg = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "        l = np.average(hlsImg[:,:,1])\n",
    "        image_title = 'Image (brightness: {:.4f})'.format(l)\n",
    "        # calculate average pixel value at bbox area\n",
    "        if bbox_coords:\n",
    "            ymin, ymax, xmin, xmax = bbox_coords\n",
    "            bbox_pred = pred_label[ymin:ymax+1, xmin:xmax+1]\n",
    "            bbox_conf = confidence[ymin:ymax+1, xmin:xmax+1]\n",
    "            avg_pixel = np.count_nonzero(bbox_pred == 1)\n",
    "        create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth, image_title=image_title, bbox_coords=bbox_coords)\n",
    "    frame = cv2.imread(canvas_path)\n",
    "    return frame, image, depth, pred_label, confidence, l, avg_pixel, bbox_conf\n",
    "\n",
    "def create_diff_frame(pred_merged_dir, _id, image1, depth1, pred_label1, confidence1, \n",
    "                      image2, depth2, pred_label2, confidence2, pred_title='prediction', conf_title='confidence'):\n",
    "    canvas_path = os.path.join(pred_merged_dir, _id+'_diff.png')\n",
    "    image = np.abs(image1 - image2)\n",
    "    depth = np.abs(depth1 - depth2)\n",
    "    pred_label = np.abs(pred_label1 - pred_label2)\n",
    "    confidence = np.abs(confidence1 - confidence2)\n",
    "    create_mpl_viz_outputs(canvas_path, image, pred_label, confidence, depth, conf_title=conf_title)\n",
    "    frame = cv2.imread(canvas_path)\n",
    "    return frame\n",
    "\n",
    "def read_raw_image_by_id(data_dir, _id):\n",
    "    image_path = os.path.join(data_dir, 'images', _id, 'artifact_debayeredrgb_0_'+_id+'.png')\n",
    "    image = cv2.imread(image_path)\n",
    "    return image\n",
    "\n",
    "def read_raw_image_by_row(data_dir, row):\n",
    "    image = cv2.imread(os.path.join(data_dir, row.artifact_debayeredrgb_0_save_path))\n",
    "    return image\n",
    "\n",
    "def create_video(ids, pred_dir, video_name, read_func=read_saved_frame, fps=2):\n",
    "    frame = read_func(pred_dir, ids[10])\n",
    "    height, width, layers = frame.shape\n",
    "    print(height, width, layers)\n",
    "\n",
    "    # .avi MJPG,  .mp4 MP4V\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width,height), isColor=True)\n",
    "    \n",
    "    good = 0\n",
    "    for _id in tqdm(ids):\n",
    "        frame = read_func(pred_dir, _id)\n",
    "        if frame is not None:\n",
    "            video.write(frame)\n",
    "            good += 1\n",
    "    print('total', len(ids), 'used', good)\n",
    "\n",
    "    # cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_sequences(df, interval=5*60, per_camera=False):\n",
    "    df = df.sort_values('collected_on')\n",
    "    df['datetime'] = df.collected_on.apply(datetime.fromisoformat)\n",
    "    sequence_dfs = []\n",
    "    delta = timedelta(seconds=interval)\n",
    "    start = True\n",
    "    i0, i = 0, 0\n",
    "    while i < len(df):\n",
    "        if start:\n",
    "            t0 = df.iloc[i].datetime\n",
    "            start = False\n",
    "        else:\n",
    "            t1 = df.iloc[i].datetime\n",
    "            if t1 - t0 > delta or i == len(df) - 1:\n",
    "                chunk_df = df.iloc[i0 : i if i < len(df) - 1 else len(df)]\n",
    "                if per_camera:\n",
    "                    camera_locations = chunk_df.camera_location.unique()\n",
    "                    camera_locations.sort()\n",
    "                    for camera_location in camera_locations:\n",
    "                        sequence_df = chunk_df[chunk_df.camera_location == camera_location]\n",
    "                        sequence_df = sequence_df.sort_values('collected_on')\n",
    "                        sequence_dfs.append(sequence_df)\n",
    "                else:\n",
    "                    sequence_dfs.append(chunk_df)\n",
    "                start = True\n",
    "                i0 = i\n",
    "            else:\n",
    "                t0 = t1\n",
    "        i += 1\n",
    "    return sequence_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create video from raw left images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1332, 171) 15\n"
     ]
    }
   ],
   "source": [
    "# create video from raw left images\n",
    "# data_dir = '/data/jupiter/datasets/halo_failure_case_of_box_in_dust'\n",
    "# data_dir = '/data/jupiter/li.yu/data/halo_sample_terrains_15images'\n",
    "data_dir = '/data/jupiter/li.yu/data/halo_sample_terrains_15images_all_camera_sequence'\n",
    "df = pd.read_csv(os.path.join(data_dir, 'annotations.csv'))\n",
    "# df = df[df.camera_location == 'rear-left']\n",
    "df = df.sort_values('collected_on')\n",
    "\n",
    "# ids = df.id.to_list()\n",
    "# video_name = os.path.join(data_dir, f'video.mp4')\n",
    "# create_video(ids, data_dir, video_name, read_raw_image_by_id, fps=1)\n",
    "\n",
    "sub_dfs = get_sequences(df)\n",
    "print(df.shape, len(sub_dfs))\n",
    "\n",
    "# for i,sub_df in enumerate(sub_dfs):\n",
    "#     ids = sub_df.id.to_list()\n",
    "#     video_name = os.path.join(data_dir, f'dust_seq{i}.mp4')\n",
    "#     create_video(ids, data_dir, video_name, read_raw_image_by_id, fps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_ids = [13, 0, 5, 7, 9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 24, 24, 24]\n"
     ]
    }
   ],
   "source": [
    "concat_dfs = [concat_dfs[-1]] + concat_dfs[:4]\n",
    "cam_dfs = [pd.concat([concat_dfs[i][j] for i in range(len(concat_dfs))]) for j in range(4)]\n",
    "min_len = min(len(cdf) for cdf in cam_dfs)\n",
    "print([len(cdf) for cdf in cam_dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T02', 'T06', 'T14', 'T10']\n",
      "0 [24, 24, 24, 24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:07<00:00,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# read from multiple cameras and put in once frame\n",
    "# cameras = ['T01', 'T02', 'T03', 'T04']\n",
    "cameras = ['T02', 'T06', 'T14', 'T10']\n",
    "# cameras = [f'T{str(i+1).zfill(2)}' for i in range(16)]\n",
    "print(cameras)\n",
    "concat_dfs = []\n",
    "for i,sub_df in enumerate(sub_dfs):\n",
    "    # cam_dfs = [sub_df[sub_df.camera_location == c] for c in cameras]\n",
    "    # min_len = min(len(cdf) for cdf in cam_dfs)\n",
    "    # cam_dfs = [cdf.sort_values('collected_on').iloc[:min_len] for cdf in cam_dfs]\n",
    "    if min_len < 2:\n",
    "        continue\n",
    "    print(i, [len(cdf) for cdf in cam_dfs])\n",
    "    # print(i, [cdf.iloc[0].collected_on for cdf in cam_dfs])\n",
    "    \n",
    "    if i in seq_ids:\n",
    "        if i == 13:\n",
    "            concat_dfs.append([cdf.sort_values('collected_on').iloc[:min_len//3*2] for cdf in cam_dfs])\n",
    "        else:\n",
    "            concat_dfs.append(cam_dfs)\n",
    "\n",
    "    frame = read_raw_image_by_row(data_dir, sub_df.iloc[0])\n",
    "    height, width, layers = frame.shape\n",
    "    # print(height, width, layers)\n",
    "\n",
    "    # .avi MJPG,  .mp4 MP4V\n",
    "    os.makedirs(os.path.join(data_dir, f'videos_{len(cameras)}cams_concat'), exist_ok=True)\n",
    "    video_name = os.path.join(data_dir, f'videos_{len(cameras)}cams_concat/seq{i}.mp4')\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), 1, (width*2,height*2), isColor=True)\n",
    "\n",
    "    H = int(len(cameras)**0.5)\n",
    "    W = len(cameras) // H\n",
    "    for ii in tqdm(range(min_len)):\n",
    "        try:\n",
    "            canvas = np.zeros((height*H, width*W, 3), dtype=np.uint8)\n",
    "            for ci in range(len(cam_dfs)):\n",
    "                frame = read_raw_image_by_row(data_dir, cam_dfs[ci].iloc[ii])\n",
    "                # frame = cv2.putText(frame, f'{cam_dfs[ci].iloc[ii].collected_on}', \n",
    "                #         (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "                fi, fj = ci // W, ci % W\n",
    "                canvas[height*fi:height*(fi+1), width*fj:width*(fj+1)] = frame\n",
    "            video.write(canvas)\n",
    "        except:\n",
    "            print(f'{ii}th image read failed')\n",
    "\n",
    "    # cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create video from saved pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(ids, pred_dir, video_name, read_func=read_saved_frame, fps=2):\n",
    "    frame = read_func(pred_dir, ids[10])\n",
    "    height, width, layers = frame.shape\n",
    "    print(height, width, layers)\n",
    "\n",
    "    # .avi MJPG,  .mp4 MP4V\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width,height), isColor=True)\n",
    "    \n",
    "    good = 0\n",
    "    for _id in tqdm(ids):\n",
    "        frame = read_func(pred_dir, _id)\n",
    "        if frame is not None:\n",
    "            video.write(frame)\n",
    "            good += 1\n",
    "    print('total', len(ids), 'used', good)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 3\n",
      "0 2021-11-10T00:33:05.931000 49\n",
      "311 3141 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:01<00:00, 37.81it/s]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 49 used 49\n",
      "1 2021-12-16T22:14:00.050000 71\n",
      "311 3141 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:01<00:00, 38.55it/s]\n",
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 71 used 71\n",
      "2 2022-11-10T23:19:10.901000 88\n",
      "311 3141 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:02<00:00, 40.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88 used 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/data/jupiter/li.yu/data/Jupiter_human_on_path_3_fn_sequence'\n",
    "# csv_file = '/data/jupiter/datasets/Jupiter_gilroy_reverse_manny_v2/master_annotations_1fps.csv'\n",
    "# csv_file = '/data/jupiter/li.yu/data/mannequin_in_dust_v1/master_annotations.csv'\n",
    "csv_file = f'{data_dir}/master_annotations.csv'\n",
    "master_df = pd.read_csv(csv_file)\n",
    "master_df = master_df.sort_values('collected_on')\n",
    "\n",
    "# pred_dir = '/mnt/sandbox1/rakhil.immidisetti/output/driveable_terrain_model/471_cloud_v45_cutnpaste_s35/Jupiter_gilroy_reverse_manny_v2reverse_val_bestmodel/'\n",
    "# pred_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model/v471_nobadiqnohard_6cls_cnp_alpha2_0805/Jupiter_gilroy_reverse_manny_v2/'\n",
    "# pred_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/mannequin_in_dust_v1'\n",
    "pred_dir = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/Jupiter_human_on_path_3_fn_sequence'\n",
    "save_dir = pred_dir\n",
    "\n",
    "# # save as a single video\n",
    "# print(master_df.shape)\n",
    "# video_name = os.path.join(save_dir, 'pred.mp4')\n",
    "# ids = master_df.image_id.to_list()\n",
    "# create_video(ids, pred_dir, video_name, fps=3)\n",
    "\n",
    "# break into sequences\n",
    "seq_dfs = get_sequences(master_df, interval=5, per_camera=True)\n",
    "print(len(master_df), len(seq_dfs))\n",
    "video_dir = os.path.join(pred_dir, 'videos')\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "for si, seq_df in enumerate(seq_dfs):\n",
    "    if len(seq_df) < 10:\n",
    "        continue\n",
    "    name = seq_df.iloc[0].collected_on\n",
    "    camera = seq_df.iloc[0].camera_location[:-5]\n",
    "    print(si, name, len(seq_df))\n",
    "    \n",
    "    # create video\n",
    "    # video_name = os.path.join(video_dir, str(si).zfill(3)+'.mp4')\n",
    "    video_name = os.path.join(video_dir, f'{name}_{camera}_{si}.mp4')\n",
    "    ids = seq_df.image_id.to_list()\n",
    "    create_video(ids, pred_dir, video_name, fps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create video from PP artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_pp_artifacts(data_dir, df_row):\n",
    "    data_path = os.path.join(data_dir, 'processed/images', df_row.id, 'stereo_output.npz')\n",
    "    img = np.load(data_path)['left']\n",
    "    img_norm = normalize_image(img, df_row.hdr_mode if 'hdr_mode' in df_row else True)\n",
    "    return (img_norm * 255).astype(np.uint8)\n",
    "\n",
    "def add_text(frame, txt_row):\n",
    "    frame = cv2.putText(frame, f'Collected on: {txt_row.collected_on}', \n",
    "                        (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 153) 3\n"
     ]
    }
   ],
   "source": [
    "data_root_dir = '/data/jupiter/li.yu/data'\n",
    "# dataset = 'mannequin_in_dust_v1'\n",
    "dataset = 'Jupiter_human_on_path_3_fn_sequence'\n",
    "data_dir = os.path.join(data_root_dir, dataset)\n",
    "video_dir = os.path.join(data_root_dir, dataset, 'videos')\n",
    "sequence_dir = os.path.join(data_root_dir, dataset, 'sequences')\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "os.makedirs(sequence_dir, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, 'master_annotations.csv'))\n",
    "seq_dfs = get_sequences(df, interval=5, per_camera=True)\n",
    "print(df.shape, len(seq_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2021-11-10T00:33:05.931000 49\n",
      "1 2021-12-16T22:14:00.050000 71\n",
      "2 2022-11-10T23:19:10.901000 88\n"
     ]
    }
   ],
   "source": [
    "height, width = 512, 1024\n",
    "for si, seq_df in enumerate(seq_dfs):\n",
    "    if len(seq_df) < 10:\n",
    "        continue\n",
    "    name = seq_df.iloc[0].collected_on\n",
    "    camera = seq_df.iloc[0].camera_location[:-5]\n",
    "    print(si, name, len(seq_df))\n",
    "    \n",
    "    # create video\n",
    "    video_name = os.path.join(video_dir, f'{name}_{camera}_{si}.mp4')\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 3, (width,height), isColor=True)\n",
    "    for fi, df_row in seq_df.iterrows():\n",
    "        frame = read_from_pp_artifacts(data_dir, df_row)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        video.write(frame)\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "    # # create sequence\n",
    "    # seq_dir_name = os.path.join(sequence_dir, f'{name}_{camera}_{si}')\n",
    "    # os.makedirs(seq_dir_name, exist_ok=True)\n",
    "    # fi = 0\n",
    "    # for _, df_row in seq_df.iterrows():\n",
    "    #     frame = read_from_pp_artifacts(data_dir, df_row)\n",
    "    #     frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    #     cv2.imwrite(os.path.join(seq_dir_name, str(fi).zfill(3)+'_'+df_row.id+'.png'), frame)\n",
    "    #     fi += 1\n",
    "    # # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read frame and add prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 11)\n"
     ]
    }
   ],
   "source": [
    "# compare BRT model pred and CenterTrack pred\n",
    "# pred_csv = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/mannequin_in_dust_v1/output.csv'\n",
    "pred_csv = '/data/jupiter/li.yu/exps/driveable_terrain_model/v188_58d_rak_local_fine_tversky11_sum_image_normT_prod5_airdyn_r3a8_s30/Jupiter_human_on_path_3_fn_sequence/output.csv'\n",
    "pred_df = pd.read_csv(pred_csv)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 2023-07-08T01:37:09.798000 153\n"
     ]
    }
   ],
   "source": [
    "video_with_pred_dir = os.path.join(data_root_dir, dataset, 'videos_with_pred')\n",
    "# pred_sequence_dir = '/home/li.yu/code/CenterTrack/results/2023-07-08T01:37:09.798000_front-center_15'\n",
    "pred_sequence_dir = '/home/li.yu/code/CenterTrack/results/brt50000/nopreimg_noprehm/2023-07-08T01:37:09.798000_front-center_15'\n",
    "os.makedirs(video_with_pred_dir, exist_ok=True)\n",
    "height, width = 512, 1024\n",
    "\n",
    "for si, seq_df in enumerate(seq_dfs):\n",
    "    if len(seq_df) < 10 or si != 15:\n",
    "        continue\n",
    "    name = seq_df.iloc[0].collected_on\n",
    "    camera = seq_df.iloc[0].camera_location[:-5]\n",
    "    print(si, name, len(seq_df))\n",
    "    \n",
    "    # merge pred from BRT model\n",
    "    seq_df = seq_df.drop(columns=['state']).merge(pred_df[['id', 'state', 'human_state']], on='id')\n",
    "\n",
    "    # create video\n",
    "    video_name = os.path.join(video_with_pred_dir, f'{name}_{camera}_{si}_nopreimg_noprehm.mp4')\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 3, (width,height), isColor=True)\n",
    "    fi = 0\n",
    "    for _, df_row in seq_df.iterrows():\n",
    "        frame = cv2.imread(os.path.join(pred_sequence_dir, str(fi).zfill(3)+'_'+df_row.id+'.png'))\n",
    "        frame = cv2.putText(frame, f'BRT Seg Pred: {df_row.state}, Strict: {df_row.human_state}', \n",
    "                            (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "        video.write(frame)\n",
    "        fi += 1\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read from video and prediction results to each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2022-11-10T23:19:10.901000 88\n"
     ]
    }
   ],
   "source": [
    "video_dir = '/home/li.yu/code/CenterTrack/results/'\n",
    "old_video_name = 'brt50000_2022-11-10T23:19:10.901000_side-right_2_preimg.mp4'\n",
    "new_video_name = 'brt50000_2022-11-10T23:19:10.901000_side-right_2_preimg_withbrtpred.mp4'\n",
    "height, width = 512, 1024\n",
    "\n",
    "for si, seq_df in enumerate(seq_dfs):\n",
    "    if len(seq_df) < 10 or si != 2:\n",
    "        continue\n",
    "    name = seq_df.iloc[0].collected_on\n",
    "    camera = seq_df.iloc[0].camera_location[:-5]\n",
    "    print(si, name, len(seq_df))\n",
    "    \n",
    "    # merge pred from BRT model\n",
    "    seq_df = seq_df.drop(columns=['state']).merge(pred_df[['id', 'state', 'human_state']], on='id')\n",
    "\n",
    "    # read video\n",
    "    cam = cv2.VideoCapture(os.path.join(video_dir, old_video_name))\n",
    "\n",
    "    # create video\n",
    "    video_name = os.path.join(video_dir, new_video_name)\n",
    "    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), 3, (width,height), isColor=True)\n",
    "    for _, df_row in seq_df.iterrows():\n",
    "        _, frame = cam.read()\n",
    "        frame = cv2.putText(frame, f'BRT Seg Pred: {df_row.state}, Strict: {df_row.human_state}', \n",
    "                            (40,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv2.LINE_AA)\n",
    "        video.write(frame)\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "    # break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 64-bit ('pytorchlightning': conda)",
   "name": "python388jvsc74a57bd01eceddbeeb55f686303d64ef8e05e300429be7c506c9f9cad24a6dfe5f27b555"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "1eceddbeeb55f686303d64ef8e05e300429be7c506c9f9cad24a6dfe5f27b555"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}